{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9e27ff",
   "metadata": {},
   "source": [
    "## lire le document de quelle type il est et si cest une image ou contien du text dans sont code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2dc739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'documents/francais.docx',\n",
       "  'ext': '.docx',\n",
       "  'mime': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
       "  'label': 'Word document (DOCX)',\n",
       "  'content': 'text'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List, Dict, Any\n",
    "\n",
    "\n",
    "# Saisie possible:\n",
    "# INPUT_FILE = \"a.pdf, b.docx, c.png\"\n",
    "# INPUT_FILE = [\"a.pdf\", \"b.docx\", \"c.png\"]\n",
    "INPUT_FILE: Optional[Union[str, Sequence[str]]] = (\n",
    "    # \"epsteanpdf.pdf, epsteain22.pdf, testexcel.xlsx, testword.docx, image2tab.webp, contras-14page.pdf, signettab.png\"\n",
    "    # \"contras-14page.pdf, testword.docx, testexcel.xlsx, signettab.png, image2tab.webp\"\n",
    "    \"documents/francais.docx\"\n",
    ")\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1     # 1 caractère => \"text\"\n",
    "MIN_CHARS_PDF = 30       # seuil de texte extrait\n",
    "PDF_MAX_PAGES = 3        # on teste les N premières pages\n",
    "\n",
    "# Dossiers de recherche si un nom est donné sans chemin (utile en notebook)\n",
    "SEARCH_DIRS = [\n",
    "    os.getcwd(),\n",
    "    \"/mnt/data\",  # utile dans l'environnement ChatGPT\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "# ----------------- input parsing -----------------\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    \"\"\"Retourne toujours une liste. Supporte une string avec virgules (CSV).\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s:\n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Résout un chemin:\n",
    "    - si p existe tel quel -> retourne p\n",
    "    - sinon essaie SEARCH_DIRS + basename(p)\n",
    "    - sinon retourne None (introuvable)\n",
    "    \"\"\"\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ----------------- format detection -----------------\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    # ZIP containers (DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB/ZIP)\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                # EPUB\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                # Office OpenXML\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                # OpenDocument\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    # Ancien Office (OLE2)\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "# ----------------- text vs image_only -----------------\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    \"\"\"Compte du texte dans du XML (éléments + fallback simple).\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    \"\"\"\n",
    "    DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB\n",
    "    True si on trouve au moins MIN_CHARS_OFFICE caractères.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                # corps\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                # headers/footers (souvent du texte “isolé”)\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                if \"content.xml\" in names:\n",
    "                    return _xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE\n",
    "                return False\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    \"\"\"Retourne PdfReader depuis pypdf ou PyPDF2, ou None si indisponible.\"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    PDF:\n",
    "    - True si extract_text() produit assez de caractères, OU si fonts / opérateurs texte présents.\n",
    "    - Si aucune lib PDF n'est dispo: fallback binaire (cherche /Font ou opérateurs BT/Tj).\n",
    "    \"\"\"\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        # fallback binaire: moins fiable, mais évite de renvoyer faux systématique\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)  # 2MB max\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "\n",
    "        extracted_score = 0\n",
    "        saw_font = False\n",
    "        saw_text_ops = False\n",
    "\n",
    "        for page in pages:\n",
    "            # 1) extraction texte\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "\n",
    "            # 2) fonts dans resources\n",
    "            try:\n",
    "                res = page.get(\"/Resources\") or {}\n",
    "                font = res.get(\"/Font\")\n",
    "                if font:\n",
    "                    saw_font = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # 3) opérateurs texte dans stream\n",
    "            try:\n",
    "                contents = page.get_contents()\n",
    "                if contents is None:\n",
    "                    continue\n",
    "                if hasattr(contents, \"get_data\"):\n",
    "                    data = contents.get_data()\n",
    "                else:\n",
    "                    data = b\"\".join(c.get_data() for c in contents)  # type: ignore\n",
    "                if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                    saw_text_ops = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return saw_font or saw_text_ops\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    \"\"\"Retourne seulement: 'text' ou 'image_only'.\"\"\"\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    # Images => image_only\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    # Formats texte compressés (Office/ODF/EPUB)\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    # Tout le reste => image_only (car tu veux 2 états)\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "def analyze_many_two_states(input_file: Optional[Union[str, Sequence[str]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sortie:\n",
    "      [{\"path\": ..., \"ext\": ..., \"mime\": ..., \"label\": ..., \"content\": \"text|image_only\"}, ...]\n",
    "    Ignore les fichiers introuvables.\n",
    "    \"\"\"\n",
    "    raw_paths = normalize_input_files(input_file)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for raw in raw_paths:\n",
    "        p = resolve_path(raw)\n",
    "        if p is None:\n",
    "            continue\n",
    "\n",
    "        ft = detect_path_type(p)\n",
    "        out.append({\n",
    "            \"path\": p,\n",
    "            \"ext\": ft.ext,\n",
    "            \"mime\": ft.mime,\n",
    "            \"label\": ft.label,\n",
    "            \"content\": content_kind_two_states(p, ft),\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test\n",
    "analyze_many_two_states(INPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d63c5",
   "metadata": {},
   "source": [
    "### si image faire passer sur un pretraitemetn lamelirer sinon un document avce text dans sont code source aallors pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99d5904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] content='text' -> c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "\n",
    "\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    np = None\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In notebooks __file__ is undefined; fall back to current working directory.\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_LANG = \"fra\"\n",
    "DEFAULT_CONTRAST = 1.5\n",
    "DEFAULT_SHARPNESS = 1.2\n",
    "DEFAULT_BRIGHTNESS = 1.0\n",
    "DEFAULT_UPSCALE = 1.5\n",
    "DEFAULT_DPI = 300\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1\n",
    "MIN_CHARS_PDF = 30\n",
    "PDF_MAX_PAGES = 3\n",
    "SEARCH_DIRS = [os.getcwd(), \"/mnt/data\"]  # utile en notebook\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s: \n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return os.path.abspath(p)\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return os.path.abspath(alt)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                return (\"content.xml\" in names) and (_xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE)\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    PdfReader = _get_pdf_reader()\n",
    "\n",
    "    if PdfReader is None:\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "        extracted_score = 0\n",
    "\n",
    "        for page in pages:\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "# --------- ROUTAGE ---------\n",
    "ORIGINAL_INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "_raw_items = normalize_input_files(ORIGINAL_INPUT_FILE)\n",
    "\n",
    "IMAGE_ONLY_FILES: List[str] = []\n",
    "TEXT_FILES: List[str] = []\n",
    "MISSING_FILES: List[str] = []\n",
    "\n",
    "for item in _raw_items:\n",
    "    p = resolve_path(item)\n",
    "    if p is None:\n",
    "        MISSING_FILES.append(item)\n",
    "        continue\n",
    "\n",
    "    ft = detect_path_type(p)\n",
    "    kind = content_kind_two_states(p, ft)\n",
    "\n",
    "    if kind == \"image_only\":\n",
    "        IMAGE_ONLY_FILES.append(p)\n",
    "    else:\n",
    "        TEXT_FILES.append(p)\n",
    "        print(f\"[skip] content='text' -> {p}\")\n",
    "\n",
    "# IMPORTANT: ton code OCR (cellule suivante) reste inchangé, il lira INPUT_FILE ici\n",
    "INPUT_FILE = IMAGE_ONLY_FILES\n",
    "\n",
    "if MISSING_FILES:\n",
    "    print(\"[missing] fichiers introuvables:\")\n",
    "    for m in MISSING_FILES:\n",
    "        print(\" -\", m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SHOW_PREPROCESSED = True   #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhanceOptions:\n",
    "    contrast: float = DEFAULT_CONTRAST\n",
    "    sharpness: float = DEFAULT_SHARPNESS\n",
    "    brightness: float = DEFAULT_BRIGHTNESS\n",
    "    upscale: float = DEFAULT_UPSCALE\n",
    "    gamma: Optional[float] = None  # gamma correction; <1 brightens darks, >1 darkens\n",
    "    pad: int = 0  # pixels to pad around the image\n",
    "    median: Optional[int] = None  # kernel size for median filter (odd int, e.g., 3)\n",
    "    unsharp_radius: Optional[float] = None  # e.g., 1.0\n",
    "    unsharp_percent: int = 150\n",
    "    invert: bool = False\n",
    "    autocontrast_cutoff: Optional[int] = None  # 0-100; percentage to clip for autocontrast\n",
    "    equalize: bool = False  # histogram equalization\n",
    "    auto_rotate: bool = False  # attempt orientation detection + rotate\n",
    "    otsu: bool = False  # auto-threshold with Otsu (requires numpy)\n",
    "    threshold: Optional[int] = None  # 0-255; if set, applies a binary threshold\n",
    "\n",
    "\n",
    "def build_config(\n",
    "    oem: Optional[int],\n",
    "    psm: Optional[int],\n",
    "    base_flags: Iterable[str],\n",
    "    dpi: Optional[int],\n",
    "    tessdata_dir: Optional[Path],\n",
    "    user_words: Optional[Path],\n",
    "    user_patterns: Optional[Path],\n",
    ") -> str:\n",
    "    parts: List[str] = []\n",
    "    if oem is not None:\n",
    "        parts.append(f\"--oem {oem}\")\n",
    "    if psm is not None:\n",
    "        parts.append(f\"--psm {psm}\")\n",
    "    if dpi is not None:\n",
    "        parts.append(f\"--dpi {dpi}\")\n",
    "    if tessdata_dir is not None:\n",
    "        parts.append(f'--tessdata-dir \"{tessdata_dir}\"')\n",
    "    if user_words is not None:\n",
    "        parts.append(f'--user-words \"{user_words}\"')\n",
    "    if user_patterns is not None:\n",
    "        parts.append(f'--user-patterns \"{user_patterns}\"')\n",
    "    parts.extend(base_flags)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def ensure_environment(lang: str) -> None:\n",
    "    try:\n",
    "        _ = pytesseract.get_tesseract_version()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        sys.exit(\"Tesseract binary not found on PATH. Install it and its language data.\")\n",
    "    if lang:\n",
    "        try:\n",
    "            available = set(pytesseract.get_languages(config=\"\"))\n",
    "            requested = set(lang.split(\"+\"))\n",
    "            missing = requested - available\n",
    "            if missing:\n",
    "                print(\n",
    "                    f\"Warning: missing languages: {', '.join(sorted(missing))}. \"\n",
    "                    f\"Available: {', '.join(sorted(available))}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "        except pytesseract.TesseractError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def auto_rotate_if_needed(img: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    if not enhance.auto_rotate:\n",
    "        return img\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        angle = None\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                try:\n",
    "                    angle = int(line.split(\":\")[1].strip())\n",
    "                except ValueError:\n",
    "                    angle = None\n",
    "                break\n",
    "        if angle is not None and angle % 360 != 0:\n",
    "            return img.rotate(-angle, expand=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    img = image.convert(\"L\")\n",
    "    img = auto_rotate_if_needed(img, enhance)\n",
    "\n",
    "    if enhance.invert:\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    if enhance.pad and enhance.pad > 0:\n",
    "        img = ImageOps.expand(img, border=enhance.pad, fill=255)\n",
    "\n",
    "    if enhance.autocontrast_cutoff is not None:\n",
    "        cutoff = max(0, min(100, enhance.autocontrast_cutoff))\n",
    "        img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "\n",
    "    if enhance.equalize:\n",
    "        img = ImageOps.equalize(img)\n",
    "\n",
    "    if enhance.upscale and enhance.upscale != 1.0:\n",
    "        w, h = img.size\n",
    "        img = img.resize((int(w * enhance.upscale), int(h * enhance.upscale)), Image.LANCZOS)\n",
    "\n",
    "    if enhance.gamma and enhance.gamma > 0:\n",
    "        inv_gamma = 1.0 / enhance.gamma\n",
    "        lut = [pow(x / 255.0, inv_gamma) * 255 for x in range(256)]\n",
    "        img = img.point(lut)\n",
    "\n",
    "    if enhance.brightness and enhance.brightness != 1.0:\n",
    "        img = ImageEnhance.Brightness(img).enhance(enhance.brightness)\n",
    "\n",
    "    if enhance.contrast and enhance.contrast != 1.0:\n",
    "        img = ImageEnhance.Contrast(img).enhance(enhance.contrast)\n",
    "\n",
    "    if enhance.sharpness and enhance.sharpness != 1.0:\n",
    "        img = ImageEnhance.Sharpness(img).enhance(enhance.sharpness)\n",
    "\n",
    "    if enhance.unsharp_radius:\n",
    "        img = img.filter(\n",
    "            ImageFilter.UnsharpMask(\n",
    "                radius=enhance.unsharp_radius,\n",
    "                percent=enhance.unsharp_percent,\n",
    "                threshold=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if enhance.median and enhance.median > 1 and enhance.median % 2 == 1:\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=enhance.median))\n",
    "\n",
    "    if enhance.threshold is not None:\n",
    "        thr = max(0, min(255, enhance.threshold))\n",
    "        img = img.point(lambda p, t=thr: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "    elif enhance.otsu and np is not None:\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        hist, _ = np.histogram(arr, bins=256, range=(0, 256))\n",
    "        total = arr.size\n",
    "        sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "        sum_b = 0.0\n",
    "        w_b = 0.0\n",
    "        max_var = 0.0\n",
    "        threshold = 0\n",
    "\n",
    "        for i in range(256):\n",
    "            w_b += hist[i]\n",
    "            if w_b == 0:\n",
    "                continue\n",
    "            w_f = total - w_b\n",
    "            if w_f == 0:\n",
    "                break\n",
    "            sum_b += i * hist[i]\n",
    "            m_b = sum_b / w_b\n",
    "            m_f = (sum_total - sum_b) / w_f\n",
    "            var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "            if var_between > max_var:\n",
    "                max_var = var_between\n",
    "                threshold = i\n",
    "\n",
    "        img = img.point(lambda p, t=threshold: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-l\", \"--lang\", default=DEFAULT_LANG)\n",
    "    parser.add_argument(\"--oem\", type=int, choices=range(0, 4), default=None)\n",
    "    parser.add_argument(\"--psm\", type=int, choices=range(0, 14), default=None)\n",
    "    parser.add_argument(\"--dpi\", type=int, default=DEFAULT_DPI)\n",
    "    parser.add_argument(\"--tessdata-dir\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-words\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-patterns\", type=Path, default=None)\n",
    "    parser.add_argument(\"--whitelist\", type=str, default=None)\n",
    "    parser.add_argument(\"--blacklist\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--contrast\", type=float, default=DEFAULT_CONTRAST)\n",
    "    parser.add_argument(\"--sharpness\", type=float, default=DEFAULT_SHARPNESS)\n",
    "    parser.add_argument(\"--brightness\", type=float, default=DEFAULT_BRIGHTNESS)\n",
    "    parser.add_argument(\"--upscale\", type=float, default=DEFAULT_UPSCALE)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--pad\", type=int, default=0)\n",
    "    parser.add_argument(\"--threshold\", type=int, default=None)\n",
    "    parser.add_argument(\"--median\", type=int, default=None)\n",
    "    parser.add_argument(\"--unsharp-radius\", type=float, default=None)\n",
    "    parser.add_argument(\"--unsharp-percent\", type=int, default=150)\n",
    "    parser.add_argument(\"--invert\", action=\"store_true\")\n",
    "    parser.add_argument(\"--autocontrast-cutoff\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--auto-rotate\", action=\"store_true\")\n",
    "    parser.add_argument(\"--otsu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"*\",\n",
    "        default=[],\n",
    "        metavar=\"CFG\",\n",
    "        help=\"Additional configuration flags passed verbatim to tesseract (e.g., -c foo=bar).\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args(list(argv) if argv is not None else [])\n",
    "\n",
    "\n",
    "#  Exécution Cellule 1 (jusqu’à l’affichage) \n",
    "\n",
    "args = parse_args()\n",
    "ensure_environment(args.lang)\n",
    "\n",
    "enhance = EnhanceOptions(\n",
    "    contrast=args.contrast,\n",
    "    sharpness=args.sharpness,\n",
    "    brightness=args.brightness,\n",
    "    upscale=args.upscale,\n",
    "    gamma=args.gamma,\n",
    "    pad=args.pad,\n",
    "    median=args.median,\n",
    "    unsharp_radius=args.unsharp_radius,\n",
    "    unsharp_percent=args.unsharp_percent,\n",
    "    invert=args.invert,\n",
    "    autocontrast_cutoff=args.autocontrast_cutoff,\n",
    "    equalize=args.equalize,\n",
    "    auto_rotate=args.auto_rotate,\n",
    "    otsu=args.otsu,\n",
    "    threshold=args.threshold,\n",
    ")\n",
    "\n",
    "config_flags: List[str] = list(args.config)\n",
    "\n",
    "# AJOUTE ÇA :\n",
    "config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "\n",
    "if args.whitelist:\n",
    "    config_flags.append(f\"-c tessedit_char_whitelist={args.whitelist}\")\n",
    "if args.blacklist:\n",
    "    config_flags.append(f\"-c tessedit_char_blacklist={args.blacklist}\")\n",
    "\n",
    "\n",
    "def _normalize_input_files(val):\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        items = list(val)\n",
    "    else:\n",
    "        items = [val]\n",
    "\n",
    "    out = []\n",
    "    for item in items:\n",
    "        if item is None:\n",
    "            continue\n",
    "        if isinstance(item, Path):\n",
    "            out.append(str(item))\n",
    "            continue\n",
    "        s = str(item).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if \",\" in s:\n",
    "            parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "            out.extend(parts)\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# Backwards-compatible alias (older cell name)\n",
    "_normalize_input_file = _normalize_input_files\n",
    "\n",
    "# Safeguard if INPUT_FILE cell not executed yet\n",
    "INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "\n",
    "\n",
    "def _load_images_from_path(path: Path, dpi: int):\n",
    "    if path.suffix.lower() == \".pdf\":\n",
    "        try:\n",
    "            from pdf2image import convert_from_path\n",
    "        except Exception:\n",
    "            sys.exit(\n",
    "                \"pdf2image is not available. Install it and Poppler to read PDF files.\"\n",
    "            )\n",
    "        try:\n",
    "            return convert_from_path(str(path), dpi=dpi)\n",
    "        except Exception as exc:\n",
    "            sys.exit(f\"PDF conversion failed for {path}: {exc}\")\n",
    "    # default: image file (supports multi-page TIFF)\n",
    "    img = Image.open(path)\n",
    "    n_frames = getattr(img, \"n_frames\", 1)\n",
    "    if n_frames and n_frames > 1:\n",
    "        images = []\n",
    "        for i in range(n_frames):\n",
    "            try:\n",
    "                img.seek(i)\n",
    "            except Exception:\n",
    "                break\n",
    "            images.append(img.copy())\n",
    "        return images\n",
    "    return [img]\n",
    "\n",
    "\n",
    "input_items = _normalize_input_files(INPUT_FILE)\n",
    "if not input_items:\n",
    "    print(\"[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\")\n",
    "    DOCS = []\n",
    "else:\n",
    "    DOCS = []\n",
    "    for item in input_items:\n",
    "        path = Path(item)\n",
    "        if not path.is_absolute():\n",
    "            path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "        print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "        dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "        images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "        if len(images) == 1:\n",
    "            original = images[0]\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "        else:\n",
    "            total = len(images)\n",
    "            for idx, original in enumerate(images, start=1):\n",
    "                prepped = preprocess_image(original, enhance)\n",
    "                DOCS.append({\n",
    "                    \"path\": path,\n",
    "                    \"original\": original,\n",
    "                    \"prepped\": prepped,\n",
    "                    \"page_index\": idx,\n",
    "                    \"page_count\": total\n",
    "                })\n",
    "\n",
    "\n",
    "DOCS = []\n",
    "for item in input_items:\n",
    "    path = Path(item)\n",
    "    if not path.is_absolute():\n",
    "        path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "    if not path.exists():\n",
    "        sys.exit(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "    print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "    dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "    images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "    if len(images) == 1:\n",
    "        original = images[0]\n",
    "        prepped = preprocess_image(original, enhance)\n",
    "        DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "    else:\n",
    "        total = len(images)\n",
    "        for idx, original in enumerate(images, start=1):\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\n",
    "                \"path\": path,\n",
    "                \"original\": original,\n",
    "                \"prepped\": prepped,\n",
    "                \"page_index\": idx,\n",
    "                \"page_count\": total\n",
    "            })\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "for doc in DOCS:\n",
    "    original = doc[\"original\"]\n",
    "    prepped = doc[\"prepped\"]\n",
    "    path = doc[\"path\"]\n",
    "\n",
    "    display(original.convert(\"RGB\") if original.mode not in (\"RGB\",\"L\") else original)\n",
    "\n",
    "    if \"SHOW_PREPROCESSED\" not in globals() or SHOW_PREPROCESSED:\n",
    "        display(prepped.convert(\"RGB\") if prepped.mode not in (\"RGB\",\"L\") else prepped)\n",
    "\n",
    "# Keep globals aligned with the last document for backwards compatibility.\n",
    "if DOCS:\n",
    "    path = DOCS[-1][\"path\"]\n",
    "    original = DOCS[-1][\"original\"]\n",
    "    prepped = DOCS[-1][\"prepped\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bb258",
   "metadata": {},
   "source": [
    "### si image passer sur teseract ou document extraire sont contenue apartire de sont contenue code a la fin les deux => input txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0db4b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[doc] francais.docx | content=text | extraction=native:docx:xml | pages=1\n",
      "[page 1/1]\n",
      "Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE:\n",
    "# Cette cellule suppose que la cellule précédente a déjà exécuté:\n",
    "# - la détection/routage (TEXT_FILES / IMAGE_ONLY_FILES / INPUT_FILE)\n",
    "# - le preprocess + affichage (DOCS avec \"prepped\")\n",
    "# Donc ici on fait:\n",
    "# 1) OCR Tesseract UNIQUEMENT sur DOCS (images -> [info])\n",
    "# 2) Extraction NATIVE (sans OCR) sur TEXT_FILES (-> [skip] content='text')\n",
    "#\n",
    "# Objectif print:\n",
    "# - 1 seule fois, à la fin\n",
    "# - affiche: fichier, nb pages, puis texte de chaque page\n",
    "\n",
    "import uuid\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# ==================== Réglage PRINT ====================\n",
    "# False => aucune sortie pendant OCR/native\n",
    "# True  => debug pendant extraction (à éviter si tu veux 1 seul print)\n",
    "PRINT_DURING_EXTRACTION = False\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (flags tesseract pour espaces/tables) --------------------\n",
    "if \"config_flags\" in globals():\n",
    "    if \"-c preserve_interword_spaces=1\" not in config_flags:\n",
    "        config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "    if \"-c textord_tabfind_find_tables=1\" not in config_flags:\n",
    "        config_flags.append(\"-c textord_tabfind_find_tables=1\")\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (reconstruction layout via TSV) --------------------\n",
    "def _median(values):\n",
    "    values = sorted(values)\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    mid = n // 2\n",
    "    if n % 2 == 1:\n",
    "        return values[mid]\n",
    "    return (values[mid - 1] + values[mid]) / 2.0\n",
    "\n",
    "def _estimate_char_metrics(data: dict):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    ws = data.get(\"width\", [])\n",
    "    hs = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        w = int(ws[i]) if i < len(ws) else 0\n",
    "        h = int(hs[i]) if i < len(hs) else 0\n",
    "        if h > 0:\n",
    "            heights.append(h)\n",
    "\n",
    "        L = len(s)\n",
    "        if w > 0 and L > 0:\n",
    "            widths.append(w / float(L))\n",
    "\n",
    "    char_w = _median(widths) or 10.0\n",
    "    line_h = _median(heights) or 20.0\n",
    "\n",
    "    if char_w <= 1:\n",
    "        char_w = 10.0\n",
    "    if line_h <= 1:\n",
    "        line_h = 20.0\n",
    "\n",
    "    return float(char_w), float(line_h)\n",
    "\n",
    "def _render_layout_from_data(data: dict, img_w: int, img_h: int) -> str:\n",
    "    char_w, line_h = _estimate_char_metrics(data)\n",
    "    line_tol = max(6.0, line_h * 0.55)\n",
    "\n",
    "    items = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    lefts = data.get(\"left\", [])\n",
    "    tops = data.get(\"top\", [])\n",
    "    widths = data.get(\"width\", [])\n",
    "    heights = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        l = int(lefts[i]) if i < len(lefts) else 0\n",
    "        tp = int(tops[i]) if i < len(tops) else 0\n",
    "        w = int(widths[i]) if i < len(widths) else 0\n",
    "        h = int(heights[i]) if i < len(heights) else 0\n",
    "\n",
    "        items.append({\"text\": s, \"left\": l, \"top\": tp, \"right\": l + w, \"height\": h})\n",
    "\n",
    "    items.sort(key=lambda x: (x[\"top\"], x[\"left\"]))\n",
    "\n",
    "    lines = []\n",
    "    for it in items:\n",
    "        placed = False\n",
    "        if lines and abs(it[\"top\"] - lines[-1][\"top\"]) <= line_tol:\n",
    "            lines[-1][\"words\"].append(it)\n",
    "            lines[-1][\"top\"] = min(lines[-1][\"top\"], it[\"top\"])\n",
    "            placed = True\n",
    "        if not placed:\n",
    "            for ln in reversed(lines):\n",
    "                if abs(it[\"top\"] - ln[\"top\"]) <= line_tol:\n",
    "                    ln[\"words\"].append(it)\n",
    "                    ln[\"top\"] = min(ln[\"top\"], it[\"top\"])\n",
    "                    placed = True\n",
    "                    break\n",
    "        if not placed:\n",
    "            lines.append({\"top\": it[\"top\"], \"words\": [it]})\n",
    "\n",
    "    lines.sort(key=lambda ln: ln[\"top\"])\n",
    "\n",
    "    out_lines = []\n",
    "    prev_row = None\n",
    "\n",
    "    for ln in lines:\n",
    "        words = sorted(ln[\"words\"], key=lambda x: x[\"left\"])\n",
    "        row = int(round(ln[\"top\"] / line_h)) if line_h > 0 else 0\n",
    "        if prev_row is not None:\n",
    "            gap = row - prev_row\n",
    "            if gap > 1:\n",
    "                for _ in range(gap - 1):\n",
    "                    out_lines.append(\"\")\n",
    "        prev_row = row\n",
    "\n",
    "        line_str = \"\"\n",
    "        cursor = 0\n",
    "        for w in words:\n",
    "            col = int(round(w[\"left\"] / char_w)) if char_w > 0 else 0\n",
    "            if col < 0:\n",
    "                col = 0\n",
    "\n",
    "            if cursor == 0 and not line_str:\n",
    "                if col > 0:\n",
    "                    line_str += \" \" * col\n",
    "                    cursor = col\n",
    "            else:\n",
    "                needed = col - cursor\n",
    "                if needed <= 0:\n",
    "                    needed = 1\n",
    "                line_str += \" \" * needed\n",
    "                cursor += needed\n",
    "\n",
    "            line_str += w[\"text\"]\n",
    "            cursor += len(w[\"text\"])\n",
    "\n",
    "        out_lines.append(line_str)\n",
    "\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "# -------------------- OCR --------------------\n",
    "config = build_config(\n",
    "    args.oem,\n",
    "    args.psm,\n",
    "    config_flags,\n",
    "    args.dpi,\n",
    "    args.tessdata_dir,\n",
    "    args.user_words,\n",
    "    args.user_patterns,\n",
    ")\n",
    "\n",
    "if \"DOCS\" not in globals():\n",
    "    DOCS = []\n",
    "\n",
    "def _basename(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return Path(val).name\n",
    "    except Exception:\n",
    "        s = str(val)\n",
    "        return s.replace(\"\\\\\", \"/\").split(\"/\")[-1]\n",
    "\n",
    "# If DOCS is a list of pages (legacy), group into document-level objects\n",
    "if DOCS and isinstance(DOCS[0], dict) and \"pages\" not in DOCS[0]:\n",
    "    groups = {}\n",
    "    for i, page in enumerate(DOCS, start=1):\n",
    "        raw = str(page.get(\"path\") or \"batch\")\n",
    "        key = f\"{raw}::p{page.get('page_index') or i}\"\n",
    "        groups.setdefault(key, []).append(page)\n",
    "\n",
    "    packed = []\n",
    "    for key, pages in groups.items():\n",
    "        pages_sorted = sorted(pages, key=lambda p: int(p.get(\"page_index\") or 0)) if pages else []\n",
    "\n",
    "        source_files = [_basename(p.get(\"path\")) for p in pages_sorted if _basename(p.get(\"path\"))]\n",
    "        source_files = list(dict.fromkeys(source_files))\n",
    "\n",
    "        filename = source_files[0] if len(source_files) == 1 else (_basename(key) or \"batch\")\n",
    "\n",
    "        doc = {\"doc_id\": str(uuid.uuid4()), \"filename\": filename, \"source_files\": source_files, \"pages\": []}\n",
    "        page_index = 1\n",
    "        for p in pages_sorted:\n",
    "            idx = int(p.get(\"page_index\") or page_index)\n",
    "            src_path = p.get(\"path\")\n",
    "            doc[\"pages\"].append({\n",
    "                \"page_index\": idx,\n",
    "                \"image\": p.get(\"original\"),\n",
    "                \"prepped\": p.get(\"prepped\"),\n",
    "                \"source_path\": src_path,\n",
    "                \"source_file\": _basename(src_path)\n",
    "            })\n",
    "            page_index += 1\n",
    "        doc[\"page_count_total\"] = len(doc[\"pages\"])\n",
    "        packed.append(doc)\n",
    "\n",
    "    DOCS = packed\n",
    "\n",
    "# Ensure doc-level metadata consistency (even if DOCS already has pages)\n",
    "for doc in DOCS:\n",
    "    pages = doc.get(\"pages\", []) or []\n",
    "    for i, page in enumerate(pages, start=1):\n",
    "        if not page.get(\"page_index\"):\n",
    "            page[\"page_index\"] = i\n",
    "        if not page.get(\"source_file\"):\n",
    "            src_path = page.get(\"source_path\") or page.get(\"path\")\n",
    "            page[\"source_file\"] = _basename(src_path)\n",
    "\n",
    "    doc[\"page_count_total\"] = len(pages)\n",
    "\n",
    "    if not doc.get(\"source_files\"):\n",
    "        source_files = [p.get(\"source_file\") for p in pages if p.get(\"source_file\")]\n",
    "        doc[\"source_files\"] = list(dict.fromkeys(source_files))\n",
    "\n",
    "    if not doc.get(\"filename\"):\n",
    "        if len(doc.get(\"source_files\", [])) == 1:\n",
    "            doc[\"filename\"] = doc[\"source_files\"][0]\n",
    "        elif len(doc.get(\"source_files\", [])) > 1:\n",
    "            doc[\"filename\"] = \"batch\"\n",
    "\n",
    "for doc in DOCS:\n",
    "    pages_text = []\n",
    "    for page in doc.get(\"pages\", []):\n",
    "        prepped = page.get(\"prepped\")\n",
    "        if prepped is None:\n",
    "            raise RuntimeError(\"prepped image missing. Run the input/preprocess cell first.\")\n",
    "\n",
    "        data = pytesseract.image_to_data(prepped, lang=args.lang, config=config, output_type=Output.DICT)\n",
    "        w, h = prepped.size\n",
    "        OCR_TEXT = _render_layout_from_data(data, w, h)\n",
    "\n",
    "        page[\"ocr_text\"] = OCR_TEXT\n",
    "        pages_text.append(OCR_TEXT)\n",
    "\n",
    "        if PRINT_DURING_EXTRACTION:\n",
    "            src = page.get(\"source_file\") or _basename(page.get(\"source_path\")) or \"\"\n",
    "            total = doc.get(\"page_count_total\", 1)\n",
    "            print(f\"[ocr] {doc.get('filename')} | file={src} | page {page.get('page_index')}/{total}\")\n",
    "            print(OCR_TEXT)\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "    doc[\"pages_text\"] = pages_text\n",
    "    doc[\"ocr_text\"] = \"\\n\\n\".join(pages_text)\n",
    "\n",
    "# Backwards compatibility\n",
    "if DOCS:\n",
    "    OCR_TEXT = DOCS[-1].get(\"ocr_text\", \"\")\n",
    "\n",
    "# -------------------- EXTRACTION NATIVE POUR [skip] (TEXT_FILES) --------------------\n",
    "def _get_pdf_reader_with_name():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader, \"pypdf\"\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader, \"PyPDF2\"\n",
    "        except ImportError:\n",
    "            return None, \"none\"\n",
    "\n",
    "# AJOUT MINIMAL: tenter un mode \"layout\" si dispo, sinon fallback normal\n",
    "def _pdf_extract_text_preserve_layout(page) -> str:\n",
    "    try:\n",
    "        return page.extract_text(extraction_mode=\"layout\") or \"\"\n",
    "    except TypeError:\n",
    "        return page.extract_text() or \"\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            return page.extract_text() or \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def _docx_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.findall(\".//w:p\", ns):\n",
    "        line_parts = []\n",
    "        for node in p.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                line_parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}tab\"):\n",
    "                line_parts.append(\"\\t\")\n",
    "            elif tag.endswith(\"}br\") or tag.endswith(\"}cr\"):\n",
    "                line_parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(line_parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _pptx_slide_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\n",
    "        \"a\": \"http://schemas.openxmlformats.org/drawingml/2006/main\",\n",
    "        \"p\": \"http://schemas.openxmlformats.org/presentationml/2006/main\",\n",
    "    }\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for para in root.findall(\".//a:p\", ns):\n",
    "        parts = []\n",
    "        for node in para.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}br\"):\n",
    "                parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _xlsx_col_to_index(col_letters: str) -> int:\n",
    "    n = 0\n",
    "    for ch in col_letters:\n",
    "        if \"A\" <= ch <= \"Z\":\n",
    "            n = n * 26 + (ord(ch) - ord(\"A\") + 1)\n",
    "    return n\n",
    "\n",
    "def _xlsx_shared_strings(xml_bytes: bytes) -> list:\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    out = []\n",
    "    for si in root.findall(\".//s:si\", ns):\n",
    "        parts = []\n",
    "        for t in si.findall(\".//s:t\", ns):\n",
    "            parts.append(t.text if t.text is not None else \"\")\n",
    "        out.append(\"\".join(parts))\n",
    "    return out\n",
    "\n",
    "def _xlsx_sheet_to_text(sheet_xml: bytes, shared: list) -> str:\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    root = ET.fromstring(sheet_xml)\n",
    "\n",
    "    lines = []\n",
    "    for row in root.findall(\".//s:row\", ns):\n",
    "        cells = row.findall(\"./s:c\", ns)\n",
    "        row_map = {}\n",
    "        max_col = 0\n",
    "\n",
    "        for c in cells:\n",
    "            r = c.get(\"r\") or \"\"\n",
    "            col_letters = \"\".join([ch for ch in r if ch.isalpha()]).upper()\n",
    "            col_idx = _xlsx_col_to_index(col_letters) if col_letters else 0\n",
    "            if col_idx > max_col:\n",
    "                max_col = col_idx\n",
    "\n",
    "            cell_type = c.get(\"t\")\n",
    "            v = c.find(\"./s:v\", ns)\n",
    "            is_node = c.find(\"./s:is\", ns)\n",
    "\n",
    "            val = \"\"\n",
    "            if cell_type == \"s\" and v is not None and v.text is not None:\n",
    "                try:\n",
    "                    val = shared[int(v.text)]\n",
    "                except Exception:\n",
    "                    val = v.text\n",
    "            elif cell_type == \"inlineStr\" and is_node is not None:\n",
    "                parts = []\n",
    "                for t in is_node.findall(\".//s:t\", ns):\n",
    "                    parts.append(t.text if t.text is not None else \"\")\n",
    "                val = \"\".join(parts)\n",
    "            else:\n",
    "                if v is not None and v.text is not None:\n",
    "                    val = v.text\n",
    "\n",
    "            row_map[col_idx] = val\n",
    "\n",
    "        if max_col <= 0:\n",
    "            lines.append(\"\")\n",
    "        else:\n",
    "            parts = []\n",
    "            for i in range(1, max_col + 1):\n",
    "                parts.append(row_map.get(i, \"\"))\n",
    "            lines.append(\"\\t\".join(parts))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _odf_content_to_text(xml_bytes: bytes) -> str:\n",
    "    ns_text = \"urn:oasis:names:tc:opendocument:xmlns:text:1.0\"\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    def walk(node):\n",
    "        pieces = []\n",
    "        if node.text is not None:\n",
    "            pieces.append(node.text)\n",
    "\n",
    "        for child in list(node):\n",
    "            tag = child.tag\n",
    "            if tag == f\"{{{ns_text}}}s\":\n",
    "                c = child.get(f\"{{{ns_text}}}c\") or child.get(\"c\") or \"1\"\n",
    "                try:\n",
    "                    pieces.append(\" \" * int(c))\n",
    "                except Exception:\n",
    "                    pieces.append(\" \")\n",
    "            else:\n",
    "                pieces.append(walk(child))\n",
    "\n",
    "            if child.tail is not None:\n",
    "                pieces.append(child.tail)\n",
    "        return \"\".join(pieces)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.iter():\n",
    "        if p.tag == f\"{{{ns_text}}}p\":\n",
    "            out_lines.append(walk(p))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _html_bytes_to_text_preserve(b: bytes) -> str:\n",
    "    b = re.sub(rb\"(?i)<br\\s*/?>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"(?i)</p\\s*>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "    try:\n",
    "        return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return str(b)\n",
    "\n",
    "def extract_text_native(path: str) -> dict:\n",
    "    ft = detect_path_type(path)  # défini dans cellule précédente\n",
    "    ext = ft.ext.lower()\n",
    "    filename = Path(path).name\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        PdfReader, backend = _get_pdf_reader_with_name()\n",
    "        if PdfReader is not None:\n",
    "            reader = PdfReader(path)\n",
    "            pages = reader.pages\n",
    "            pages_text = []\n",
    "            total = len(pages)\n",
    "\n",
    "            for i, page in enumerate(pages, start=1):\n",
    "                # MODIF MINIMALE: extraction \"layout\" si dispo => garde mieux espaces/sauts de ligne\n",
    "                txt = _pdf_extract_text_preserve_layout(page)\n",
    "                pages_text.append(txt)\n",
    "                if PRINT_DURING_EXTRACTION:\n",
    "                    print(f\"[native:{backend}] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": f\"native:pdf:{backend}\",\n",
    "                \"text\": full,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "\n",
    "        # Fallback pdfminer\n",
    "        try:\n",
    "            from pdfminer.high_level import extract_text  # type: ignore\n",
    "            full = extract_text(path) or \"\"\n",
    "            pages = full.split(\"\\f\")\n",
    "            pages_text = [p for p in pages]  # garder brut\n",
    "            total = len(pages_text)\n",
    "\n",
    "            if PRINT_DURING_EXTRACTION:\n",
    "                for i, txt in enumerate(pages_text, start=1):\n",
    "                    print(f\"[native:pdfminer] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full2 = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:pdfminer\",\n",
    "                \"text\": full2,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:none\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "            }\n",
    "\n",
    "    # Office/OpenDocument/EPUB\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = z.namelist()\n",
    "\n",
    "                if ext == \".docx\":\n",
    "                    parts = []\n",
    "                    if \"word/document.xml\" in names:\n",
    "                        parts.append(_docx_xml_to_text(z.read(\"word/document.xml\")))\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                        if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(parts)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:docx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],      # docx: pas de \"pages\" fiables => 1 bloc\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".xlsx\":\n",
    "                    shared = []\n",
    "                    if \"xl/sharedStrings.xml\" in names:\n",
    "                        try:\n",
    "                            shared = _xlsx_shared_strings(z.read(\"xl/sharedStrings.xml\"))\n",
    "                        except Exception:\n",
    "                            shared = []\n",
    "\n",
    "                    sheet_files = [nm for nm in names if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\")]\n",
    "                    sheet_files_sorted = sorted(sheet_files)\n",
    "\n",
    "                    pages_text = []\n",
    "                    total = len(sheet_files_sorted)\n",
    "                    for nm in sheet_files_sorted:\n",
    "                        sheet_text = _xlsx_sheet_to_text(z.read(nm), shared)\n",
    "                        pages_text.append(sheet_text)\n",
    "\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:xlsx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # sheets = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext == \".pptx\":\n",
    "                    slides = [nm for nm in names if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\")]\n",
    "                    slides_sorted = sorted(slides)\n",
    "                    pages_text = []\n",
    "                    total = len(slides_sorted)\n",
    "                    for nm in slides_sorted:\n",
    "                        pages_text.append(_pptx_slide_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:pptx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # slides = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                    text = \"\"\n",
    "                    if \"content.xml\" in names:\n",
    "                        text = _odf_content_to_text(z.read(\"content.xml\"))\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": f\"native:{ext[1:]}:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".epub\":\n",
    "                    htmls = [nm for nm in names if nm.lower().endswith((\".xhtml\", \".html\", \".htm\"))]\n",
    "                    htmls_sorted = sorted(htmls)\n",
    "                    pages_text = []\n",
    "                    total = len(htmls_sorted)\n",
    "                    for nm in htmls_sorted:\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            b = b\"\"\n",
    "                        pages_text.append(_html_bytes_to_text_preserve(b))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:epub:html\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:zip:error\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": str(uuid.uuid4()),\n",
    "        \"filename\": filename,\n",
    "        \"source_path\": path,\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": \"native:unsupported\",\n",
    "        \"text\": \"\",\n",
    "        \"pages_text\": [\"\"],\n",
    "        \"page_count_total\": 1,\n",
    "    }\n",
    "\n",
    "# TEXT_FILES vient de la cellule précédente (celle qui a fait les [skip])\n",
    "TEXT_DOCS: List[dict] = []\n",
    "if \"TEXT_FILES\" not in globals():\n",
    "    TEXT_FILES = []\n",
    "\n",
    "for p in TEXT_FILES:\n",
    "    try:\n",
    "        TEXT_DOCS.append(extract_text_native(p))\n",
    "    except Exception as e:\n",
    "        TEXT_DOCS.append({\n",
    "            \"doc_id\": str(uuid.uuid4()),\n",
    "            \"filename\": Path(p).name,\n",
    "            \"source_path\": p,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": \"native:error\",\n",
    "            \"text\": \"\",\n",
    "            \"pages_text\": [\"\"],\n",
    "            \"page_count_total\": 1,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "# -------------------- SORTIE FINALE (OCR + NATIVE) --------------------\n",
    "FINAL_DOCS: List[dict] = []\n",
    "\n",
    "# OCR docs (images)\n",
    "for d in DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"image_only\",\n",
    "        \"extraction\": \"ocr:tesseract\",\n",
    "        \"text\": d.get(\"ocr_text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "# Native docs (text)\n",
    "for d in TEXT_DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or [d.get(\"text\") or \"\"]\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": d.get(\"extraction\"),\n",
    "        \"text\": d.get(\"text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "for d in FINAL_DOCS:\n",
    "    filename = d.get(\"filename\")\n",
    "    content = d.get(\"content\")\n",
    "    extraction = d.get(\"extraction\")\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    total = int(d.get(\"page_count_total\") or len(pages_text) or 1)\n",
    "\n",
    "    print(f\"[doc] {filename} | content={content} | extraction={extraction} | pages={total}\")\n",
    "\n",
    "    if not pages_text:\n",
    "        print(\"\")\n",
    "        print(\"\\n\" + (\"-\" * 120) + \"\\n\")\n",
    "        continue\n",
    "\n",
    "    for i, txt in enumerate(pages_text, start=1):\n",
    "        print(f\"[page {i}/{total}]\")\n",
    "        print(txt if txt is not None else \"\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097c827",
   "metadata": {},
   "source": [
    "### Tokenisation \"layout\" (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c54db6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "[doc] francais.docx\n",
      "  doc_id       : 96d0060a-70ad-4515-90c7-ed5efebc86d4\n",
      "  content      : text\n",
      "  extraction   : native:docx:xml\n",
      "  pages_total  : 1\n",
      "  chars_total  : 1706\n",
      "  recompose_ok : False\n",
      "  paths:\n",
      "    - c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[page 1/1] source_path=c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "  lang         : fr\n",
      "  chars        : 1706\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  sentences_layout: 1 chunks total | sentences=1 | noise=0 | showing 1/1 (filter_is_sentence=True, fallback=False, min_nonspace=12)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[sent 1/1] page=1 start=0 end=1706 line=1 col=0 chars=1706 nonspace=1442 is_noise=False is_sentence=True layout=table\n",
      "Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# ==================== Réglages ====================\n",
    "TARGET = None\n",
    "\n",
    "PRINT_SENTENCES = True\n",
    "MAX_SENTENCES_PREVIEW = 80   # None => imprime tout\n",
    "PRINT_REPR = False           # True => debug espaces invisibles via repr(chunk)\n",
    "\n",
    "MIN_SENTENCE_NONSPACE = 12\n",
    "PRINT_ONLY_SENTENCES = True\n",
    "PRINT_PAGE_TEXT = False\n",
    "\n",
    "# ==================== NLTK data ====================\n",
    "def _ensure_nltk():\n",
    "    for pkg, probe in ((\"punkt\", \"tokenizers/punkt\"), (\"punkt_tab\", \"tokenizers/punkt_tab\")):\n",
    "        try:\n",
    "            nltk.data.find(probe)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] NLTK download failed for {pkg}: {e}\")\n",
    "\n",
    "_ensure_nltk()\n",
    "\n",
    "# ==================== Détection langue (simple) ====================\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "# ==================== Sentence split \"layout\" (fallback) ====================\n",
    "_AR_END_RE = re.compile(r\"([.!?؟]+)(\\s+|$)\", flags=re.UNICODE)\n",
    "\n",
    "def split_ar_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    last = 0\n",
    "    for m in _AR_END_RE.finditer(text):\n",
    "        end = m.end()\n",
    "        chunks.append(text[last:end])\n",
    "        last = end\n",
    "    if last < len(text):\n",
    "        chunks.append(text[last:])\n",
    "    return chunks\n",
    "\n",
    "def _load_punkt_pickle(lang_pickle_name: str):\n",
    "    p = nltk.data.find(f\"tokenizers/punkt/{lang_pickle_name}.pickle\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def split_punkt_layout(text: str, lang_pickle_name: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    tok = _load_punkt_pickle(lang_pickle_name)\n",
    "    spans = list(tok.span_tokenize(text))\n",
    "    if not spans:\n",
    "        return [text]\n",
    "    starts = [0] + [spans[i][0] for i in range(1, len(spans))]\n",
    "    ends = [spans[i+1][0] for i in range(len(spans)-1)] + [len(text)]\n",
    "    return [text[starts[i]:ends[i]] for i in range(len(ends))]\n",
    "\n",
    "def sentence_chunks_layout(text: str, lang: str):\n",
    "    lang = (lang or \"\").lower()\n",
    "    if lang.startswith(\"ar\"):\n",
    "        return split_ar_layout(text)\n",
    "    if lang.startswith(\"fr\"):\n",
    "        return split_punkt_layout(text, \"french\")\n",
    "    if lang.startswith(\"en\"):\n",
    "        return split_punkt_layout(text, \"english\")\n",
    "    return split_punkt_layout(text, \"english\")\n",
    "\n",
    "# ==================== Split sections/alinéas (layout-preserving) ====================\n",
    "def _iter_line_spans(text: str):\n",
    "    if not text:\n",
    "        return\n",
    "    start = 0\n",
    "    for m in re.finditer(r\"\\n\", text):\n",
    "        end = m.end()\n",
    "        yield start, end\n",
    "        start = end\n",
    "    if start < len(text):\n",
    "        yield start, len(text)\n",
    "\n",
    "def _collapse_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def _mask_digits(s: str) -> str:\n",
    "    return re.sub(r\"\\d\", \"#\", s)\n",
    "\n",
    "_NUM_SIMPLE_RE = re.compile(r\"(?i)^[ \\t]*\\(?\\d{1,3}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ALPHA_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[a-z]\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ROMAN_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[ivxlcdm]{1,8}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_NUM_MULTI_RE  = re.compile(r\"^[ \\t]*\\d{1,3}(?:\\.\\d{1,3})+[ \\t]*(?:[.)])?[ \\t]+(?=\\S)\")\n",
    "\n",
    "_KEYWORD_STRONG_RE = re.compile(r\"(?i)^[ \\t]*(article|section|chapitre|chapter|part)\\b\")\n",
    "_KEYWORD_WEAK_HEADING_RE = re.compile(\n",
    "    r\"\"\"(?ix)^[ \\t]*\n",
    "    (schedule|exhibit|appendix|annexe|annex)\n",
    "    [ \\t]+\n",
    "    ([A-Z0-9]{1,8}|[ivxlcdm]{1,8}|\\d{1,3})\n",
    "    [ \\t]*\n",
    "    (?:[:\\-–—][ \\t]*\\S.*)?\n",
    "    [ \\t]*$\n",
    "    \"\"\"\n",
    ")\n",
    "_SEP_RE = re.compile(r\"^[ \\t]*[-_]{4,}[ \\t]*$\")\n",
    "\n",
    "_LABEL_ONLY_RE = re.compile(\n",
    "    r\"(?is)^[ \\t]*\"\n",
    "    r\"(?:\\(?\\d{1,3}\\)?|\\(?[a-z]\\)?|\\(?[ivxlcdm]{1,8}\\)?)\"\n",
    "    r\"[ \\t]*[.)][ \\t]*$\"\n",
    ")\n",
    "\n",
    "def _is_section_start_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _KEYWORD_STRONG_RE.match(s):\n",
    "        return True\n",
    "    if _KEYWORD_WEAK_HEADING_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_SIMPLE_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_MULTI_RE.match(s):\n",
    "        label = _collapse_ws(s).split(\" \", 1)[0]\n",
    "        parts = label.split(\".\")\n",
    "        if len(parts) >= 2 and parts[-1] in (\"00\", \"000\"):\n",
    "            return False\n",
    "        return True\n",
    "    if _ALPHA_RE.match(s) or _ROMAN_RE.match(s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _merge_label_only(chunks):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        if i + 1 < len(chunks) and _LABEL_ONLY_RE.match(chunks[i]):\n",
    "            out.append(chunks[i] + chunks[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(chunks[i])\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def split_sections_layout(text: str, allow_alpha_roman: bool = True):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = {0}\n",
    "    for ls, le in _iter_line_spans(text):\n",
    "        line = text[ls:le]\n",
    "        if _is_section_start_line(line):\n",
    "            if not allow_alpha_roman:\n",
    "                s = line.rstrip(\"\\n\")\n",
    "                if (\n",
    "                    _KEYWORD_STRONG_RE.match(s)\n",
    "                    or _KEYWORD_WEAK_HEADING_RE.match(s)\n",
    "                    or _NUM_SIMPLE_RE.match(s)\n",
    "                    or _NUM_MULTI_RE.match(s)\n",
    "                ):\n",
    "                    starts.add(ls)\n",
    "            else:\n",
    "                starts.add(ls)\n",
    "\n",
    "    starts = sorted(starts)\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        a, b = starts[i], starts[i+1]\n",
    "        if a != b:\n",
    "            chunks.append(text[a:b])\n",
    "    chunks.append(text[starts[-1]:])\n",
    "\n",
    "    return _merge_label_only(chunks)\n",
    "\n",
    "_PARA_BREAK_RE = re.compile(r\"(?:\\n[ \\t]*){2,}\")\n",
    "\n",
    "def split_paragraphs_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = [0]\n",
    "    for m in _PARA_BREAK_RE.finditer(text):\n",
    "        starts.append(m.end())\n",
    "    starts = sorted(set(starts))\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "    out = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        out.append(text[starts[i]:starts[i+1]])\n",
    "    out.append(text[starts[-1]:])\n",
    "    return out\n",
    "\n",
    "def chunk_layout_universal(text: str, lang: str):\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    lines = [text[ls:le].rstrip(\"\\n\") for ls, le in _iter_line_spans(text)]\n",
    "    num_kw_hits = 0\n",
    "    alpha_roman_hits = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        if (\n",
    "            _KEYWORD_STRONG_RE.match(ln)\n",
    "            or _KEYWORD_WEAK_HEADING_RE.match(ln)\n",
    "            or _NUM_SIMPLE_RE.match(ln)\n",
    "            or _NUM_MULTI_RE.match(ln)\n",
    "        ):\n",
    "            num_kw_hits += 1\n",
    "        elif _ALPHA_RE.match(ln) or _ROMAN_RE.match(ln):\n",
    "            alpha_roman_hits += 1\n",
    "\n",
    "    is_structured = (num_kw_hits >= 2) or (alpha_roman_hits >= 3)\n",
    "\n",
    "    if is_structured:\n",
    "        chunks = split_sections_layout(text, allow_alpha_roman=True)\n",
    "        if len(chunks) > 1:\n",
    "            return chunks\n",
    "\n",
    "    paras = split_paragraphs_layout(text)\n",
    "    if len(paras) > 1:\n",
    "        return paras\n",
    "\n",
    "    return sentence_chunks_layout(text, lang)\n",
    "\n",
    "# ======================================================================\n",
    "#  MULTI-COLONNES (général, robuste) + TABLE (inchangé)\n",
    "#  + micro-table: interpréter les headers multi-col comme un \"table chunk\"\n",
    "# ======================================================================\n",
    "\n",
    "GAP_MIN_OCR = 10\n",
    "GAP_MIN_NATIVE = 6\n",
    "\n",
    "MERGE_COL_DIST_OCR = 22\n",
    "MERGE_COL_DIST_NATIVE = 16\n",
    "\n",
    "MICROTABLE_MAX_ROWS = 30\n",
    "MICROTABLE_MIN_DENS = 0.25\n",
    "MICROTABLE_MIN_MULTIROW = 2\n",
    "\n",
    "TABLE_HINT_RE = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    \\b(\n",
    "        qt[ée]|\n",
    "        désignation|designation|\n",
    "        prix|\n",
    "        montan?t|\n",
    "        r[ée]f[ée]rence|reference|\n",
    "        description|\n",
    "        quantit[ée]|\n",
    "        p\\.?\\s*unitaire|\n",
    "        valeur|\n",
    "        total\\s*ht|total|\n",
    "        tva|vat\n",
    "    )\\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d+(?:[.,]\\d+)?\")\n",
    "DEC_RE = re.compile(r\"\\d+[.,]\\d+\")\n",
    "\n",
    "def _space_runs_ge(s: str, n: int):\n",
    "    return [(m.start(), m.end()) for m in re.finditer(r\"[ ]{%d,}\" % n, s or \"\")]\n",
    "\n",
    "def _has_big_gap(s: str, gap_min: int, min_count: int = 1) -> bool:\n",
    "    return len(_space_runs_ge(s, gap_min)) >= min_count\n",
    "\n",
    "def _num_tokens(s: str) -> int:\n",
    "    return len(NUM_RE.findall(s or \"\"))\n",
    "\n",
    "def _dec_tokens(s: str) -> int:\n",
    "    return len(DEC_RE.findall(s or \"\"))\n",
    "\n",
    "def _is_table_line(line: str, gap_min: int) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return False\n",
    "    if s.count(\"\\t\") >= 2:\n",
    "        return True\n",
    "    if TABLE_HINT_RE.search(s):\n",
    "        return True\n",
    "    if _has_big_gap(s, gap_min, min_count=2):\n",
    "        if _num_tokens(s) >= 3:\n",
    "            return True\n",
    "        if _dec_tokens(s) >= 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _cluster_centers(values, tol=2, min_hits=1):\n",
    "    if not values:\n",
    "        return []\n",
    "    xs = sorted(values)\n",
    "    clusters = []\n",
    "    cur = [xs[0]]\n",
    "    for v in xs[1:]:\n",
    "        if abs(v - cur[-1]) <= tol:\n",
    "            cur.append(v)\n",
    "        else:\n",
    "            clusters.append(cur)\n",
    "            cur = [v]\n",
    "    clusters.append(cur)\n",
    "\n",
    "    centers = []\n",
    "    for c in clusters:\n",
    "        if len(c) >= min_hits:\n",
    "            c2 = sorted(c)\n",
    "            centers.append(c2[len(c2)//2])\n",
    "    return sorted(set(centers))\n",
    "\n",
    "def _upper_ratio(s: str) -> float:\n",
    "    letters = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s or \"\")\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for ch in letters if ch.isupper())\n",
    "    return upp / max(1, len(letters))\n",
    "\n",
    "def _sep_spans(line: str, gap_min: int):\n",
    "    s = line or \"\"\n",
    "    spans = []\n",
    "    for m in re.finditer(r\"\\t+\", s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    for m in re.finditer(r\"[ ]{%d,}\" % gap_min, s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    if not spans:\n",
    "        return []\n",
    "    spans.sort()\n",
    "    merged = [spans[0]]\n",
    "    for a, b in spans[1:]:\n",
    "        la, lb = merged[-1]\n",
    "        if a <= lb:\n",
    "            merged[-1] = (la, max(lb, b))\n",
    "        else:\n",
    "            merged.append((a, b))\n",
    "    return merged\n",
    "\n",
    "def _line_segments_by_gaps(line: str, gap_min: int):\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return []\n",
    "    seps = _sep_spans(s, gap_min)\n",
    "    segs = []\n",
    "    prev = 0\n",
    "    cuts = seps + [(len(s), len(s))]\n",
    "    for a, b in cuts:\n",
    "        if a < prev:\n",
    "            continue\n",
    "        chunk = s[prev:a]\n",
    "        m1 = re.search(r\"\\S\", chunk)\n",
    "        if m1:\n",
    "            l = m1.start()\n",
    "            r = len(chunk.rstrip(\" \\t\"))\n",
    "            text = chunk[l:r]\n",
    "            segs.append({\"x\": prev + l, \"a\": prev + l, \"b\": prev + r, \"text\": text})\n",
    "        prev = b\n",
    "    return segs\n",
    "\n",
    "def _looks_like_title_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\").strip()\n",
    "    if not s or len(s) > 50:\n",
    "        return False\n",
    "    if _SEP_RE.match(s):\n",
    "        return False\n",
    "    if _is_section_start_line(line):\n",
    "        return True\n",
    "    if _upper_ratio(s) >= 0.85 and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s) and not re.search(r\"\\d\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_multicol_candidate_line(line: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _is_table_line(line, gap_min):\n",
    "        return False\n",
    "\n",
    "    segs = _line_segments_by_gaps(s, gap_min)\n",
    "    if len(segs) >= 3:\n",
    "        return True\n",
    "    if len(segs) == 2:\n",
    "        if is_ocr:\n",
    "            return True\n",
    "        if _has_big_gap(s, gap_min, 1):\n",
    "            return True\n",
    "        if re.search(r\"[:#№°/\\\\\\-–—]\", s) or re.search(r\"\\d\", s):\n",
    "            return True\n",
    "        if _upper_ratio(s) >= 0.70:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "_KV_GENERIC_RE = re.compile(r\"^\\s*(?P<k>[^:]{1,80}?)\\s{2,}(?P<v>\\S.+?)\\s*$\")\n",
    "\n",
    "def _looks_like_header_pair(k: str, v: str) -> bool:\n",
    "    k2 = (k or \"\").strip()\n",
    "    v2 = (v or \"\").strip()\n",
    "    if not k2 or not v2:\n",
    "        return False\n",
    "    if len(k2) <= 25 and len(v2) <= 25 and _upper_ratio(k2) >= 0.85 and _upper_ratio(v2) >= 0.85:\n",
    "        if not re.search(r\"\\d\", k2 + v2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _looks_like_addressish(line: str) -> bool:\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if re.search(r\"(rue|route|avenue|bd|boulevard|street|st\\.|road|zip|code\\s*postal|bp)\", s, flags=re.I):\n",
    "        return True\n",
    "    if len(s) >= 10 and not s.endswith(\":\") and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _normalize_kv_generic(text: str) -> str:\n",
    "    out = []\n",
    "    for raw in (text or \"\").splitlines():\n",
    "        line = raw.rstrip(\"\\n\")\n",
    "        if not line.strip():\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if \":\" in line:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if _looks_like_addressish(line):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        m = _KV_GENERIC_RE.match(line)\n",
    "        if not m:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        k = _collapse_ws(m.group(\"k\"))\n",
    "        v = _collapse_ws(m.group(\"v\"))\n",
    "        if _looks_like_header_pair(k, v):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if not re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", k):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        out.append(f\"{k}: {v}\" if v else k)\n",
    "    return \"\\n\".join(out) + (\"\\n\" if (text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _strip_sep_lines(block_text: str) -> str:\n",
    "    if not block_text:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for ln in (block_text or \"\").splitlines():\n",
    "        if _SEP_RE.match(ln.strip()):\n",
    "            continue\n",
    "        out.append(ln.rstrip())\n",
    "    txt = \"\\n\".join(out).rstrip()\n",
    "    return txt + (\"\\n\" if (block_text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _assign_to_centers(x: int, centers, tol: int):\n",
    "    if not centers:\n",
    "        return 0\n",
    "    best_i = 0\n",
    "    best_d = abs(x - centers[0])\n",
    "    for i in range(1, len(centers)):\n",
    "        d = abs(x - centers[i])\n",
    "        if d < best_d:\n",
    "            best_d = d\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def _merge_close_columns(centers, row_cells, merge_dist: int):\n",
    "    i = 0\n",
    "    while i < len(centers) - 1:\n",
    "        if (centers[i+1] - centers[i]) <= merge_dist:\n",
    "            both = 0\n",
    "            alone_next = 0\n",
    "            for r in row_cells:\n",
    "                hi = i in r\n",
    "                hj = (i+1) in r\n",
    "                if hj and hi:\n",
    "                    both += 1\n",
    "                elif hj and not hi:\n",
    "                    alone_next += 1\n",
    "            if both >= 1 and alone_next <= max(1, int(0.2 * (both + alone_next))):\n",
    "                for r in row_cells:\n",
    "                    if (i+1) in r:\n",
    "                        t2, sp2 = r.pop(i+1)\n",
    "                        if i in r:\n",
    "                            t1, sp1 = r[i]\n",
    "                            r[i] = ((t1 + \"  \" + t2).strip(), sp1 + sp2)\n",
    "                        else:\n",
    "                            r[i] = (t2, sp2)\n",
    "\n",
    "                centers.pop(i+1)\n",
    "\n",
    "                for r in row_cells:\n",
    "                    ks = sorted([k for k in r.keys() if k > i+1])\n",
    "                    for k in ks:\n",
    "                        r[k-1] = r.pop(k)\n",
    "                continue\n",
    "        i += 1\n",
    "    return centers, row_cells\n",
    "\n",
    "def _is_grid_like(row_cells, col_count: int):\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if not rows:\n",
    "        return False\n",
    "    n_rows = len(rows)\n",
    "    if n_rows > 5:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / n_rows\n",
    "    return dens >= 0.70\n",
    "\n",
    "def _is_micro_table_like(row_cells, col_count: int) -> bool:\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if len(rows) < 2:\n",
    "        return False\n",
    "    if len(rows) > MICROTABLE_MAX_ROWS:\n",
    "        return False\n",
    "    multi = sum(1 for r in rows if len(r) >= 2)\n",
    "    if multi < MICROTABLE_MIN_MULTIROW:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / max(1, len(rows))\n",
    "    return dens >= MICROTABLE_MIN_DENS\n",
    "\n",
    "def _transpose_or_group_multicol(block_text: str, abs_start: int, gap_min: int, is_ocr: bool):\n",
    "    lines = []\n",
    "    segs_by_line = []\n",
    "\n",
    "    for ls, le in _iter_line_spans(block_text):\n",
    "        line_full = block_text[ls:le]\n",
    "        s = line_full[:-1] if line_full.endswith(\"\\n\") else line_full\n",
    "\n",
    "        lines.append((ls, le, line_full, s))\n",
    "\n",
    "        if _SEP_RE.match(s.strip()):\n",
    "            segs_by_line.append([])\n",
    "            continue\n",
    "\n",
    "        segs = _line_segments_by_gaps(s, gap_min)\n",
    "        segs = [g for g in segs if g.get(\"text\", \"\").strip()]\n",
    "        segs_by_line.append(segs)\n",
    "\n",
    "    xs = []\n",
    "    for segs in segs_by_line:\n",
    "        for g in segs:\n",
    "            txt = g[\"text\"].strip()\n",
    "            if len(txt) == 1 and txt in (\":\", \"|\", \"-\", \"_\"):\n",
    "                continue\n",
    "            xs.append(int(g[\"x\"]))\n",
    "\n",
    "    if not xs:\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    tol_cluster = 3 if is_ocr else 2\n",
    "    centers = _cluster_centers(xs, tol=tol_cluster, min_hits=1)\n",
    "    min_x = min(xs)\n",
    "    if min_x not in centers:\n",
    "        centers = sorted([min_x] + centers)\n",
    "    centers = centers[:8]\n",
    "\n",
    "    tol_assign = 6 if is_ocr else 4\n",
    "    row_cells = []\n",
    "    for (ls, le, line_full, s), segs in zip(lines, segs_by_line):\n",
    "        r = {}\n",
    "        for g in segs:\n",
    "            ci = _assign_to_centers(int(g[\"x\"]), centers, tol_assign)\n",
    "            a = abs_start + ls + int(g[\"a\"])\n",
    "            b = abs_start + ls + int(g[\"b\"])\n",
    "            txt = g[\"text\"].strip()\n",
    "\n",
    "            if ci in r:\n",
    "                t0, sp0 = r[ci]\n",
    "                r[ci] = ((t0 + \" \" + txt).strip(), sp0 + [(a, b)])\n",
    "            else:\n",
    "                r[ci] = (txt, [(a, b)])\n",
    "        row_cells.append(r)\n",
    "\n",
    "    merge_dist = MERGE_COL_DIST_OCR if is_ocr else MERGE_COL_DIST_NATIVE\n",
    "    centers, row_cells = _merge_close_columns(centers, row_cells, merge_dist=merge_dist)\n",
    "    col_count = len(centers)\n",
    "\n",
    "    if _is_micro_table_like(row_cells, col_count):\n",
    "        table_rows = []\n",
    "        for (ls, le, line_full, s) in lines:\n",
    "            table_rows.append({\"text\": line_full, \"spans\": [(abs_start + ls, abs_start + le)]})\n",
    "\n",
    "        table_cells = []\n",
    "        for r in row_cells:\n",
    "            row = []\n",
    "            for ci in range(col_count):\n",
    "                if ci in r:\n",
    "                    t, sp = r[ci]\n",
    "                    row.append({\"col\": ci, \"text\": t, \"spans\": [(int(a), int(b)) for a, b in sp if b > a]})\n",
    "                else:\n",
    "                    row.append({\"col\": ci, \"text\": \"\", \"spans\": []})\n",
    "            table_cells.append(row)\n",
    "\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"header\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "            \"table_rows\": table_rows,\n",
    "            \"table_cells\": table_cells,\n",
    "            \"header_source\": \"micro_multicol\",\n",
    "            \"column_centers\": centers,\n",
    "        }]\n",
    "\n",
    "    if _is_grid_like(row_cells, col_count):\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"multicol_grid\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    col_items = []\n",
    "    for ci in range(col_count):\n",
    "        out_lines = []\n",
    "        spans = []\n",
    "        for r in row_cells:\n",
    "            if ci in r:\n",
    "                t, sp = r[ci]\n",
    "                out_lines.append(t)\n",
    "                spans.extend(sp)\n",
    "            else:\n",
    "                out_lines.append(\"\")\n",
    "\n",
    "        while out_lines and not out_lines[0].strip():\n",
    "            out_lines.pop(0)\n",
    "        while out_lines and not out_lines[-1].strip():\n",
    "            out_lines.pop()\n",
    "\n",
    "        compact = []\n",
    "        blank = 0\n",
    "        for ln in out_lines:\n",
    "            if not ln.strip():\n",
    "                blank += 1\n",
    "                if blank <= 1:\n",
    "                    compact.append(\"\")\n",
    "            else:\n",
    "                blank = 0\n",
    "                compact.append(ln)\n",
    "\n",
    "        txt = \"\\n\".join(compact).rstrip() + (\"\\n\" if block_text.endswith(\"\\n\") else \"\")\n",
    "        txt = _normalize_kv_generic(txt)\n",
    "\n",
    "        if not _collapse_ws(txt).strip():\n",
    "            continue\n",
    "\n",
    "        if spans:\n",
    "            st = min(a for a, _ in spans)\n",
    "            en = max(b for _, b in spans)\n",
    "        else:\n",
    "            st = abs_start\n",
    "            en = abs_start + len(block_text)\n",
    "\n",
    "        col_items.append({\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(int(a), int(b)) for (a, b) in spans if b > a],\n",
    "            \"start\": st,\n",
    "            \"end\": en,\n",
    "            \"layout_kind\": \"multicol_col\",\n",
    "            \"col_index\": ci,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        })\n",
    "\n",
    "    if not col_items:\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    return col_items\n",
    "\n",
    "def _looks_like_paragraphish(line_full: str) -> bool:\n",
    "    s = (line_full or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if len(s) >= 120:\n",
    "        words = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", s)\n",
    "        if len(words) >= 10 and not _has_big_gap(s, 6, 1):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _is_address_continuation_line(line_full: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line_full or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return True\n",
    "    if _SEP_RE.match(st):\n",
    "        return True\n",
    "    if _is_table_line(line_full, gap_min):\n",
    "        return False\n",
    "    if TABLE_HINT_RE.search(st):\n",
    "        return False\n",
    "    if _is_section_start_line(line_full):\n",
    "        return False\n",
    "    if _dec_tokens(st) > 0:\n",
    "        return False\n",
    "    if _num_tokens(st) > (4 if is_ocr else 6):\n",
    "        return False\n",
    "    if re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", st) or _AR_RE.search(st):\n",
    "        return True\n",
    "    if re.match(r\"^\\d{4,6}$\", st):\n",
    "        return True\n",
    "    if re.search(r\"[@+/,-]\", st) and len(st) <= 120:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _collect_table_block(lines, start_i, gap_min):\n",
    "    n = len(lines)\n",
    "    i = start_i\n",
    "    blank_run = 0\n",
    "    seen_data = 0\n",
    "    collected = []\n",
    "\n",
    "    # FIX: reconnaître une \"wrap line\" indépendamment de la ligne précédente (utile pour chaîner wrap+wrap)\n",
    "    def _looks_like_wrap_line(s_raw: str) -> bool:\n",
    "        if not s_raw:\n",
    "            return False\n",
    "        # doit être indenté (comme dans ton exemple)\n",
    "        if not re.match(r\"^[ \\t]{2,}\\S\", s_raw):\n",
    "            return False\n",
    "        s_l = s_raw.lstrip(\" \\t\")\n",
    "        # pas une ligne structurante, pas de structure table\n",
    "        if _is_section_start_line(s_raw):\n",
    "            return False\n",
    "        if s_l.count(\"\\t\") >= 2:\n",
    "            return False\n",
    "        if _has_big_gap(s_l, gap_min, min_count=1):\n",
    "            return False\n",
    "        # très peu de signaux numériques (évite d'absorber TOTAL/TVA/etc)\n",
    "        if _dec_tokens(s_l) != 0:\n",
    "            return False\n",
    "        if _num_tokens(s_l) > 1:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    while i < n:\n",
    "        line_full, ls, le = lines[i]\n",
    "        s = line_full.rstrip(\"\\n\")\n",
    "\n",
    "        if not s.strip():\n",
    "            blank_run += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        is_tbl = _is_table_line(line_full, gap_min)\n",
    "\n",
    "        if is_tbl:\n",
    "            blank_run = 0\n",
    "            if _dec_tokens(s) >= 1 or _num_tokens(s) >= 3 or TABLE_HINT_RE.search(s):\n",
    "                seen_data += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # FIX: accepter wrap, y compris wrap qui suit wrap (pas seulement table_line)\n",
    "        if seen_data >= 1 and _looks_like_wrap_line(s):\n",
    "            prev_nonblank = None\n",
    "            for plf, _, _ in reversed(collected):\n",
    "                if plf.strip():\n",
    "                    prev_nonblank = plf.rstrip(\"\\n\")\n",
    "                    break\n",
    "            # on continue si la ligne précédente est soit une ligne de table, soit déjà une wrap\n",
    "            if prev_nonblank and (_is_table_line(prev_nonblank, gap_min) or _looks_like_wrap_line(prev_nonblank)):\n",
    "                blank_run = 0\n",
    "                collected.append((line_full, ls, le))\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        if seen_data >= 2 and blank_run >= 2:\n",
    "            break\n",
    "        if seen_data >= 1 and blank_run >= 1:\n",
    "            break\n",
    "        break\n",
    "\n",
    "    while collected and not collected[-1][0].strip():\n",
    "        collected.pop()\n",
    "\n",
    "    return collected, i\n",
    "\n",
    "def _make_span_item(page_text, spans, text_override, kind, meta=None):\n",
    "    spans2 = [(int(a), int(b)) for (a, b) in (spans or []) if b > a]\n",
    "    if spans2:\n",
    "        st = min(a for a, _ in spans2)\n",
    "        en = max(b for _, b in spans2)\n",
    "    else:\n",
    "        st = 0\n",
    "        en = 0\n",
    "    it = {\"text\": text_override, \"spans\": spans2, \"start\": st, \"end\": en, \"layout_kind\": kind}\n",
    "    if meta:\n",
    "        it.update(meta)\n",
    "    return it\n",
    "\n",
    "def layout_items(page_text: str, lang: str, extraction: str = \"\"):\n",
    "    if not page_text:\n",
    "        return []\n",
    "\n",
    "    is_ocr = str(extraction or \"\").startswith(\"ocr:\")\n",
    "    gap_min = GAP_MIN_OCR if is_ocr else GAP_MIN_NATIVE\n",
    "\n",
    "    lines = []\n",
    "    for ls, le in _iter_line_spans(page_text):\n",
    "        lines.append((page_text[ls:le], ls, le))\n",
    "\n",
    "    items = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    def _starts_table(i0):\n",
    "        return _is_table_line(lines[i0][0], gap_min)\n",
    "\n",
    "    def _starts_multicol(i0):\n",
    "        return _is_multicol_candidate_line(lines[i0][0], gap_min=gap_min, is_ocr=is_ocr)\n",
    "\n",
    "    while i < n:\n",
    "        if _starts_table(i):\n",
    "            collected, j = _collect_table_block(lines, i, gap_min=gap_min)\n",
    "            if collected:\n",
    "                a0 = collected[0][1]\n",
    "                b0 = collected[-1][2]\n",
    "                block_text = page_text[a0:b0]\n",
    "                table_rows = [{\"text\": lf, \"spans\": [(lls, lle)]} for (lf, lls, lle) in collected]\n",
    "                items.append(_make_span_item(\n",
    "                    page_text,\n",
    "                    spans=[(a0, b0)],\n",
    "                    text_override=block_text,\n",
    "                    kind=\"table\",\n",
    "                    meta={\"table_rows\": table_rows}\n",
    "                ))\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        if _starts_multicol(i):\n",
    "            start = i\n",
    "\n",
    "            if start - 1 >= 0:\n",
    "                prev_line = lines[start - 1][0]\n",
    "                if _looks_like_title_line(prev_line) and not _starts_table(start - 1):\n",
    "                    start -= 1\n",
    "\n",
    "            j = i\n",
    "            saw_any = False\n",
    "            blank_run = 0\n",
    "            noncol_inside = 0\n",
    "\n",
    "            MAX_INBLOCK_BLANK = 6\n",
    "            MAX_INBLOCK_LINES = 140\n",
    "            MAX_NONCOL_INSIDE = 25\n",
    "            weak_gap = max(3, gap_min - (3 if is_ocr else 2))\n",
    "\n",
    "            while j < n and (j - start) < MAX_INBLOCK_LINES:\n",
    "                if _starts_table(j):\n",
    "                    break\n",
    "\n",
    "                lf, lls, lle = lines[j]\n",
    "                ss = lf.rstrip(\"\\n\")\n",
    "\n",
    "                if not ss.strip() or _SEP_RE.match(ss.strip()):\n",
    "                    blank_run += 1\n",
    "                    j += 1\n",
    "                    if saw_any and blank_run >= MAX_INBLOCK_BLANK:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                blank_run = 0\n",
    "\n",
    "                if _starts_multicol(j):\n",
    "                    saw_any = True\n",
    "                    noncol_inside = 0\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                if saw_any and noncol_inside < MAX_NONCOL_INSIDE:\n",
    "                    if _is_address_continuation_line(lf, gap_min=gap_min, is_ocr=is_ocr) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    if _has_big_gap(ss, weak_gap, min_count=1) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "\n",
    "                break\n",
    "\n",
    "            end = j if j > i else i + 1\n",
    "\n",
    "            a0 = lines[start][1]\n",
    "            b0 = lines[end-1][2] if end-1 >= start else lines[start][2]\n",
    "            block_text = page_text[a0:b0]\n",
    "\n",
    "            items.extend(_transpose_or_group_multicol(block_text, abs_start=a0, gap_min=gap_min, is_ocr=is_ocr))\n",
    "\n",
    "            i = end\n",
    "            continue\n",
    "\n",
    "        start = i\n",
    "        j = i\n",
    "        while j < n:\n",
    "            if _starts_table(j) or _starts_multicol(j):\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        a0 = lines[start][1]\n",
    "        b0 = lines[j-1][2] if j-1 >= start else lines[start][2]\n",
    "        plain_text = page_text[a0:b0]\n",
    "\n",
    "        chunks = chunk_layout_universal(plain_text, lang)\n",
    "        pos = 0\n",
    "        for ch in chunks:\n",
    "            ca = a0 + pos\n",
    "            cb = ca + len(ch)\n",
    "            pos += len(ch)\n",
    "            items.append(_make_span_item(page_text, spans=[(ca, cb)], text_override=ch, kind=\"plain\"))\n",
    "\n",
    "        i = j if j > start else i + 1\n",
    "\n",
    "    def _k(it):\n",
    "        if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\"):\n",
    "            return (it.get(\"block_start\", it.get(\"start\", 0)), it.get(\"col_index\", 0) if it.get(\"col_index\") is not None else -1)\n",
    "        return (it.get(\"start\", 0), 0)\n",
    "\n",
    "    items.sort(key=_k)\n",
    "    return items\n",
    "\n",
    "# ==================== Noise detection (audit) ====================\n",
    "_NOISE_LINE_RE = re.compile(\n",
    "    r\"(?i)^\\s*(sample|confidential|draft)\\s*$|\"\n",
    "    r\"^\\s*page\\s+\\d+\\s*(?:of|/)\\s*\\d+\\s*$|\"\n",
    "    r\"^\\s*\\d+\\s*(?:of|/)\\s*\\d+\\s*$\"\n",
    ")\n",
    "\n",
    "def build_noise_keys_for_doc(pages_text):\n",
    "    if not pages_text:\n",
    "        return set()\n",
    "    page_count = len(pages_text)\n",
    "    if page_count < 3:\n",
    "        return set()\n",
    "    min_pages = max(3, int(math.ceil(page_count * 0.30)))\n",
    "\n",
    "    counts = {}\n",
    "    counts_masked = {}\n",
    "\n",
    "    for txt in pages_text:\n",
    "        seen = set()\n",
    "        seen_m = set()\n",
    "        for ls, le in _iter_line_spans(txt or \"\"):\n",
    "            line = (txt[ls:le]).rstrip(\"\\n\")\n",
    "            key = _collapse_ws(line).lower()\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            if _SEP_RE.match(key) or _NOISE_LINE_RE.match(line):\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                continue\n",
    "\n",
    "            mkey = _mask_digits(key)\n",
    "\n",
    "            if key not in seen:\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                seen.add(key)\n",
    "            if mkey not in seen_m:\n",
    "                counts_masked[mkey] = counts_masked.get(mkey, 0) + 1\n",
    "                seen_m.add(mkey)\n",
    "\n",
    "    noise_keys = set()\n",
    "    for k, c in counts.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(k)\n",
    "    for mk, c in counts_masked.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(mk)\n",
    "\n",
    "    return noise_keys\n",
    "\n",
    "def chunk_is_noise(chunk_text: str, noise_keys: set) -> bool:\n",
    "    if not chunk_text:\n",
    "        return True\n",
    "\n",
    "    has_nonempty = False\n",
    "    for ls, le in _iter_line_spans(chunk_text):\n",
    "        line = chunk_text[ls:le].rstrip(\"\\n\")\n",
    "        st = line.strip()\n",
    "        if not st:\n",
    "            continue\n",
    "        if _SEP_RE.match(st):\n",
    "            continue\n",
    "\n",
    "        has_nonempty = True\n",
    "        key = _collapse_ws(line).lower()\n",
    "        mkey = _mask_digits(key)\n",
    "\n",
    "        if _NOISE_LINE_RE.match(line):\n",
    "            continue\n",
    "        if key in noise_keys or mkey in noise_keys:\n",
    "            continue\n",
    "\n",
    "        return False\n",
    "\n",
    "    return True if has_nonempty else True\n",
    "\n",
    "# ==================== Helpers emplacement (page) ====================\n",
    "_WS_RE = re.compile(r\"\\s+\", flags=re.UNICODE)\n",
    "\n",
    "def _nonspace_len(s: str) -> int:\n",
    "    return len(_WS_RE.sub(\"\", s or \"\"))\n",
    "\n",
    "def _line_col_from_offset(text: str, off: int):\n",
    "    if off < 0:\n",
    "        off = 0\n",
    "    if off > len(text):\n",
    "        off = len(text)\n",
    "    line = text.count(\"\\n\", 0, off) + 1\n",
    "    last_nl = text.rfind(\"\\n\", 0, off)\n",
    "    col = off if last_nl < 0 else (off - last_nl - 1)\n",
    "    return line, col\n",
    "\n",
    "# ==================== Metadonnées depuis DOCS / TEXT_DOCS ====================\n",
    "def _safe_str(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _unique_keep_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def _pdf_extract_pages_text(path: str):\n",
    "    PdfReader = _get_pdf_reader()  # défini dans ta cellule d'extraction\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        out = []\n",
    "        for p in reader.pages:\n",
    "            out.append(p.extract_text() or \"\")\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _pdf_page_count(path: str):\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        return len(PdfReader(path).pages)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ==================== Vérifier FINAL_DOCS ====================\n",
    "if \"FINAL_DOCS\" not in globals() or not isinstance(FINAL_DOCS, list):\n",
    "    raise RuntimeError(\"FINAL_DOCS not found. Exécute d'abord la cellule précédente (celle qui imprime FINAL PRINT).\")\n",
    "\n",
    "# ==================== Construire une structure DOC -> PAGES ====================\n",
    "DOC_PACK = []\n",
    "\n",
    "# 1) OCR: DOCS (si dispo)\n",
    "if \"DOCS\" in globals() and isinstance(DOCS, list):\n",
    "    for d in DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        pages = d.get(\"pages\", []) or []\n",
    "        page_count_total = d.get(\"page_count_total\") if d.get(\"page_count_total\") else len(pages)\n",
    "\n",
    "        paths = []\n",
    "        for p in pages:\n",
    "            sp = p.get(\"source_path\") or p.get(\"path\")\n",
    "            if sp:\n",
    "                paths.append(_safe_str(sp))\n",
    "        paths = _unique_keep_order(paths)\n",
    "\n",
    "        pages_out = []\n",
    "        for p in pages:\n",
    "            pages_out.append({\n",
    "                \"page_index\": int(p.get(\"page_index\") or 1),\n",
    "                \"text\": p.get(\"ocr_text\") or \"\",\n",
    "                \"source_path\": _safe_str(p.get(\"source_path\") or p.get(\"path\") or \"\"),\n",
    "            })\n",
    "        pages_out.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"image_only\",\n",
    "            \"extraction\": \"ocr:tesseract\",\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 2) NATIVE: TEXT_DOCS (si dispo)\n",
    "if \"TEXT_DOCS\" in globals() and isinstance(TEXT_DOCS, list):\n",
    "    for d in TEXT_DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        extraction = d.get(\"extraction\") or \"native:unknown\"\n",
    "        sp = d.get(\"source_path\") or \"\"\n",
    "        paths = _unique_keep_order([_safe_str(sp)]) if sp else []\n",
    "        full_text = d.get(\"text\") or \"\"\n",
    "\n",
    "        pages_out = []\n",
    "        page_count_total = d.get(\"page_count_total\", None)\n",
    "        pages_text = d.get(\"pages_text\", None)\n",
    "\n",
    "        if pages_text is not None and isinstance(pages_text, list) and len(pages_text) > 0:\n",
    "            page_count_total = page_count_total or len(pages_text)\n",
    "            for i2, txt in enumerate(pages_text, start=1):\n",
    "                pages_out.append({\n",
    "                    \"page_index\": i2,\n",
    "                    \"text\": txt or \"\",\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "        else:\n",
    "            if sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "                pages_text2 = _pdf_extract_pages_text(sp)\n",
    "                if pages_text2:\n",
    "                    page_count_total = page_count_total or len(pages_text2)\n",
    "                    for i2, txt in enumerate(pages_text2, start=1):\n",
    "                        pages_out.append({\n",
    "                            \"page_index\": i2,\n",
    "                            \"text\": txt or \"\",\n",
    "                            \"source_path\": _safe_str(sp),\n",
    "                        })\n",
    "                else:\n",
    "                    pages_out.append({\n",
    "                        \"page_index\": 1,\n",
    "                        \"text\": full_text,\n",
    "                        \"source_path\": _safe_str(sp),\n",
    "                    })\n",
    "                    page_count_total = page_count_total or 1\n",
    "            else:\n",
    "                pages_out.append({\n",
    "                    \"page_index\": 1,\n",
    "                    \"text\": full_text,\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "                page_count_total = page_count_total or 1\n",
    "\n",
    "        if page_count_total is None and sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "            pc = _pdf_page_count(sp)\n",
    "            if pc is not None:\n",
    "                page_count_total = pc\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": extraction,\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 3) Fallback à FINAL_DOCS\n",
    "if not DOC_PACK:\n",
    "    for d in FINAL_DOCS:\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": d.get(\"doc_id\"),\n",
    "            \"filename\": d.get(\"filename\") or \"unknown\",\n",
    "            \"content\": d.get(\"content\"),\n",
    "            \"extraction\": d.get(\"extraction\"),\n",
    "            \"paths\": [],\n",
    "            \"page_count_total\": 1,\n",
    "            \"pages\": [{\"page_index\": 1, \"text\": d.get(\"text\") or \"\", \"source_path\": \"\"}],\n",
    "        })\n",
    "\n",
    "# ==================== Tokeniser: construire TOK_DOCS ====================\n",
    "TOK_DOCS = []\n",
    "\n",
    "for doc in DOC_PACK:\n",
    "    doc_id = doc.get(\"doc_id\")\n",
    "    filename = doc.get(\"filename\") or \"unknown\"\n",
    "    extraction = doc.get(\"extraction\") or \"\"\n",
    "    content_type = doc.get(\"content\")\n",
    "    paths = doc.get(\"paths\") or []\n",
    "    page_count_total = doc.get(\"page_count_total\")\n",
    "\n",
    "    pages_text_for_noise = [(p.get(\"text\") or \"\") for p in (doc.get(\"pages\") or [])]\n",
    "    noise_keys = build_noise_keys_for_doc(pages_text_for_noise)\n",
    "\n",
    "    pages_tok = []\n",
    "    doc_chars_total = 0\n",
    "    recompose_ok_doc = True\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        page_index = int(pg.get(\"page_index\") or 1)\n",
    "        page_text = pg.get(\"text\") or \"\"\n",
    "        doc_chars_total += len(page_text)\n",
    "\n",
    "        lang = detect_lang(page_text)\n",
    "\n",
    "        items = layout_items(page_text, lang, extraction=extraction)\n",
    "        recompose_ok = False if any(it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\") for it in items) else True\n",
    "        if not recompose_ok:\n",
    "            recompose_ok_doc = False\n",
    "\n",
    "        sent_items = []\n",
    "        for it in items:\n",
    "            chunk = it[\"text\"]\n",
    "            start = int(it.get(\"start\", 0))\n",
    "            end = int(it.get(\"end\", start + len(chunk)))\n",
    "\n",
    "            line, col = _line_col_from_offset(page_text, start)\n",
    "            nonspace = _nonspace_len(chunk)\n",
    "\n",
    "            is_noise = chunk_is_noise(chunk, noise_keys)\n",
    "\n",
    "            if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\"):\n",
    "                is_sentence = (not is_noise) and (nonspace >= 1)\n",
    "            else:\n",
    "                is_sentence = (nonspace >= MIN_SENTENCE_NONSPACE) and (not is_noise)\n",
    "\n",
    "            sent_items.append({\n",
    "                \"text\": chunk,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"line\": line,\n",
    "                \"col\": col,\n",
    "                \"chars\": len(chunk),\n",
    "                \"nonspace\": nonspace,\n",
    "                \"is_noise\": is_noise,\n",
    "                \"is_sentence\": is_sentence,\n",
    "                \"spans\": it.get(\"spans\", []),\n",
    "                \"layout_kind\": it.get(\"layout_kind\", \"plain\"),\n",
    "                \"col_index\": it.get(\"col_index\", None),\n",
    "                \"table_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_cells\": it.get(\"table_cells\", None),\n",
    "                \"header_source\": it.get(\"header_source\", None),\n",
    "            })\n",
    "\n",
    "        pages_tok.append({\n",
    "            \"page_index\": page_index,\n",
    "            \"source_path\": pg.get(\"source_path\") or \"\",\n",
    "            \"lang\": lang,\n",
    "            \"chars\": len(page_text),\n",
    "            \"recompose_ok\": recompose_ok,\n",
    "            \"sentences_layout\": sent_items,\n",
    "            \"page_text\": page_text,\n",
    "        })\n",
    "\n",
    "    pages_tok.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "    TOK_DOCS.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": filename,\n",
    "        \"paths\": paths,\n",
    "        \"page_count_total\": page_count_total,\n",
    "        \"content\": content_type,\n",
    "        \"extraction\": extraction,\n",
    "        \"pages\": pages_tok,\n",
    "        \"chars_total\": doc_chars_total,\n",
    "        \"recompose_ok\": recompose_ok_doc,\n",
    "    })\n",
    "\n",
    "def _sort_key(x):\n",
    "    p = (x.get(\"paths\") or [\"\"])[0]\n",
    "    return (x.get(\"filename\") or \"\", str(p))\n",
    "\n",
    "TOK_DOCS.sort(key=_sort_key)\n",
    "\n",
    "TOK_BY_ID = {d[\"doc_id\"]: d for d in TOK_DOCS if d.get(\"doc_id\")}\n",
    "TOK_BY_FILENAME = {}\n",
    "for d in TOK_DOCS:\n",
    "    TOK_BY_FILENAME.setdefault(d[\"filename\"], []).append(d)\n",
    "\n",
    "def _select_doc(target):\n",
    "    if target is None:\n",
    "        return TOK_DOCS\n",
    "    if isinstance(target, int):\n",
    "        if 0 <= target < len(TOK_DOCS):\n",
    "            return [TOK_DOCS[target]]\n",
    "        raise IndexError(f\"TARGET index out of range: {target} (0..{len(TOK_DOCS)-1})\")\n",
    "    if isinstance(target, str):\n",
    "        t = target.strip()\n",
    "        if t in TOK_BY_ID:\n",
    "            return [TOK_BY_ID[t]]\n",
    "        if t in TOK_BY_FILENAME:\n",
    "            return TOK_BY_FILENAME[t]\n",
    "        hits = []\n",
    "        for d in TOK_DOCS:\n",
    "            if t.lower() in (d.get(\"filename\",\"\").lower()):\n",
    "                hits.append(d)\n",
    "                continue\n",
    "            for p in d.get(\"paths\") or []:\n",
    "                if t.lower() in str(p).lower():\n",
    "                    hits.append(d)\n",
    "                    break\n",
    "        if hits:\n",
    "            return hits\n",
    "        raise ValueError(f\"No document matches TARGET='{target}' (by doc_id/filename/path).\")\n",
    "    raise TypeError(\"TARGET must be None, int, or str\")\n",
    "\n",
    "def print_one_doc(doc):\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"[doc] {doc['filename']}\")\n",
    "    print(f\"  doc_id       : {doc.get('doc_id')}\")\n",
    "    print(f\"  content      : {doc.get('content')}\")\n",
    "    print(f\"  extraction   : {doc.get('extraction')}\")\n",
    "    print(f\"  pages_total  : {doc.get('page_count_total')}\")\n",
    "    print(f\"  chars_total  : {doc.get('chars_total')}\")\n",
    "    print(f\"  recompose_ok : {doc.get('recompose_ok')}\")\n",
    "    print(\"  paths:\")\n",
    "    if doc.get(\"paths\"):\n",
    "        for p in doc[\"paths\"]:\n",
    "            print(f\"    - {p}\")\n",
    "    else:\n",
    "        print(\"    - (unknown)\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    if not PRINT_SENTENCES:\n",
    "        return\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        print(f\"[page {pg['page_index']}/{doc.get('page_count_total') or '?'}] source_path={pg.get('source_path')}\")\n",
    "        print(f\"  lang         : {pg.get('lang')}\")\n",
    "        print(f\"  chars        : {pg.get('chars')}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        if PRINT_PAGE_TEXT:\n",
    "            print(pg.get(\"page_text\") or \"\")\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "        sent_items = pg.get(\"sentences_layout\") or []\n",
    "\n",
    "        total_all = len(sent_items)\n",
    "        total_noise = sum(1 for s in sent_items if s.get(\"is_noise\"))\n",
    "        total_sentence = sum(1 for s in sent_items if s.get(\"is_sentence\"))\n",
    "\n",
    "        if PRINT_ONLY_SENTENCES:\n",
    "            view = [s for s in sent_items if s.get(\"is_sentence\")]\n",
    "        else:\n",
    "            view = list(sent_items)\n",
    "\n",
    "        fallback_used = False\n",
    "        if PRINT_ONLY_SENTENCES and not view and total_all > 0:\n",
    "            view = list(sent_items)\n",
    "            fallback_used = True\n",
    "\n",
    "        total_view = len(view)\n",
    "        show = total_view if MAX_SENTENCES_PREVIEW is None else min(total_view, MAX_SENTENCES_PREVIEW)\n",
    "\n",
    "        print(\n",
    "            f\"  sentences_layout: {total_all} chunks total | \"\n",
    "            f\"sentences={total_sentence} | noise={total_noise} | \"\n",
    "            f\"showing {show}/{total_view} \"\n",
    "            f\"(filter_is_sentence={PRINT_ONLY_SENTENCES}, fallback={fallback_used}, min_nonspace={MIN_SENTENCE_NONSPACE})\"\n",
    "        )\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        for i2 in range(show):\n",
    "            s = view[i2]\n",
    "            chunk = s[\"text\"]\n",
    "            print(\n",
    "                f\"[sent {i2+1}/{total_view}] page={pg['page_index']} start={s['start']} end={s['end']} \"\n",
    "                f\"line={s['line']} col={s['col']} chars={s['chars']} nonspace={s['nonspace']} \"\n",
    "                f\"is_noise={s.get('is_noise')} is_sentence={s['is_sentence']} layout={s.get('layout_kind')}\"\n",
    "            )\n",
    "            print(chunk, end=\"\" if chunk.endswith(\"\\n\") else \"\\n\")\n",
    "            if PRINT_REPR:\n",
    "                print(\"repr:\", repr(chunk))\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        if MAX_SENTENCES_PREVIEW is not None and total_view > show:\n",
    "            print(f\"... {total_view - show} chunks restants non affichés (MAX_SENTENCES_PREVIEW={MAX_SENTENCES_PREVIEW})\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# ==================== Exécution ====================\n",
    "selected = _select_doc(TARGET)\n",
    "\n",
    "if not selected:\n",
    "    print(\"[info] Aucun document à traiter.\")\n",
    "else:\n",
    "    for doc in selected:\n",
    "        print_one_doc(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b2b4a",
   "metadata": {},
   "source": [
    "### attribue une catégorie grammaticale // jeu d’étiquettes NN, NNS, VB, VBD, JJ ...///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10970649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps: (absent)\n",
      "Type an Arabic sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n",
      "\n",
      "========================================================================================================================\n",
      "RUN EN (engcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "========================================================================================================================\n",
      "RUN FR (frcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "##########################################################################################\n",
      "DOC=francais.docx | page=1 | sent=0 | lang=fr\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "                              Le  DT      lemma=le\n",
      "                              14  CD      lemma=14\n",
      "                         février  NNP     lemma=février\n",
      "                            2026  CD      lemma=2026\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               à  IN      lemma=à\n",
      "                              07  CD      lemma=07\n",
      "                               :  PUNCT   lemma=∅\n",
      "                              35  CD      lemma=35\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                     MaghrebRail  NNP     lemma=MaghrebRail\n",
      "                               a  VB      lemma=avoir\n",
      "                         annoncé  VB      lemma=annoncer\n",
      "                          depuis  IN      lemma=depuis\n",
      "                    Alger-Centre  NNP     lemma=Alger-Centre\n",
      "                             une  DT      lemma=un\n",
      "                        nouvelle  JJ      lemma=nouveau\n",
      "                           phase  NN      lemma=phase\n",
      "                              de  IN      lemma=de\n",
      "                   modernisation  NN      lemma=modernisation\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                           ligne  NN      lemma=ligne\n",
      "                              de  IN      lemma=de\n",
      "                        banlieue  NN      lemma=banlieue\n",
      "                         reliant  JJ      lemma=relier\n",
      "                             Bab  NNP     lemma=Bab\n",
      "                         Ezzouar  NNP     lemma=Ezzouar\n",
      "                               à  IN      lemma=à\n",
      "                           Blida  NNP     lemma=Blida\n",
      "                             via  IN      lemma=via\n",
      "                              El  NNP     lemma=El\n",
      "                         Harrach  NNP     lemma=Harrach\n",
      "                              et  CC      lemma=et\n",
      "                        Birtouta  NNP     lemma=Birtouta\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              en  IN      lemma=en\n",
      "                              s’  PRP     lemma=se\n",
      "                        appuyant  JJ      lemma=appuyer\n",
      "                             sur  IN      lemma=sur\n",
      "                             une  DT      lemma=un\n",
      "                            note  NN      lemma=note\n",
      "                         interne  NN      lemma=interne\n",
      "                          signée  NN      lemma=signé\n",
      "                             par  IN      lemma=par\n",
      "                           Samir  NNP     lemma=Samir\n",
      "                       Ould-Kaci  NNP     lemma=Ould-Kaci\n",
      "                               (  PUNCT   lemma=∅\n",
      "                           Chief  NNP     lemma=Chief\n",
      "                      Operations  NNP     lemma=Operations\n",
      "                         Officer  NNP     lemma=Officer\n",
      "                               )  PUNCT   lemma=∅\n",
      "                       indiquant  JJ      lemma=indiquer\n",
      "                              un  DT      lemma=un\n",
      "                       démarrage  NN      lemma=démarrage\n",
      "                              le  DT      lemma=le\n",
      "                               3  CD      lemma=3\n",
      "                            mars  NNP     lemma=mars\n",
      "                            2026  CD      lemma=2026\n",
      "                              et  CC      lemma=et\n",
      "                             une  DT      lemma=un\n",
      "                           durée  NN      lemma=durée\n",
      "                         estimée  NN      lemma=estimer\n",
      "                              de  IN      lemma=de\n",
      "                              18  CD      lemma=18\n",
      "                               à  IN      lemma=à\n",
      "                              24  CD      lemma=24\n",
      "                            mois  NN      lemma=mois\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            pour  IN      lemma=pour\n",
      "                              un  DT      lemma=un\n",
      "                          budget  NN      lemma=budget\n",
      "                              de  IN      lemma=de\n",
      "                             1,8  CD      lemma=1,8\n",
      "                        milliard  NN      lemma=milliard\n",
      "                             DZD  NNP     lemma=DZD\n",
      "                               (  PUNCT   lemma=∅\n",
      "                               ≈  NN      lemma=≈\n",
      "                            13,4  CD      lemma=13,4\n",
      "                               M  NN      lemma=mètre\n",
      "                               $  NN      lemma=$\n",
      "                               )  PUNCT   lemma=∅\n",
      "                            dont  PRP     lemma=dont\n",
      "                              35  CD      lemma=35\n",
      "                               %  NN      lemma=%\n",
      "                        financés  NN      lemma=financer\n",
      "                             par  IN      lemma=par\n",
      "                              un  DT      lemma=un\n",
      "                      consortium  NN      lemma=consortium\n",
      "                           privé  NN      lemma=privé\n",
      "                            mené  NN      lemma=mener\n",
      "                             par  IN      lemma=par\n",
      "                       NorthGate  NNP     lemma=NorthGate\n",
      "                  Infrastructure  NNP     lemma=infrastructure\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            avec  IN      lemma=avec\n",
      "                              l’  DT      lemma=le\n",
      "                           appui  NN      lemma=appui\n",
      "                              d’  IN      lemma=de\n",
      "                      ingénieurs  NN      lemma=ingénieur\n",
      "                           basés  NN      lemma=basé\n",
      "                               à  IN      lemma=à\n",
      "                            Lyon  NNP     lemma=Lyon\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           Milan  NNP     lemma=milan\n",
      "                              et  CC      lemma=et\n",
      "                        Hambourg  NNP     lemma=Hambourg\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              un  DT      lemma=un\n",
      "                     partenariat  NN      lemma=partenariat\n",
      "                              de  IN      lemma=de\n",
      "                       recherche  NN      lemma=recherche\n",
      "                            avec  IN      lemma=avec\n",
      "                              l’  DT      lemma=le\n",
      "                           USTHB  NNP     lemma=USTHB\n",
      "                               (  PUNCT   lemma=∅\n",
      "                      Université  NNP     lemma=université\n",
      "                             des  DT      lemma=un\n",
      "                        Sciences  NNP     lemma=science\n",
      "                              et  CC      lemma=et\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                     Technologie  NNP     lemma=technologie\n",
      "               Houari-Boumédiène  NNP     lemma=Houari-Boumédiène\n",
      "                               )  PUNCT   lemma=∅\n",
      "                            afin  NN      lemma=afin\n",
      "                              de  IN      lemma=de\n",
      "                        déployer  VB      lemma=déployer\n",
      "                              un  DT      lemma=un\n",
      "                         tableau  NN      lemma=tableau\n",
      "                              de  IN      lemma=de\n",
      "                            bord  NN      lemma=bord\n",
      "                        sécurité  NN      lemma=sécurité\n",
      "                         suivant  JJ      lemma=suivant\n",
      "                             des  DT      lemma=un\n",
      "                             KPI  NNP     lemma=KPI\n",
      "                           comme  NN      lemma=comme\n",
      "                              le  DT      lemma=le\n",
      "                            taux  JJ      lemma=taux\n",
      "                              de  IN      lemma=de\n",
      "                     ponctualité  NN      lemma=ponctualité\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             les  DT      lemma=le\n",
      "                       incidents  JJ      lemma=incident\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              la  DT      lemma=le\n",
      "                          charge  NN      lemma=charge\n",
      "                              en  IN      lemma=en\n",
      "                          heures  NN      lemma=heure\n",
      "                              de  IN      lemma=de\n",
      "                          pointe  NN      lemma=pointe\n",
      "                         pendant  IN      lemma=pendant\n",
      "                              le  DT      lemma=le\n",
      "                         Ramadan  NNP     lemma=ramadan\n",
      "                              et  CC      lemma=et\n",
      "                             les  DT      lemma=le\n",
      "                           jours  NN      lemma=jour\n",
      "                              de  IN      lemma=de\n",
      "                           match  NN      lemma=match\n",
      "                              au  DT      lemma=au\n",
      "                           stade  NN      lemma=stade\n",
      "                              de  IN      lemma=de\n",
      "                          Baraki  NNP     lemma=Baraki\n",
      "                               ;  PUNCT   lemma=∅\n",
      "                             sur  IN      lemma=sur\n",
      "                             les  DT      lemma=le\n",
      "                         réseaux  JJ      lemma=réseau\n",
      "                         sociaux  JJ      lemma=social\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             les  DT      lemma=le\n",
      "                       réactions  NN      lemma=réaction\n",
      "                         étaient  VB      lemma=être\n",
      "                     contrastées  VB      lemma=contrasté\n",
      "                               —  PUNCT   lemma=∅\n",
      "                               «  PUNCT   lemma=∅\n",
      "                              On  PRP     lemma=on\n",
      "                            veut  VB      lemma=vouloir\n",
      "                           moins  RB      lemma=moins\n",
      "                              de  IN      lemma=de\n",
      "                         retards  NN      lemma=retard\n",
      "                              et  CC      lemma=et\n",
      "                            plus  RB      lemma=plus\n",
      "                              de  IN      lemma=de\n",
      "                        sécurité  NN      lemma=sécurité\n",
      "                               »  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           écrit  VB      lemma=écrit\n",
      "                           Amina  NNP     lemma=Amina\n",
      "                               B  NN      lemma=B\n",
      "                               .  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          tandis  NN      lemma=tandis\n",
      "                             que  CC      lemma=que\n",
      "                          Mourad  NNP     lemma=Mourad\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                      commerçant  JJ      lemma=commerçant\n",
      "                            près  NN      lemma=près\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                            gare  VB      lemma=gare\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          craint  NN      lemma=craindre\n",
      "                             que  CC      lemma=que\n",
      "                             les  DT      lemma=le\n",
      "                         travaux  JJ      lemma=travail\n",
      "                      perturbent  NN      lemma=perturber\n",
      "                             les  DT      lemma=le\n",
      "                      livraisons  NN      lemma=livraison\n",
      "                               —  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            pour  IN      lemma=pour\n",
      "                         pousser  VB      lemma=pousser\n",
      "                             ton  DT      lemma=ton\n",
      "                        pipeline  NN      lemma=pipeline\n",
      "           POS/lemmatisation/NER  NNP     lemma=POS/lemmatisation/NER\n",
      "                            dans  IN      lemma=dans\n",
      "                             ses  DT      lemma=son\n",
      "                  retranchements  JJ      lemma=retranchement\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              le  DT      lemma=le\n",
      "                            même  NN      lemma=même\n",
      "                           texte  NN      lemma=texte\n",
      "                         mélange  NN      lemma=mélange\n",
      "                             des  DT      lemma=un\n",
      "                          tokens  NN      lemma=tokens\n",
      "                               “  PUNCT   lemma=∅\n",
      "                      difficiles  NN      lemma=difficile\n",
      "                               ”  PUNCT   lemma=∅\n",
      "                           comme  NN      lemma=comme\n",
      "                              un  DT      lemma=un\n",
      "                            UUID  NNP     lemma=UUID\n",
      "                               5  CD      lemma=5\n",
      "                               5  CD      lemma=5\n",
      "                               0  CD      lemma=0\n",
      "                               e  NN      lemma=e\n",
      "                               8  CD      lemma=8\n",
      "                               4  CD      lemma=4\n",
      "                               0  CD      lemma=0\n",
      "                               0  CD      lemma=0\n",
      "                               -  PUNCT   lemma=∅\n",
      "     e29b-41d4-a716-446655440000  NNP     lemma=e29b-41d4-a716-446655440000\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              un  DT      lemma=un\n",
      "                          chemin  NN      lemma=chemin\n",
      "                            Unix  NNP     lemma=Unix\n",
      "                               /  NN      lemma=/\n",
      "            var/log/nginx/access  NNP     lemma=var/log/nginx/access\n",
      "                               .  PUNCT   lemma=∅\n",
      "                             log  NN      lemma=log\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                       notations  NN      lemma=notation\n",
      "                   scientifiques  NN      lemma=scientifique\n",
      "                               3  CD      lemma=3\n",
      "                               .  PUNCT   lemma=∅\n",
      "                               2  CD      lemma=2\n",
      "                               e  NN      lemma=e\n",
      "                               -  PUNCT   lemma=∅\n",
      "                               4  CD      lemma=4\n",
      "                              et  CC      lemma=et\n",
      "                               2  CD      lemma=2\n",
      "                               ×  NN      lemma=×\n",
      "                              10  CD      lemma=10\n",
      "                               −  NN      lemma=−\n",
      "                               3  CD      lemma=3\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                        hashtags  NN      lemma=hashtags\n",
      "                               #  NN      lemma=#\n",
      "                          DevOps  NNP     lemma=DevOps\n",
      "                              et  CC      lemma=et\n",
      "                               #  NN      lemma=#\n",
      "                              IA  NNP     lemma=IA\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                         handles  NN      lemma=handles\n",
      "                               @  NN      lemma=@\n",
      "                         support  NN      lemma=support\n",
      "                              et  CC      lemma=et\n",
      "                               @  NN      lemma=@\n",
      "                       team-lead  NNP     lemma=team-lead\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             une  DT      lemma=un\n",
      "                             URL  NNP     lemma=URL\n",
      "https://example.org/api/v1/users?id=42  NNP     lemma=https://example.org/api/v1/users?id=42\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              un  DT      lemma=un\n",
      "                          e-mail  NNP     lemma=e-mail\n",
      "      prenom.nom+test@domaine.dz  NNP     lemma=prenom.nom+test@domaine.dz\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             une  DT      lemma=un\n",
      "                       référence  NN      lemma=référence\n",
      "                              de  IN      lemma=de\n",
      "                          ticket  NN      lemma=ticket\n",
      "                               #  NN      lemma=#\n",
      "                          A-9981  NNP     lemma=A-9981\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           ainsi  RB      lemma=ainsi\n",
      "                             que  CC      lemma=que\n",
      "                             des  DT      lemma=un\n",
      "                          termes  NN      lemma=terme\n",
      "                 semi-techniques  NNP     lemma=semi-techniques\n",
      "                               (  PUNCT   lemma=∅\n",
      "                             SLA  NNP     lemma=SLA\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            logs  NN      lemma=logs\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          export  NN      lemma=export\n",
      "                             CSV  NNP     lemma=CSV\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            RGPD  NNP     lemma=RGPD\n",
      "                               )  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                     ponctuation  NN      lemma=ponctuation\n",
      "                        atypique  JJ      lemma=atypique\n",
      "                               (  PUNCT   lemma=∅\n",
      "                               «  PUNCT   lemma=∅\n",
      "                               »  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               —  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               %  NN      lemma=%\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               :  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               /  NN      lemma=/\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               +  NN      lemma=+\n",
      "                               )  PUNCT   lemma=∅\n",
      "                             qui  PRP     lemma=qui\n",
      "                         peuvent  VB      lemma=pouvoir\n",
      "                           faire  VB      lemma=faire\n",
      "                       dérailler  VB      lemma=dérailler\n",
      "                              la  DT      lemma=le\n",
      "                   normalisation  NN      lemma=normalisation\n",
      "                              si  NN      lemma=si\n",
      "                            elle  PRP     lemma=elle\n",
      "                             est  VB      lemma=être\n",
      "                            trop  RB      lemma=trop\n",
      "                       agressive  JJ      lemma=agressive\n",
      "                               .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): Davlan/bert-base-multilingual-cased-ner-hrl) (token, label):\n",
      "[('Le', 'O'), ('14', 'B-DATE'), ('février', 'I-DATE'), ('2026', 'I-DATE'), (',', 'O'), ('à', 'O'), ('07', 'O'), (':', 'O'), ('35', 'O'), (',', 'O'), ('MaghrebRail', 'B-ORG'), ('a', 'O'), ('annoncé', 'O'), ('depuis', 'O'), ('Alger-Centre', 'B-LOC'), ('une', 'O'), ('nouvelle', 'O'), ('phase', 'O'), ('de', 'O'), ('modernisation', 'O'), ('de', 'O'), ('la', 'O'), ('ligne', 'O'), ('de', 'O'), ('banlieue', 'O'), ('reliant', 'O'), ('Bab', 'B-LOC'), ('Ezzouar', 'I-LOC'), ('à', 'O'), ('Blida', 'B-LOC'), ('via', 'O'), ('El', 'B-LOC'), ('Harrach', 'I-LOC'), ('et', 'O'), ('Birtouta', 'B-LOC'), (',', 'O'), ('en', 'O'), ('s’', 'O'), ('appuyant', 'O'), ('sur', 'O'), ('une', 'O'), ('note', 'O'), ('interne', 'O'), ('signée', 'O'), ('par', 'O'), ('Samir', 'B-PERS'), ('Ould-Kaci', 'I-PERS'), ('(', 'O'), ('Chief', 'O'), ('Operations', 'O'), ('Officer', 'O'), (')', 'O'), ('indiquant', 'O'), ('un', 'O'), ('démarrage', 'O'), ('le', 'O'), ('3', 'B-DATE'), ('mars', 'I-DATE'), ('2026', 'I-DATE'), ('et', 'O'), ('une', 'O'), ('durée', 'O'), ('estimée', 'O'), ('de', 'O'), ('18', 'O'), ('à', 'O'), ('24', 'O'), ('mois', 'O'), (',', 'O'), ('pour', 'O'), ('un', 'O'), ('budget', 'O'), ('de', 'O'), ('1,8', 'O'), ('milliard', 'O'), ('DZD', 'O'), ('(', 'O'), ('≈', 'O'), ('13,4', 'O'), ('M', 'O'), ('$', 'O'), (')', 'O'), ('dont', 'O'), ('35', 'O'), ('%', 'O'), ('financés', 'O'), ('par', 'O'), ('un', 'O'), ('consortium', 'O'), ('privé', 'O'), ('mené', 'O'), ('par', 'O'), ('NorthGate', 'B-ORG'), ('Infrastructure', 'I-ORG'), (',', 'O'), ('avec', 'O'), ('l’', 'O'), ('appui', 'O'), ('d’', 'O'), ('ingénieurs', 'O'), ('basés', 'O'), ('à', 'O'), ('Lyon', 'B-LOC'), (',', 'O'), ('Milan', 'B-LOC'), ('et', 'O'), ('Hambourg', 'B-LOC'), (',', 'O'), ('et', 'O'), ('un', 'O'), ('partenariat', 'O'), ('de', 'O'), ('recherche', 'O'), ('avec', 'O'), ('l’', 'O'), ('USTHB', 'B-ORG'), ('(', 'O'), ('Université', 'B-ORG'), ('des', 'I-ORG'), ('Sciences', 'I-ORG'), ('et', 'I-ORG'), ('de', 'I-ORG'), ('la', 'I-ORG'), ('Technologie', 'I-ORG'), ('Houari-Boumédiène', 'I-ORG'), (')', 'O'), ('afin', 'O'), ('de', 'O'), ('déployer', 'O'), ('un', 'O'), ('tableau', 'O'), ('de', 'O'), ('bord', 'O'), ('sécurité', 'O'), ('suivant', 'O'), ('des', 'O'), ('KPI', 'O'), ('comme', 'O'), ('le', 'O'), ('taux', 'O'), ('de', 'O'), ('ponctualité', 'O'), (',', 'O'), ('les', 'O'), ('incidents', 'O'), (',', 'O'), ('et', 'O'), ('la', 'O'), ('charge', 'O'), ('en', 'O'), ('heures', 'O'), ('de', 'O'), ('pointe', 'O'), ('pendant', 'O'), ('le', 'O'), ('Ramadan', 'O'), ('et', 'O'), ('les', 'O'), ('jours', 'O'), ('de', 'O'), ('match', 'O'), ('au', 'O'), ('stade', 'O'), ('de', 'O'), ('Baraki', 'B-LOC'), (';', 'O'), ('sur', 'O'), ('les', 'O'), ('réseaux', 'O'), ('sociaux', 'O'), (',', 'O'), ('les', 'O'), ('réactions', 'O'), ('étaient', 'O'), ('contrastées', 'O'), ('—', 'O'), ('«', 'O'), ('On', 'O'), ('veut', 'O'), ('moins', 'O'), ('de', 'O'), ('retards', 'O'), ('et', 'O'), ('plus', 'O'), ('de', 'O'), ('sécurité', 'O'), ('»', 'O'), (',', 'O'), ('écrit', 'O'), ('Amina', 'B-PERS'), ('B', 'I-PERS'), ('.', 'I-PERS'), (',', 'O'), ('tandis', 'O'), ('que', 'O'), ('Mourad', 'B-PERS'), (',', 'O'), ('commerçant', 'O'), ('près', 'O'), ('de', 'O'), ('la', 'O'), ('gare', 'O'), (',', 'O'), ('craint', 'O'), ('que', 'O'), ('les', 'O'), ('travaux', 'O'), ('perturbent', 'O'), ('les', 'O'), ('livraisons', 'O'), ('—', 'O'), ('et', 'O'), (',', 'O'), ('pour', 'O'), ('pousser', 'O'), ('ton', 'O'), ('pipeline', 'O'), ('POS/lemmatisation/NER', 'O'), ('dans', 'O'), ('ses', 'O'), ('retranchements', 'O'), (',', 'O'), ('le', 'O'), ('même', 'O'), ('texte', 'O'), ('mélange', 'O'), ('des', 'O'), ('tokens', 'O'), ('“', 'O'), ('difficiles', 'O'), ('”', 'O'), ('comme', 'O'), ('un', 'O'), ('UUID', 'O'), ('5', 'O'), ('5', 'O'), ('0', 'O'), ('e', 'O'), ('8', 'O'), ('4', 'O'), ('0', 'O'), ('0', 'O'), ('-', 'O'), ('e29b-41d4-a716-446655440000', 'O'), (',', 'O'), ('un', 'O'), ('chemin', 'O'), ('Unix', 'O'), ('/', 'O'), ('var/log/nginx/access', 'O'), ('.', 'O'), ('log', 'O'), (',', 'O'), ('des', 'O'), ('notations', 'O'), ('scientifiques', 'O'), ('3', 'O'), ('.', 'O'), ('2', 'O'), ('e', 'O'), ('-', 'O'), ('4', 'O'), ('et', 'O'), ('2', 'O'), ('×', 'O'), ('10', 'O'), ('−', 'O'), ('3', 'O'), (',', 'O'), ('des', 'O'), ('hashtags', 'O'), ('#', 'O'), ('DevOps', 'O'), ('et', 'O'), ('#', 'O'), ('IA', 'O'), (',', 'O'), ('des', 'O'), ('handles', 'O'), ('@', 'O'), ('support', 'O'), ('et', 'O'), ('@', 'O'), ('team-lead', 'O'), (',', 'O'), ('une', 'O'), ('URL', 'O'), ('https://example.org/api/v1/users?id=42', 'O'), (',', 'O'), ('un', 'O'), ('e-mail', 'O'), ('prenom.nom+test@domaine.dz', 'O'), (',', 'O'), ('une', 'O'), ('référence', 'O'), ('de', 'O'), ('ticket', 'O'), ('#', 'O'), ('A-9981', 'O'), (',', 'O'), ('ainsi', 'O'), ('que', 'O'), ('des', 'O'), ('termes', 'O'), ('semi-techniques', 'O'), ('(', 'O'), ('SLA', 'O'), (',', 'O'), ('logs', 'O'), (',', 'O'), ('export', 'O'), ('CSV', 'O'), (',', 'O'), ('RGPD', 'O'), (')', 'O'), ('et', 'O'), ('de', 'O'), ('la', 'O'), ('ponctuation', 'O'), ('atypique', 'O'), ('(', 'O'), ('«', 'O'), ('»', 'O'), (',', 'O'), ('—', 'O'), (',', 'O'), ('%', 'O'), (',', 'O'), (':', 'O'), (',', 'O'), ('/', 'O'), (',', 'O'), ('+', 'O'), (')', 'O'), ('qui', 'O'), ('peuvent', 'O'), ('faire', 'O'), ('dérailler', 'O'), ('la', 'O'), ('normalisation', 'O'), ('si', 'O'), ('elle', 'O'), ('est', 'O'), ('trop', 'O'), ('agressive', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: 14 février 2026\n",
      "  DATE: 3 mars 2026\n",
      "  LOC: Alger-Centre\n",
      "  LOC: Bab Ezzouar\n",
      "  LOC: Blida\n",
      "  LOC: El Harrach\n",
      "  LOC: Birtouta\n",
      "  LOC: Lyon\n",
      "  LOC: Milan\n",
      "  LOC: Hambourg\n",
      "  LOC: Baraki\n",
      "  ORG: MaghrebRail\n",
      "  ORG: NorthGate Infrastructure\n",
      "  ORG: USTHB\n",
      "  ORG: Université des Sciences et de la Technologie Houari-Boumédiène\n",
      "  PERS: Samir Ould-Kaci\n",
      "  PERS: Amina B.\n",
      "  PERS: Mourad\n",
      "\n",
      "========================================================================================================================\n",
      "RUN AR (arabcode.py)\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Chemin vers tes .py\n",
    "# =========================\n",
    "import sys, types, re, importlib\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\"  # dossier qui contient engcode.py / frcode.py / arabcode.py\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# =========================\n",
    "# 2) Petit \"nb_utils\" en mémoire (pas besoin de créer nb_utils.py)\n",
    "#    -> utilisé par run_from_previous_cell() dans tes scripts\n",
    "# =========================\n",
    "nb_utils = types.ModuleType(\"nb_utils\")\n",
    "\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "def get_previous_cell_input():\n",
    "    g = globals()\n",
    "    for k in (\"selected\", \"TOK_DOCS\", \"FINAL_DOCS\", \"DOCS\", \"TEXT_DOCS\", \"_\"):\n",
    "        if k in g and g[k] is not None:\n",
    "            return g[k]\n",
    "    return None\n",
    "\n",
    "def iter_sentences_from_input(data):\n",
    "    \"\"\"\n",
    "    Yield: (doc_name, page_idx, sent_idx, sent_text)\n",
    "    Supporte: TOK_DOCS/selected (pages->sentences_layout), FINAL_DOCS (list[{text}]), etc.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    # Cas 1: liste de docs avec pages (TOK_DOCS / selected)\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"pages\" in data[0]:\n",
    "        for d_i, doc in enumerate(data):\n",
    "            doc_name = doc.get(\"filename\") or doc.get(\"doc_id\") or f\"doc#{d_i}\"\n",
    "            pages = doc.get(\"pages\") or []\n",
    "            for p_i, pg in enumerate(pages):\n",
    "                page_idx = pg.get(\"page_index\", pg.get(\"page\", p_i+1))\n",
    "                sent_items = pg.get(\"sentences_layout\") or pg.get(\"sentences\") or pg.get(\"chunks\") or []\n",
    "                for s_i, s in enumerate(sent_items):\n",
    "                    if isinstance(s, dict):\n",
    "                        if s.get(\"is_sentence\") is False:\n",
    "                            continue\n",
    "                        sent = s.get(\"text\") or \"\"\n",
    "                    else:\n",
    "                        sent = str(s)\n",
    "                    yield doc_name, page_idx, s_i, sent\n",
    "        return\n",
    "\n",
    "    # Cas 2: FINAL_DOCS : list[{text, filename?}]\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"text\" in data[0]:\n",
    "        for i, d in enumerate(data):\n",
    "            doc_name = d.get(\"filename\") or d.get(\"doc_id\") or f\"doc#{i}\"\n",
    "            yield doc_name, None, None, d.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 3: dict {text:...}\n",
    "    if isinstance(data, dict) and \"text\" in data:\n",
    "        doc_name = data.get(\"filename\") or data.get(\"doc_id\") or \"doc\"\n",
    "        yield doc_name, None, None, data.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 4: string direct\n",
    "    if isinstance(data, str):\n",
    "        yield \"text\", None, None, data\n",
    "        return\n",
    "\n",
    "    raise TypeError(f\"Format d'entrée non supporté: {type(data)}\")\n",
    "\n",
    "nb_utils.detect_lang = detect_lang\n",
    "nb_utils.get_previous_cell_input = get_previous_cell_input\n",
    "nb_utils.iter_sentences_from_input = iter_sentences_from_input\n",
    "sys.modules[\"nb_utils\"] = nb_utils  # rend \"import nb_utils\" possible\n",
    "\n",
    "# =========================\n",
    "# 3) Import + reload tes 3 modules\n",
    "# =========================\n",
    "import engcode\n",
    "import frcode\n",
    "\n",
    "# arabcode peut échouer si camel_tools n'est pas installé => on skip proprement\n",
    "try:\n",
    "    import arabcode\n",
    "    HAVE_AR = True\n",
    "except Exception as e:\n",
    "    HAVE_AR = False\n",
    "    print(\"[warn] arabcode.py non chargé (dépendances manquantes ?). Détail:\", e)\n",
    "\n",
    "importlib.reload(engcode)\n",
    "importlib.reload(frcode)\n",
    "if HAVE_AR:\n",
    "    importlib.reload(arabcode)\n",
    "\n",
    "# =========================\n",
    "# 4) Exécution: chaque script filtre sa langue et print son output\n",
    "# =========================\n",
    "data = get_previous_cell_input()\n",
    "if data is None:\n",
    "    raise RuntimeError(\"Je ne trouve pas de données d'entrée. Assure-toi que la cellule précédente crée 'selected' (ou FINAL_DOCS / TOK_DOCS).\")\n",
    "\n",
    "MAX_SENTENCES_PER_LANG = None  # ex: 30 pour debug, ou None pour tout\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN EN (engcode.py)\")\n",
    "print(\"=\"*120)\n",
    "engcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN FR (frcode.py)\")\n",
    "print(\"=\"*120)\n",
    "frcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "if HAVE_AR:\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"RUN AR (arabcode.py)\")\n",
    "    print(\"=\"*120)\n",
    "    arabcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8f6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59526b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bd9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912a45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14d428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa213d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0269c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps: (absent)\n",
      "Type an Arabic sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: ميقرت تامالع + نيوانع + ماقرأ + يبرع صن — ةيبرعلا  دانسإلاو ،ةئزجتلاو ،ةغللا فاشتكاو ،يبرعلا صنلا جارختسا رابتخال مَّمصُم فلملا اذه .تانايكلا ىلع فّرعتلاو ،يوحنلا  ،جئاتنلا يف ًاجيجض ظحالتس ،لمعت مل اذإ ؟حيحص لكشب ةيمزراوخلا لمعت له :لاؤس .ميسقتلا يف ًءاطخأ وأ  12 21 213+ :فتاه ، test.user+tag@sub.domain.org ينورتكلإ ديرب :لاصتالا تانايب 34 56  1,234.56 :غلبملا ، INV-AR-0005 :ةروتافلا مقر ، 15/02/2026 :خيراتلا  .رئازجلا ،16000 رئازجلا ،دارم شوديد عراش ١٢ :ناونعلا  .يه امك ظفحُت نأ بجي )؟\n",
      "       ميقرت  NNP     lemma=ميقرت\n",
      "      تامالع  NNP     lemma=تامالع\n",
      "           +  NN      lemma=+\n",
      "      نيوانع  NNP     lemma=نيوانع\n",
      "           +  NN      lemma=+\n",
      "       ماقرأ  NNP     lemma=ماقرا\n",
      "           +  NN      lemma=+\n",
      "        يبرع  VB      lemma=بَرَع\n",
      "          صن  VB      lemma=صان\n",
      "           —  PUNCT   lemma=∅\n",
      "     ةيبرعلا  NNP     lemma=هيبرعلا\n",
      "    دانسإلاو  NNP     lemma=دانسالاو\n",
      "           ،  PUNCT   lemma=∅\n",
      "    ةئزجتلاو  NNP     lemma=هئزجتلاو\n",
      "           ،  PUNCT   lemma=∅\n",
      "       ةغللا  NNP     lemma=هغللا\n",
      "     فاشتكاو  NNP     lemma=فاشتكاو\n",
      "           ،  PUNCT   lemma=∅\n",
      "      يبرعلا  NNP     lemma=يبرعلا\n",
      "        صنلا  NNP     lemma=صنلا\n",
      "     جارختسا  NNP     lemma=جارختسا\n",
      "     رابتخال  NNP     lemma=رابتخال\n",
      "        ممصم  NNP     lemma=ممصم\n",
      "       فلملا  NN      lemma=مَلَأ\n",
      "          اذ  CC      lemma=إِذ\n",
      "           ه  PRP     lemma=ه\n",
      "           .  PUNCT   lemma=∅\n",
      "    تانايكلا  NNP     lemma=تانايكلا\n",
      "         ىلع  VB      lemma=لاع\n",
      "     فرعتلاو  NNP     lemma=فرعتلاو\n",
      "           ،  PUNCT   lemma=∅\n",
      "      يوحنلا  NNP     lemma=يوحنلا\n",
      "           ،  PUNCT   lemma=∅\n",
      "     جئاتنلا  NNP     lemma=جئاتنلا\n",
      "          يف  NNP     lemma=يف\n",
      "           ً  NN      lemma=\n",
      "       اجيجض  NNP     lemma=اجيجض\n",
      "      ظحالتس  NNP     lemma=ظحالتس\n",
      "           ،  PUNCT   lemma=∅\n",
      "        لمعت  VB      lemma=لَمَع\n",
      "          مل  VB      lemma=مَلّ\n",
      "         اذإ  CC      lemma=إِذا\n",
      "           ؟  PUNCT   lemma=∅\n",
      "        حيحص  NNP     lemma=حيحص\n",
      "        لكشب  NNP     lemma=لكشب\n",
      "  ةيمزراوخلا  NNP     lemma=هيمزراوخلا\n",
      "        لمعت  VB      lemma=لَمَع\n",
      "          له  IN      lemma=لِ\n",
      "           :  PUNCT   lemma=∅\n",
      "           ل  IN      lemma=ل\n",
      "         اؤس  NNP     lemma=اؤس\n",
      "           .  PUNCT   lemma=∅\n",
      "     ميسقتلا  NNP     lemma=ميسقتلا\n",
      "          يف  NNP     lemma=يف\n",
      "           ً  NN      lemma=\n",
      "       ءاطخأ  NNP     lemma=ءاطخا\n",
      "          وأ  NN      lemma=وا\n",
      "          12  CD      lemma=12\n",
      "          21  CD      lemma=21\n",
      "         213  CD      lemma=213\n",
      "           +  NN      lemma=+\n",
      "           :  PUNCT   lemma=∅\n",
      "         فتا  VB      lemma=فَتَأ\n",
      "           ه  PRP     lemma=ه\n",
      "           ،  PUNCT   lemma=∅\n",
      "        test  NN      lemma=test\n",
      "           .  PUNCT   lemma=∅\n",
      "        user  NN      lemma=user\n",
      "           +  NN      lemma=+\n",
      "         tag  NN      lemma=tag\n",
      "           @  PUNCT   lemma=∅\n",
      "         sub  NN      lemma=sub\n",
      "           .  PUNCT   lemma=∅\n",
      "      domain  NN      lemma=domain\n",
      "           .  PUNCT   lemma=∅\n",
      "         org  NN      lemma=org\n",
      "    ينورتكلإ  NNP     lemma=ينورتكلا\n",
      "        ديرب  NNP     lemma=ديرب\n",
      "           :  PUNCT   lemma=∅\n",
      "     لاصتالا  NNP     lemma=لاصتالا\n",
      "      تانايب  NNP     lemma=تانايب\n",
      "          34  CD      lemma=34\n",
      "          56  CD      lemma=56\n",
      "           1  CD      lemma=1\n",
      "           ,  PUNCT   lemma=∅\n",
      "         234  CD      lemma=234\n",
      "           .  PUNCT   lemma=∅\n",
      "          56  CD      lemma=56\n",
      "           :  PUNCT   lemma=∅\n",
      "      غلبملا  NNP     lemma=غلبملا\n",
      "           ،  PUNCT   lemma=∅\n",
      "      INV-AR  NNP     lemma=INV-AR\n",
      "           -  PUNCT   lemma=∅\n",
      "        0005  CD      lemma=0005\n",
      "           :  PUNCT   lemma=∅\n",
      "    ةروتافلا  NNP     lemma=هروتافلا\n",
      "         مقر  NN      lemma=مَقَرّ\n",
      "           ،  PUNCT   lemma=∅\n",
      "          15  CD      lemma=15\n",
      "           /  PUNCT   lemma=∅\n",
      "          02  CD      lemma=02\n",
      "           /  PUNCT   lemma=∅\n",
      "        2026  CD      lemma=2026\n",
      "           :  PUNCT   lemma=∅\n",
      "     خيراتلا  NNP     lemma=خيراتلا\n",
      "           .  PUNCT   lemma=∅\n",
      "     رئازجلا  NNP     lemma=رئازجلا\n",
      "           ،  PUNCT   lemma=∅\n",
      "       16000  CD      lemma=16000\n",
      "     رئازجلا  NNP     lemma=رئازجلا\n",
      "           ،  PUNCT   lemma=∅\n",
      "        دارم  NNP     lemma=دارم\n",
      "       شوديد  NNP     lemma=شوديد\n",
      "        عراش  NNP     lemma=عراش\n",
      "           ١  CD      lemma=١\n",
      "           ٢  CD      lemma=٢\n",
      "           :  PUNCT   lemma=∅\n",
      "     ناونعلا  NNP     lemma=ناونعلا\n",
      "           .  PUNCT   lemma=∅\n",
      "          يه  NNP     lemma=يه\n",
      "          ام  CC      lemma=أَم\n",
      "           ك  PRP     lemma=ك\n",
      "        ظفحت  NNP     lemma=ظفحت\n",
      "          نأ  NNP     lemma=نا\n",
      "          بج  NNP     lemma=بج\n",
      "           ي  PRP     lemma=ي\n",
      "           )  PUNCT   lemma=∅\n",
      "           ؟  PUNCT   lemma=∅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\moura\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER skipped (reason): list index out of range\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=73 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 2/73 = 2.74 %\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: ؛ ،( لثم ةيبرعلا زومرلا :ةظحالم  )ًالودج هبشُت( ةاذاحمب ةدمعأ ةلتك  ةيمكلا  فصولا  رعسلا  ةبيرضلا  يلامجإلا 2  صوصن ليلحت ةمدخ  45000.00  19%  107100.00 ةريثك ماقرأ نودب فصولل عبات رطس   1  رئازجلا - — Baker Street عيرس نحش  8000.00  0%  8000.00  Mixed-script stress line  AR/FR/EN in one line: رئازجلا Algiers — Rue Didouche Mourad — customerID42 — ديرب: facturation@entreprise.dz — URL: https://example.com/x?y=2\n",
      "           ؛  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "           (  PUNCT   lemma=∅\n",
      "         لثم  VB      lemma=لَثَم\n",
      "     ةيبرعلا  NNP     lemma=هيبرعلا\n",
      "      زومرلا  NNP     lemma=زومرلا\n",
      "           :  PUNCT   lemma=∅\n",
      "      ةظحالم  NNP     lemma=هظحالم\n",
      "           )  PUNCT   lemma=∅\n",
      "           ً  NN      lemma=\n",
      "       الودج  NN      lemma=وَدَج\n",
      "        هبشت  VB      lemma=هَبَش\n",
      "           (  PUNCT   lemma=∅\n",
      "     ةاذاحمب  NNP     lemma=هاذاحمب\n",
      "       ةدمعأ  NNP     lemma=هدمعا\n",
      "         ةلت  VB      lemma=هال\n",
      "           ك  PRP     lemma=ك\n",
      "      ةيمكلا  NNP     lemma=هيمكلا\n",
      "       فصولا  NN      lemma=فَصْل\n",
      "       رعسلا  NNP     lemma=رعسلا\n",
      "     ةبيرضلا  NNP     lemma=هبيرضلا\n",
      "    يلامجإلا  NNP     lemma=يلامجالا\n",
      "           2  CD      lemma=2\n",
      "        صوصن  NNP     lemma=صوصن\n",
      "       ليلحت  NNP     lemma=ليلحت\n",
      "        ةمدخ  NNP     lemma=همدخ\n",
      "       45000  CD      lemma=45000\n",
      "           .  PUNCT   lemma=∅\n",
      "          00  CD      lemma=00\n",
      "          19  CD      lemma=19\n",
      "           %  PUNCT   lemma=∅\n",
      "      107100  CD      lemma=107100\n",
      "           .  PUNCT   lemma=∅\n",
      "          00  CD      lemma=00\n",
      "        ةريث  NNP     lemma=هريث\n",
      "           ك  PRP     lemma=ك\n",
      "       ماقرأ  NNP     lemma=ماقرا\n",
      "        نودب  NNP     lemma=نودب\n",
      "       فصولل  NNP     lemma=فصولل\n",
      "        عبات  VB      lemma=عَبَأ\n",
      "         رطس  NNP     lemma=رطس\n",
      "           1  CD      lemma=1\n",
      "     رئازجلا  NNP     lemma=رئازجلا\n",
      "           -  PUNCT   lemma=∅\n",
      "           —  PUNCT   lemma=∅\n",
      "       Baker  NN      lemma=Baker\n",
      "      Street  NN      lemma=Street\n",
      "        عيرس  NNP     lemma=عيرس\n",
      "         نحش  VB      lemma=حَشا\n",
      "        8000  CD      lemma=8000\n",
      "           .  PUNCT   lemma=∅\n",
      "          00  CD      lemma=00\n",
      "           0  CD      lemma=0\n",
      "           %  PUNCT   lemma=∅\n",
      "        8000  CD      lemma=8000\n",
      "           .  PUNCT   lemma=∅\n",
      "          00  CD      lemma=00\n",
      "Mixed-script  NNP     lemma=Mixed-script\n",
      "      stress  NN      lemma=stress\n",
      "        line  NN      lemma=line\n",
      "          AR  NN      lemma=AR\n",
      "           /  PUNCT   lemma=∅\n",
      "          FR  NN      lemma=FR\n",
      "           /  PUNCT   lemma=∅\n",
      "          EN  NN      lemma=EN\n",
      "          in  NN      lemma=in\n",
      "         one  NN      lemma=one\n",
      "        line  NN      lemma=line\n",
      "           :  PUNCT   lemma=∅\n",
      "     رئازجلا  NNP     lemma=رئازجلا\n",
      "     Algiers  NN      lemma=Algiers\n",
      "           —  PUNCT   lemma=∅\n",
      "         Rue  NN      lemma=Rue\n",
      "    Didouche  NN      lemma=Didouche\n",
      "      Mourad  NN      lemma=Mourad\n",
      "           —  PUNCT   lemma=∅\n",
      "  customerID  NN      lemma=customerID\n",
      "          42  CD      lemma=42\n",
      "           —  PUNCT   lemma=∅\n",
      "        ديرب  NNP     lemma=ديرب\n",
      "           :  PUNCT   lemma=∅\n",
      " facturation  NN      lemma=facturation\n",
      "           @  PUNCT   lemma=∅\n",
      "  entreprise  NN      lemma=entreprise\n",
      "           .  PUNCT   lemma=∅\n",
      "          dz  NN      lemma=dz\n",
      "           —  PUNCT   lemma=∅\n",
      "         URL  NN      lemma=URL\n",
      "           :  PUNCT   lemma=∅\n",
      "       https  NN      lemma=https\n",
      "           :  PUNCT   lemma=∅\n",
      "           /  PUNCT   lemma=∅\n",
      "           /  PUNCT   lemma=∅\n",
      "     example  NN      lemma=example\n",
      "           .  PUNCT   lemma=∅\n",
      "         com  NN      lemma=com\n",
      "           /  PUNCT   lemma=∅\n",
      "           x  NN      lemma=x\n",
      "           ?  PUNCT   lemma=∅\n",
      "           y  NN      lemma=y\n",
      "           =  NN      lemma==\n",
      "           2  CD      lemma=2\n",
      "\n",
      "NER skipped (reason): list index out of range\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=55 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 0/55 = 0.00 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# ============================================================\n",
    "#  AR - Token/POS/Lemma (CAMeL Analyzer) + NER (pretrained)\n",
    "#  Fixes ajoutés (global, sans ML entraîné par toi) :\n",
    "#   1) Segmentation clitiques (و/ف + ب/ك/ل + suffixes pronoms)\n",
    "LOCAL_DEPS_DIR = os.path.join(os.getcwd(), \".pylibs\")\n",
    "if os.path.isdir(LOCAL_DEPS_DIR) and LOCAL_DEPS_DIR not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_DEPS_DIR)\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Local deps:\", LOCAL_DEPS_DIR if os.path.isdir(LOCAL_DEPS_DIR) else \"(absent)\")\n",
    "\n",
    "def _install_help():\n",
    "    py = sys.executable\n",
    "    print(\"\\n[install help] Sans venv, local dans .pylibs (utilise CE python):\")\n",
    "    print(f'  \"{py}\" -m pip install --upgrade --no-user-cfg --target \"{LOCAL_DEPS_DIR}\" camel-tools transformers tokenizers')\n",
    "    print(f'  \"{py}\" -m camel_tools.cli.camel_data -i morphology-db-msa-r13')\n",
    "    print(f'  \"{py}\" -m camel_tools.cli.camel_data -i ner-arabert')\n",
    "    print(\"\\nSi tu as l’erreur: Can not combine '--user' and '--target'\")\n",
    "    print(\"  PowerShell:\")\n",
    "    print('    $env:PIP_CONFIG_FILE=\"NUL\"')\n",
    "    print(\"    Remove-Item Env:PIP_USER -ErrorAction SilentlyContinue\")\n",
    "    print('    py -3.11 -m pip install --upgrade --no-user-cfg --target .\\\\.pylibs camel-tools')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Imports CAMeL\n",
    "# ----------------------------\n",
    "try:\n",
    "    from camel_tools.morphology.database import MorphologyDB\n",
    "    from camel_tools.morphology.analyzer import Analyzer\n",
    "except Exception as e:\n",
    "    print(\"\\n[error] camel_tools not available:\", e)\n",
    "    _install_help()\n",
    "    raise\n",
    "\n",
    "# NER pretrained\n",
    "try:\n",
    "    from camel_tools.ner import NERecognizer\n",
    "except Exception:\n",
    "    NERecognizer = None\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build analyzer (requires morphology-db-msa-r13)\n",
    "# ----------------------------\n",
    "try:\n",
    "    DB = MorphologyDB.builtin_db()\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\n",
    "        \"CAMeL morphology DB not found. Run:\\n\"\n",
    "        f'  \"{sys.executable}\" -m camel_tools.cli.camel_data -i morphology-db-msa-r13\\n'\n",
    "    ) from e\n",
    "\n",
    "ANALYZER = Analyzer(DB, backoff=\"NOAN_PROP\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Tokenization (Arabic words + digits + Latin + single-char punctuation)\n",
    "# ----------------------------\n",
    "AR_BASE = r\"\\u0621-\\u063A\\u0641-\\u064A\\u066E-\\u066F\\u0671-\\u06D3\\u06FA-\\u06FC\"\n",
    "AR_DIAC = r\"\\u064B-\\u065F\\u0670\\u0640\"  # harakat + shadda + superscript alef + tatweel\n",
    "AR_WORD = rf\"(?:[{AR_BASE}][{AR_DIAC}]*)+\"\n",
    "\n",
    "PUNCT_SET = set(list(\".,;:!?()[]{}<>\\\"'“”‘’«»…—–-\")) | {\"،\", \"؛\", \"؟\", \"٪\", \"%\", \"ـ\", \"/\", \"\\\\\", \"|\", \"@\", \"…\"}\n",
    "TOKEN_RE = re.compile(rf\"({AR_WORD}|[0-9]+|[A-Za-z]+(?:['’\\-][A-Za-z]+)*|[^\\s])\", re.UNICODE)\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return TOKEN_RE.findall(text or \"\")\n",
    "\n",
    "def is_punct(tok: str) -> bool:\n",
    "    return tok in PUNCT_SET\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Normalization helpers (0-ML)\n",
    "# ----------------------------\n",
    "_DIACRITICS_RE = re.compile(rf\"[{AR_DIAC}]\")\n",
    "HAMZA_CHARS = set(\"ءأإؤئٱ\")\n",
    "\n",
    "def strip_diacritics(s: str) -> str:\n",
    "    return _DIACRITICS_RE.sub(\"\", s or \"\")\n",
    "\n",
    "def norm_alef(s: str) -> str:\n",
    "    return (s or \"\").replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\").replace(\"ٱ\", \"ا\")\n",
    "\n",
    "def has_hamza(s: str) -> bool:\n",
    "    return any(ch in HAMZA_CHARS for ch in (s or \"\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) CAMeL POS -> Penn-like\n",
    "# ----------------------------\n",
    "AR_POS2PENN = {\n",
    "    \"noun\": \"NN\",\n",
    "    \"noun_prop\": \"NNP\",\n",
    "    \"verb\": \"VB\",\n",
    "    \"adj\": \"JJ\",\n",
    "    \"adv\": \"RB\",\n",
    "    \"prep\": \"IN\",\n",
    "    \"conj\": \"CC\",\n",
    "    \"pron\": \"PRP\",\n",
    "    \"pron_dem\": \"PRP\",\n",
    "    \"det\": \"DT\",\n",
    "    \"num\": \"CD\",\n",
    "    \"part\": \"RP\",\n",
    "    \"part_fut\": \"RP\",\n",
    "    \"abbrev\": \"NN\",\n",
    "}\n",
    "\n",
    "def penn_from_analysis(a: Dict) -> str:\n",
    "    pos = (a.get(\"pos\") or \"\").lower()\n",
    "    return AR_POS2PENN.get(pos, \"NN\")\n",
    "\n",
    "def lemma_from_analysis(a: Dict, fallback_word: str) -> str:\n",
    "    return a.get(\"lex\") or a.get(\"lemma\") or fallback_word\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Strong closed-class overrides (0-ML)\n",
    "# ----------------------------\n",
    "FUT_PARTS = {\"سوف\"}\n",
    "NEG_PARTS = {\"لم\", \"لن\", \"لا\", \"ما\"}\n",
    "ASPECT_PARTS = {\"قد\"}\n",
    "Q_PARTS = {\"هل\"}\n",
    "REL_PRON = {\"الذي\", \"التي\", \"الذين\", \"اللذين\", \"اللذان\", \"اللتان\", \"اللاتي\", \"اللواتي\"}\n",
    "DEM_WORDS = {\"هذا\", \"هذه\", \"هؤلاء\", \"ذلك\", \"تلك\", \"هٰذا\", \"هٰذه\"}\n",
    "PREP_WORDS_EXT = {\"إلى\", \"في\", \"على\", \"من\", \"عن\", \"مع\", \"حتى\", \"عبر\", \"بين\", \"قبل\", \"بعد\", \"دون\", \"حول\", \"عند\", \"لدى\", \"مثل\", \"خلال\"}\n",
    "CONJ_WORDS = {\"و\", \"ف\", \"ثم\", \"أو\", \"لكن\", \"بل\", \"أم\"}\n",
    "NEG_ADJ = {\"غير\"}\n",
    "FIX_NOUNS = {\"بعض\"}\n",
    "\n",
    "RX_INNA_CLITIC = re.compile(r\"^(إن|أن|لأن)(ه|ها|هم|هن|كما|كم|كن|نا)?$\", re.UNICODE)\n",
    "RX_LAKIN_CLITIC = re.compile(r\"^لكن(ه|ها|هم|هن|كما|كم|كن|نا)?$\", re.UNICODE)\n",
    "\n",
    "def override_tag(tok: str) -> Optional[Tuple[str, str]]:\n",
    "    if not tok or is_punct(tok):\n",
    "        return None\n",
    "\n",
    "    t0 = strip_diacritics(tok)\n",
    "\n",
    "    if t0 in FIX_NOUNS:\n",
    "        return (\"NN\", t0)\n",
    "\n",
    "    if t0 in CONJ_WORDS:\n",
    "        return (\"CC\", t0)\n",
    "\n",
    "    if t0 in FUT_PARTS or t0 in NEG_PARTS or t0 in ASPECT_PARTS or t0 in Q_PARTS:\n",
    "        return (\"RP\", t0)\n",
    "\n",
    "    if t0 in REL_PRON:\n",
    "        return (\"WDT\", t0)\n",
    "\n",
    "    if t0 in DEM_WORDS:\n",
    "        return (\"PRP\", t0)\n",
    "\n",
    "    if t0 in PREP_WORDS_EXT:\n",
    "        return (\"IN\", t0)\n",
    "\n",
    "    if t0 in NEG_ADJ:\n",
    "        return (\"JJ\", t0)\n",
    "\n",
    "    m = RX_INNA_CLITIC.match(t0)\n",
    "    if m:\n",
    "        return (\"RP\", m.group(1))\n",
    "\n",
    "    m2 = RX_LAKIN_CLITIC.match(t0)\n",
    "    if m2:\n",
    "        return (\"CC\", \"لكن\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Segmentation clitiques (fix global)\n",
    "# ----------------------------\n",
    "PROCLITIC_CONJ = {\"و\", \"ف\"}\n",
    "PROCLITIC_PREP = {\"ب\", \"ك\", \"ل\"}\n",
    "FUT_CLITIC = {\"س\"}  # سـ (future)\n",
    "\n",
    "# suffix pronouns (triés par longueur)\n",
    "PRON_SUFFIXES = [\n",
    "    \"كما\", \"كم\", \"كن\",\n",
    "    \"هما\", \"هم\", \"هن\",\n",
    "    \"ها\", \"ه\",\n",
    "    \"نا\", \"ني\",\n",
    "    \"ي\", \"ك\"\n",
    "]\n",
    "\n",
    "def _looks_like_verb_imperfect(core: str) -> bool:\n",
    "    # ي/ت/أ/ن + au moins 3 lettres ensuite\n",
    "    return bool(core) and core[0] in {\"ي\", \"ت\", \"أ\", \"ن\"} and len(core) >= 4\n",
    "\n",
    "def _safe_split_conj(tok0: str) -> bool:\n",
    "    # split و/ف seulement si reste commence par:\n",
    "    # - préposition fermée (من/في/على/عن/إلى/...)\n",
    "    # - ال...\n",
    "    # - verbe imperfect (ي/ت/أ/ن...)\n",
    "    if len(tok0) < 3:\n",
    "        return False\n",
    "    rem = tok0[1:]\n",
    "    if rem.startswith(\"ال\"):\n",
    "        return True\n",
    "    if rem in PREP_WORDS_EXT:\n",
    "        return True\n",
    "    if any(rem.startswith(x) for x in (\"من\", \"في\", \"على\", \"عن\", \"إلى\")):\n",
    "        return True\n",
    "    if _looks_like_verb_imperfect(rem):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _best_analysis_for(form_undiac: str) -> List[Dict]:\n",
    "    try:\n",
    "        return ANALYZER.analyze(form_undiac)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _char_overlap_ratio(a: str, b: str) -> float:\n",
    "    sa = set(a) - set(\"ـ\")\n",
    "    sb = set(b) - set(\"ـ\")\n",
    "    if not sa:\n",
    "        return 0.0\n",
    "    return len(sa & sb) / max(1, len(sa))\n",
    "\n",
    "def pick_analysis(\n",
    "    word_raw: str,\n",
    "    analyses: List[Dict],\n",
    "    prev_tag: Optional[str] = None,\n",
    "    prev_word: Optional[str] = None,\n",
    "    next_word: Optional[str] = None,\n",
    "    is_first_content: bool = False\n",
    ") -> Optional[Dict]:\n",
    "    if not analyses:\n",
    "        return None\n",
    "\n",
    "    w_raw = word_raw or \"\"\n",
    "    w = strip_diacritics(w_raw)\n",
    "    w_norm = norm_alef(w)\n",
    "\n",
    "    nw = strip_diacritics(next_word or \"\")\n",
    "    pw = strip_diacritics(prev_word or \"\")\n",
    "\n",
    "    w_len = len(w)\n",
    "    definite_article = w.startswith(\"ال\")\n",
    "    imperfect_prefix = _looks_like_verb_imperfect(w)\n",
    "    looks_like_perfect = (w_len in {3, 4}) and (not imperfect_prefix) and (not definite_article)\n",
    "\n",
    "    # patterns helpful for verbs like يَروا / يرون\n",
    "    ends_verb_plural = w.endswith(\"وا\") or w.endswith(\"ون\") or w.endswith(\"ين\")\n",
    "\n",
    "    has_simple_verb = any(((a.get(\"pos\") or \"\").lower() == \"verb\") for a in analyses)\n",
    "\n",
    "    best = None\n",
    "    best_score = -10**9\n",
    "\n",
    "    for a in analyses:\n",
    "        pos = (a.get(\"pos\") or \"\").lower()\n",
    "        lem = lemma_from_analysis(a, w_raw)\n",
    "        lem0 = strip_diacritics(lem)\n",
    "        lem_norm = norm_alef(lem0)\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # closed-class boost\n",
    "        if pos in {\"prep\", \"conj\", \"det\", \"pron\", \"pron_dem\", \"part\", \"part_fut\", \"num\"}:\n",
    "            score += 4\n",
    "\n",
    "        # strong imperfect preference\n",
    "        if imperfect_prefix:\n",
    "            if pos == \"verb\":\n",
    "                score += 10\n",
    "            else:\n",
    "                score -= 6\n",
    "\n",
    "        if ends_verb_plural:\n",
    "            if pos == \"verb\":\n",
    "                score += 6\n",
    "            else:\n",
    "                score -= 4\n",
    "\n",
    "        # context after preposition\n",
    "        if prev_tag == \"IN\":\n",
    "            if pos in {\"noun\", \"adj\", \"noun_prop\"}:\n",
    "                score += 4\n",
    "            if pos == \"verb\":\n",
    "                score -= 2\n",
    "\n",
    "        # definite article\n",
    "        if definite_article:\n",
    "            if pos in {\"noun\", \"adj\"}:\n",
    "                score += 4\n",
    "            if pos == \"verb\":\n",
    "                score -= 3\n",
    "            if pos == \"noun_prop\":\n",
    "                score -= 2\n",
    "\n",
    "        # sentence-initial bias\n",
    "        if is_first_content:\n",
    "            if pos == \"verb\":\n",
    "                score += 2\n",
    "            if pos == \"noun_prop\":\n",
    "                score -= 1\n",
    "\n",
    "        # if next word has definite article, current verb less likely (weak)\n",
    "        if nw.startswith(\"ال\") and pos == \"verb\":\n",
    "            score += 1\n",
    "\n",
    "        # prefer lemma similar to surface\n",
    "        score += int(10 * _char_overlap_ratio(lem_norm, w_norm))\n",
    "\n",
    "        # penalty: شدة في lemma بدون شدة في surface (fix مصر vs مُصِرّ et hallucinations)\n",
    "        if \"ّ\" in (lem or \"\") and \"ّ\" not in (w_raw or \"\"):\n",
    "            score -= 6\n",
    "\n",
    "        # verbs: sanity\n",
    "        if pos == \"verb\":\n",
    "            if looks_like_perfect and lem_norm == w_norm:\n",
    "                score += 6\n",
    "            # hamza consistency (mild)\n",
    "            if has_hamza(w_raw) and not has_hamza(lem):\n",
    "                score -= 2\n",
    "\n",
    "        # noun_prop: prefer when lemma exactly matches surface (proper nouns)\n",
    "        if pos == \"noun_prop\" and lem_norm == w_norm:\n",
    "            score += 8\n",
    "\n",
    "        # if we have a verb candidate set, penalize rare non-verb picks for imperfect\n",
    "        if has_simple_verb and imperfect_prefix and pos in {\"noun_prop\", \"noun\"}:\n",
    "            score -= 3\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = a\n",
    "\n",
    "    # last-ditch: if selected not verb but word clearly imperfect and there exists verb analysis => pick best verb\n",
    "    if imperfect_prefix and best is not None and (best.get(\"pos\",\"\").lower() != \"verb\"):\n",
    "        verb_as = [a for a in analyses if (a.get(\"pos\") or \"\").lower() == \"verb\"]\n",
    "        if verb_as:\n",
    "            vb = None\n",
    "            vb_sc = -10**9\n",
    "            for a in verb_as:\n",
    "                lem = lemma_from_analysis(a, w_raw)\n",
    "                lem_norm = norm_alef(strip_diacritics(lem))\n",
    "                sc = int(10 * _char_overlap_ratio(lem_norm, w_norm))\n",
    "                if \"ّ\" in (lem or \"\") and \"ّ\" not in (w_raw or \"\"):\n",
    "                    sc -= 6\n",
    "                if sc > vb_sc:\n",
    "                    vb_sc = sc\n",
    "                    vb = a\n",
    "            if vb is not None:\n",
    "                best = vb\n",
    "\n",
    "    return best\n",
    "\n",
    "def split_proclitics(tok: str) -> Tuple[List[Tuple[str,str,str]], str]:\n",
    "    \"\"\"\n",
    "    Return (prefix_morphemes, core_undiac)\n",
    "    prefix morphemes: (surface, tag, lemma)\n",
    "    \"\"\"\n",
    "    t0 = strip_diacritics(tok)\n",
    "    prefixes: List[Tuple[str,str,str]] = []\n",
    "\n",
    "    # 1) و/ف (conj) only if safe\n",
    "    if t0 and t0[0] in PROCLITIC_CONJ and _safe_split_conj(t0):\n",
    "        prefixes.append((t0[0], \"CC\", t0[0]))\n",
    "        t0 = t0[1:]\n",
    "\n",
    "    # 2) س (future) if followed by imperfect verb\n",
    "    if t0 and t0[0] in FUT_CLITIC and len(t0) >= 3 and _looks_like_verb_imperfect(t0[1:]):\n",
    "        prefixes.append((t0[0], \"RP\", t0[0]))\n",
    "        t0 = t0[1:]\n",
    "\n",
    "    # 3) ب/ك/ل (prep) : split if remainder has analyses as noun/prop/adj OR starts with ال OR is known prep base\n",
    "    if t0 and t0[0] in PROCLITIC_PREP and len(t0) >= 3:\n",
    "        rem = t0[1:]\n",
    "        do_split = False\n",
    "        if rem.startswith(\"ال\") or rem in PREP_WORDS_EXT:\n",
    "            do_split = True\n",
    "        else:\n",
    "            # try analyze remainder; if it yields noun/noun_prop/adj => split\n",
    "            ans = _best_analysis_for(rem)\n",
    "            if any((a.get(\"pos\") or \"\").lower() in {\"noun\", \"noun_prop\", \"adj\"} for a in ans):\n",
    "                # BUT avoid splitting if whole word is a strong noun_prop match (e.g., بشار)\n",
    "                whole = _best_analysis_for(t0)\n",
    "                best_whole = pick_analysis(tok, whole)\n",
    "                if not (best_whole and (best_whole.get(\"pos\",\"\").lower()==\"noun_prop\") and\n",
    "                        (norm_alef(strip_diacritics(lemma_from_analysis(best_whole, t0))) == norm_alef(rem) or\n",
    "                         norm_alef(strip_diacritics(lemma_from_analysis(best_whole, t0))) == norm_alef(t0))):\n",
    "                    do_split = True\n",
    "\n",
    "        if do_split:\n",
    "            prefixes.append((t0[0], \"IN\", t0[0]))  # display as preposition\n",
    "            t0 = rem\n",
    "\n",
    "    return prefixes, t0\n",
    "\n",
    "def split_enclitics(core_undiac: str) -> Tuple[str, List[Tuple[str,str,str]]]:\n",
    "    \"\"\"\n",
    "    Split suffix pronouns from the right.\n",
    "    Return (stem, suffix_morphemes) where suffix are in correct reading order.\n",
    "    \"\"\"\n",
    "    stem = core_undiac\n",
    "    suffixes: List[Tuple[str,str,str]] = []\n",
    "\n",
    "    # iterative stripping: longest first\n",
    "    changed = True\n",
    "    while changed and stem:\n",
    "        changed = False\n",
    "\n",
    "        # special connector \"و\" before a pronoun like ...وها / ...وهم / ...وكم\n",
    "        if stem.endswith(\"وها\"):\n",
    "            stem = stem[:-3]\n",
    "            suffixes.insert(0, (\"و\", \"PRP\", \"و\"))\n",
    "            suffixes.insert(1, (\"ها\", \"PRP\", \"ها\"))\n",
    "            changed = True\n",
    "            continue\n",
    "        if stem.endswith(\"وهم\"):\n",
    "            stem = stem[:-3]\n",
    "            suffixes.insert(0, (\"و\", \"PRP\", \"و\"))\n",
    "            suffixes.insert(1, (\"هم\", \"PRP\", \"هم\"))\n",
    "            changed = True\n",
    "            continue\n",
    "        if stem.endswith(\"وكم\"):\n",
    "            stem = stem[:-3]\n",
    "            suffixes.insert(0, (\"و\", \"PRP\", \"و\"))\n",
    "            suffixes.insert(1, (\"كم\", \"PRP\", \"كم\"))\n",
    "            changed = True\n",
    "            continue\n",
    "\n",
    "        for suf in PRON_SUFFIXES:\n",
    "            if stem.endswith(suf) and len(stem) > len(suf) + 1:\n",
    "                stem = stem[:-len(suf)]\n",
    "                suffixes.insert(0, (suf, \"PRP\", suf))\n",
    "                changed = True\n",
    "                break\n",
    "\n",
    "    return stem, suffixes\n",
    "\n",
    "# -------------------- NER helpers (pretrained + deterministic fixes) --------------------\n",
    "_NER = None\n",
    "\n",
    "def get_ner():\n",
    "    global _NER\n",
    "    if _NER is not None:\n",
    "        return _NER\n",
    "    if NERecognizer is None:\n",
    "        raise ImportError(\n",
    "            \"camel_tools.ner is not available.\\n\"\n",
    "            \"Install and download data:\\n\"\n",
    "            f'  \"{sys.executable}\" -m pip install --upgrade --no-user-cfg --target \"{LOCAL_DEPS_DIR}\" camel-tools transformers tokenizers\\n'\n",
    "            f'  \"{sys.executable}\" -m camel_tools.cli.camel_data -i ner-arabert\\n'\n",
    "        )\n",
    "    _NER = NERecognizer.pretrained()\n",
    "    return _NER\n",
    "\n",
    "def ner_predict_tokens(tokens: List[str]) -> List[str]:\n",
    "    ner = get_ner()\n",
    "    if hasattr(ner, \"predict_sentence\"):\n",
    "        return ner.predict_sentence(tokens)\n",
    "    if hasattr(ner, \"predict\"):\n",
    "        return ner.predict(tokens)\n",
    "    raise AttributeError(\"NERecognizer has no predict_sentence/predict method in this version.\")\n",
    "\n",
    "TITLE_NORMS = {\n",
    "    \"سيد\", \"السيد\",\n",
    "    \"سيدة\", \"السيدة\",\n",
    "    \"دكتور\", \"الدكتور\",\n",
    "    \"دكتورة\", \"الدكتورة\",\n",
    "    \"استاذ\", \"الأستاذ\", \"الاستاذ\", \"استاذة\", \"الأستاذة\", \"الاستاذة\",\n",
    "    \"مهندس\", \"المهندس\",\n",
    "    \"شيخ\", \"الشيخ\",\n",
    "    \"رئيس\", \"الرئيس\",\n",
    "    \"وزير\", \"الوزير\",\n",
    "    \"امير\", \"الأمير\", \"الامير\",\n",
    "    \"ملك\", \"الملك\",\n",
    "}\n",
    "\n",
    "def norm_for_match(tok: str) -> str:\n",
    "    return norm_alef(strip_diacritics(tok))\n",
    "\n",
    "def fix_titles(tokens: List[str], labels: List[str]) -> List[str]:\n",
    "    labels = labels[:]\n",
    "    for i in range(1, len(tokens)):\n",
    "        if labels[i].startswith(\"B-PERS\") and labels[i - 1] == \"O\":\n",
    "            prev_norm = norm_for_match(tokens[i - 1])\n",
    "            prev_norm2 = prev_norm[2:] if prev_norm.startswith(\"ال\") and len(prev_norm) > 2 else prev_norm\n",
    "            if prev_norm in TITLE_NORMS or prev_norm2 in TITLE_NORMS:\n",
    "                labels[i - 1] = \"B-PERS\"\n",
    "                labels[i] = \"I-PERS\"\n",
    "    return labels\n",
    "\n",
    "def add_year_dates(tokens: List[str], labels: List[str]) -> List[str]:\n",
    "    labels = labels[:]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok.isdigit() and len(tok) == 4:\n",
    "            y = int(tok)\n",
    "            if 1500 <= y <= 2100 and labels[i] == \"O\":\n",
    "                labels[i] = \"B-DATE\"\n",
    "    return labels\n",
    "\n",
    "def enforce_bio(labels: List[str]) -> List[str]:\n",
    "    out = labels[:]\n",
    "    for i in range(len(out)):\n",
    "        lab = out[i]\n",
    "        if lab.startswith(\"I-\"):\n",
    "            typ = lab[2:]\n",
    "            if i == 0 or out[i - 1] == \"O\" or out[i - 1][2:] != typ:\n",
    "                out[i] = \"B-\" + typ\n",
    "    return out\n",
    "\n",
    "def strip_entity_clitics_for_display(tok: str, lemma: str) -> str:\n",
    "    t = strip_diacritics(tok)\n",
    "    l = strip_diacritics(lemma or \"\")\n",
    "\n",
    "    if len(t) > 2 and (t.startswith(\"وال\") or t.startswith(\"فال\")):\n",
    "        return tok[1:]\n",
    "\n",
    "    if len(t) > 1 and t.startswith(\"ب\"):\n",
    "        rem = t[1:]\n",
    "        rem2 = rem[2:] if rem.startswith(\"ال\") and len(rem) > 2 else rem\n",
    "        l2 = l[2:] if l.startswith(\"ال\") and len(l) > 2 else l\n",
    "        if l2 == rem2 and rem:\n",
    "            return tok[1:]\n",
    "\n",
    "    return tok\n",
    "\n",
    "def bio_to_entities(tokens: List[str], labels: List[str], morph_rows: List[Dict[str, str]]) -> List[Tuple[str, str]]:\n",
    "    ents = []\n",
    "    i = 0\n",
    "    n = len(tokens)\n",
    "\n",
    "    def pos_of(k: int) -> str:\n",
    "        return (morph_rows[k].get(\"tag\") or \"\").strip()\n",
    "\n",
    "    def lemma_of(k: int) -> str:\n",
    "        return (morph_rows[k].get(\"lemma\") or \"\").strip()\n",
    "\n",
    "    while i < n:\n",
    "        lab = labels[i]\n",
    "        if lab == \"O\" or \"-\" not in lab:\n",
    "            i += 1\n",
    "            continue\n",
    "        pref, typ = lab.split(\"-\", 1)\n",
    "        if pref != \"B\":\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        j = i + 1\n",
    "        while j < n and labels[j] == f\"I-{typ}\":\n",
    "            j += 1\n",
    "\n",
    "        start, end = i, j\n",
    "        while start < end and pos_of(start) in {\"IN\", \"CC\", \"RP\"}:\n",
    "            start += 1\n",
    "        while end > start and pos_of(end - 1) in {\"IN\", \"CC\", \"RP\"}:\n",
    "            end -= 1\n",
    "\n",
    "        if start < end:\n",
    "            parts = []\n",
    "            for k in range(start, end):\n",
    "                if is_punct(tokens[k]):\n",
    "                    continue\n",
    "                parts.append(strip_entity_clitics_for_display(tokens[k], lemma_of(k)))\n",
    "            text = \" \".join(p for p in parts if p)\n",
    "            if text:\n",
    "                ents.append((typ, text))\n",
    "\n",
    "        i = j\n",
    "\n",
    "    return ents\n",
    "\n",
    "# -------------------- Audits (heuristiques) --------------------\n",
    "def _estimate_clitic_count(undiac: str) -> int:\n",
    "    if not undiac:\n",
    "        return 0\n",
    "    c = 0\n",
    "    if undiac.startswith((\"و\", \"ف\")) and len(undiac) > 2 and _safe_split_conj(undiac):\n",
    "        c += 1\n",
    "        undiac = undiac[1:]\n",
    "    if undiac.startswith((\"ب\", \"ك\", \"ل\")) and len(undiac) > 2:\n",
    "        c += 1\n",
    "        undiac = undiac[1:]\n",
    "    # suffix pronouns\n",
    "    for suf in PRON_SUFFIXES:\n",
    "        if undiac.endswith(suf):\n",
    "            c += 1\n",
    "            break\n",
    "    if undiac.endswith((\"وها\", \"وهم\", \"وكم\")):\n",
    "        c += 2\n",
    "    return c\n",
    "\n",
    "def _lemma_suspicious(word_raw: str, lemma: str) -> bool:\n",
    "    w = norm_alef(strip_diacritics(word_raw))\n",
    "    l = norm_alef(strip_diacritics(lemma or \"\"))\n",
    "    if not w or not l:\n",
    "        return False\n",
    "    # suspicious if lemma contains shadda but word doesn't\n",
    "    if \"ّ\" in (lemma or \"\") and \"ّ\" not in (word_raw or \"\"):\n",
    "        return True\n",
    "    # suspicious if overlap too low\n",
    "    if _char_overlap_ratio(l, w) < 0.34 and len(l) >= 4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# -------------------- Main analysis --------------------\n",
    "def analyze_sentence(text: str):\n",
    "    toks = simple_tokenize(text)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"INPUT:\", text)\n",
    "\n",
    "    prev_tag = None\n",
    "    prev_word = \"\"\n",
    "    seen_content = False\n",
    "\n",
    "    # These rows are for NER trimming, so keep 1 row per ORIGINAL token\n",
    "    morph_rows_for_ner: List[Dict[str, str]] = []\n",
    "\n",
    "    # For display, we may output multiple rows per token (segmentation)\n",
    "    display_rows: List[Tuple[str, str, str]] = []\n",
    "\n",
    "    seg_ok = 0\n",
    "    seg_err = 0\n",
    "    lem_susp = 0\n",
    "    lem_total = 0\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        if is_punct(tok):\n",
    "            display_rows.append((tok, \"PUNCT\", \"∅\"))\n",
    "            morph_rows_for_ner.append({\"tok\": tok, \"tag\": \"PUNCT\", \"lemma\": \"\"})\n",
    "            prev_tag = None\n",
    "            prev_word = \"\"\n",
    "            continue\n",
    "\n",
    "        if tok.isdigit():\n",
    "            display_rows.append((tok, \"CD\", tok))\n",
    "            morph_rows_for_ner.append({\"tok\": tok, \"tag\": \"CD\", \"lemma\": tok})\n",
    "            prev_tag = \"CD\"\n",
    "            prev_word = tok\n",
    "            seen_content = True\n",
    "            continue\n",
    "\n",
    "        # next word (skip punctuation)\n",
    "        next_word = \"\"\n",
    "        for j in range(i + 1, len(toks)):\n",
    "            if not is_punct(toks[j]):\n",
    "                next_word = toks[j]\n",
    "                break\n",
    "\n",
    "        # Closed-class overrides (single token)\n",
    "        ov = override_tag(tok)\n",
    "        if ov is not None:\n",
    "            tag, lem = ov\n",
    "            display_rows.append((tok, tag, lem))\n",
    "            morph_rows_for_ner.append({\"tok\": tok, \"tag\": tag, \"lemma\": lem})\n",
    "            prev_tag = tag\n",
    "            prev_word = strip_diacritics(tok)\n",
    "            seen_content = True\n",
    "            continue\n",
    "\n",
    "        # ---- segmentation (fix) ----\n",
    "        prefixes, core0 = split_proclitics(tok)\n",
    "        stem0, suffixes = split_enclitics(core0)\n",
    "\n",
    "        # segmentation audit\n",
    "        clitic_est = _estimate_clitic_count(strip_diacritics(tok))\n",
    "        if clitic_est >= 2 and (len(prefixes) + len(suffixes) == 0):\n",
    "            seg_err += 1\n",
    "        else:\n",
    "            seg_ok += 1\n",
    "\n",
    "        # print prefixes rows\n",
    "        for (p_surf, p_tag, p_lem) in prefixes:\n",
    "            display_rows.append((p_surf, p_tag, p_lem))\n",
    "\n",
    "        # Analyze STEM only (important fix for بمصر, ومنهم, ... pronoun verbs)\n",
    "        analyze_form = stem0 if stem0 else core0\n",
    "        analyses = _best_analysis_for(analyze_form)\n",
    "\n",
    "        best = pick_analysis(\n",
    "            word_raw=analyze_form,\n",
    "            analyses=analyses,\n",
    "            prev_tag=prev_tag,\n",
    "            prev_word=prev_word,\n",
    "            next_word=next_word,\n",
    "            is_first_content=(not seen_content),\n",
    "        )\n",
    "\n",
    "        if best is None:\n",
    "            tag = \"NN\"\n",
    "            lemma = analyze_form\n",
    "        else:\n",
    "            tag = penn_from_analysis(best)\n",
    "            lemma = lemma_from_analysis(best, analyze_form)\n",
    "\n",
    "        display_rows.append((analyze_form if analyze_form else tok, tag, lemma))\n",
    "\n",
    "        # suffix pronouns (display)\n",
    "        for (s_surf, s_tag, s_lem) in suffixes:\n",
    "            display_rows.append((s_surf, s_tag, s_lem))\n",
    "\n",
    "        # For NER row (1 per original token): store the chosen tag/lemma for the whole token\n",
    "        # We store the STEM lemma to avoid misleading NER trimming/cleaning.\n",
    "        morph_rows_for_ner.append({\"tok\": tok, \"tag\": tag, \"lemma\": lemma})\n",
    "\n",
    "        # lemma audit\n",
    "        if not is_punct(tok):\n",
    "            lem_total += 1\n",
    "            if _lemma_suspicious(tok, lemma):\n",
    "                lem_susp += 1\n",
    "\n",
    "        prev_tag = tag\n",
    "        prev_word = analyze_form\n",
    "        seen_content = True\n",
    "\n",
    "    # ---- Display table (possibly morpheme-expanded) ----\n",
    "    maxw = max(12, max(len(r[0]) for r in display_rows) if display_rows else 12)\n",
    "    for t, tag, lem in display_rows:\n",
    "        print(f\"{t:>{maxw}}  {tag:<6}  lemma={lem}\")\n",
    "\n",
    "    # ---- NER pretrained + deterministic fixes ----\n",
    "    try:\n",
    "        ner_labels = ner_predict_tokens(toks)\n",
    "        ner_labels = fix_titles(toks, ner_labels)\n",
    "        ner_labels = add_year_dates(toks, ner_labels)\n",
    "        ner_labels = enforce_bio(ner_labels)\n",
    "\n",
    "        print(\"\\nNER (pretrained + deterministic fixes) (token, label):\")\n",
    "        print(list(zip(toks, ner_labels)))\n",
    "\n",
    "        ents = bio_to_entities(toks, ner_labels, morph_rows_for_ner)\n",
    "        print(\"Entities:\")\n",
    "        for typ, txt in ents:\n",
    "            print(f\"  {typ}: {txt}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nNER skipped (reason):\", str(e))\n",
    "\n",
    "    # ---- Audits (heuristiques) ----\n",
    "    total_seg = max(1, seg_ok + seg_err)\n",
    "    print(\"\\nAudit (heuristique):\")\n",
    "    print(f\"  Segmentation: OK={seg_ok} | Err={seg_err} | Err%={(seg_err/total_seg*100):.2f} %\")\n",
    "    if lem_total:\n",
    "        print(f\"  Lemma suspect: {lem_susp}/{lem_total} = {(lem_susp/lem_total*100):.2f} %\")\n",
    "    else:\n",
    "        print(\"  Lemma suspect: n/a\")\n",
    "\n",
    "def split_input_into_sentences(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    dq = re.findall(r\"\\\"([^\\\"]+)\\\"\", s)\n",
    "    gu = re.findall(r\"«([^»]+)»\", s)\n",
    "    parts = [p.strip() for p in (dq + gu) if p.strip()]\n",
    "    if parts:\n",
    "        return parts\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\؟\\?])\\s+\", s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "print(\"Type an Arabic sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\")\n",
    "while True:\n",
    "    s = input(\"\\nArabic> \").strip()\n",
    "    if not s:\n",
    "        break\n",
    "    for sent in split_input_into_sentences(s):\n",
    "        analyze_sentence(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f967254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Running in venv: False\n",
      "Local deps dir: (absent)\n",
      "\n",
      "Type a French sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: On veut moins de retards et plus de sécurité\n",
      "        On  PRP     lemma=on\n",
      "      veut  NN      lemma=vouloir\n",
      "     moins  NN      lemma=moins\n",
      "        de  DT      lemma=de\n",
      "   retards  NN      lemma=retard\n",
      "        et  CC      lemma=et\n",
      "      plus  RP      lemma=plus\n",
      "        de  DT      lemma=de\n",
      "  sécurité  NN      lemma=sécurité\n",
      "\n",
      "NER (règles (fallback) : titres/capitalisation/prépositions) (token, label):\n",
      "[('On', 'O'), ('veut', 'O'), ('moins', 'O'), ('de', 'O'), ('retards', 'O'), ('et', 'O'), ('plus', 'O'), ('de', 'O'), ('sécurité', 'O')]\n",
      "Entities:\n",
      "\n",
      "Audit (heuristique, pas vérité terrain):\n",
      "1) POS+Lemma\n",
      "   Réussite : 77.78 % (7/9)\n",
      "   Erreur   : 22.22 % (2/9)\n",
      "   Lemma    : simplemma actif\n",
      "2) NER (structure)\n",
      "   Réussite : 0.00 % (0/1)\n",
      "   Erreur   : 0.00 % (0/1)\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys, unicodedata\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Option \"deps local\" (sans venv)\n",
    "# ----------------------------\n",
    "LOCAL_DEPS_DIR = os.path.join(os.getcwd(), \".pylibs\")\n",
    "if os.path.isdir(LOCAL_DEPS_DIR) and LOCAL_DEPS_DIR not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_DEPS_DIR)\n",
    "\n",
    "def _in_venv() -> bool:\n",
    "    return getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix or hasattr(sys, \"real_prefix\")\n",
    "\n",
    "print(\"Python kernel:\", sys.executable)\n",
    "print(\"Running in venv:\", _in_venv())\n",
    "print(\"Local deps dir:\", LOCAL_DEPS_DIR if os.path.isdir(LOCAL_DEPS_DIR) else \"(absent)\")\n",
    "\n",
    "def print_install_help():\n",
    "    py = sys.executable\n",
    "    print(\"\\n[install help] Installer sans venv, local dans .pylibs (utilise CE python):\")\n",
    "    print(f'  \"{py}\" -m pip install --upgrade --target \"{LOCAL_DEPS_DIR}\" simplemma')\n",
    "    print(f'  \"{py}\" -m pip install --upgrade --target \"{LOCAL_DEPS_DIR}\" torch transformers tokenizers')\n",
    "    print(\"\\n[si pip refuse --target à cause de --user auto]\")\n",
    "    print(\"  PowerShell:\")\n",
    "    print('    $env:PIP_CONFIG_FILE=\"NUL\"')\n",
    "    print(\"    Remove-Item Env:PIP_USER -ErrorAction SilentlyContinue\")\n",
    "    print('    py -3.11 -m pip install --upgrade --target .\\\\.pylibs simplemma')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Lemmatisation (optionnelle)\n",
    "# ----------------------------\n",
    "SIMPLEMMA_OK = False\n",
    "try:\n",
    "    import simplemma\n",
    "    SIMPLEMMA_OK = True\n",
    "except Exception:\n",
    "    SIMPLEMMA_OK = False\n",
    "\n",
    "# ----------------------------\n",
    "# 2) NER IA (optionnelle, SANS sentencepiece)\n",
    "#    IMPORTANT: choisis un modèle WordPiece/BERT (mBERT) => pas sentencepiece.\n",
    "# ----------------------------\n",
    "HF_OK = False\n",
    "_HF_IMPORT_ERR = None\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "    HF_OK = True\n",
    "except Exception as e:\n",
    "    HF_OK = False\n",
    "    _HF_IMPORT_ERR = e\n",
    "\n",
    "# Modèles sûrs (WordPiece, pas sentencepiece)\n",
    "# - Davlan/bert-base-multilingual-cased-ner-hrl : PER/ORG/LOC (solide en FR)\n",
    "# - Babelscape/wikineural-multilingual-ner      : PER/ORG/LOC/MISC (souvent bon aussi)\n",
    "HF_MODEL_NAME = \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Tokenisation robuste + élisions\n",
    "# ----------------------------\n",
    "#  - Ne casse PAS les tirets : ex-collaborateurs, va-nu-pieds, garde-fous, outre-Atlantique, France-KLM\n",
    "#  - Capture les élisions en 1 token puis on les \"re-split\" intelligemment : d'Air -> d' + Air\n",
    "#  - Punctuation isolée\n",
    "BASE_WORD = r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\"\n",
    "ACRONYM   = r\"[A-Z]{2,8}\"\n",
    "\n",
    "WORD = rf\"{BASE_WORD}(?:(?:[’']{BASE_WORD})|(?:-(?:{BASE_WORD}|{ACRONYM})))*\"\n",
    "\n",
    "TOKEN_RE = re.compile(\n",
    "    rf\"\"\"\n",
    "    (?:\\d{{1,4}}[/-]\\d{{1,2}}[/-]\\d{{1,4}}) |      # dates 12/02/2026\n",
    "    (?:\\d+(?:[.,]\\d+)?) |                           # nombres\n",
    "    (?:{WORD}) |                                    # mots (apostrophes/tirets inclus)\n",
    "    (?:[“”\"«»()\\[\\]{{}}…,:;.!?¿¡]) |                # ponctuation\n",
    "    (?:[–—-]) |                                     # tirets typographiques séparés\n",
    "    (?:\\S)                                          # fallback 1 char non-espace\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class Tok:\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "def tokenize_raw(text: str) -> List[Tok]:\n",
    "    return [Tok(m.group(0), m.start(), m.end()) for m in TOKEN_RE.finditer(text or \"\")]\n",
    "\n",
    "# Élisions à splitter (on évite de splitter des mots lexicalisés type \"aujourd'hui\")\n",
    "ELISION_PREFIXES = {\n",
    "    \"d\", \"l\", \"j\", \"t\", \"m\", \"s\", \"n\", \"c\", \"qu\",\n",
    "    \"jusqu\", \"lorsqu\", \"puisqu\"\n",
    "}\n",
    "ELISION_EXCEPTIONS = {\n",
    "    \"aujourd'hui\", \"aujourd’hui\",\n",
    "    \"quelqu'un\", \"quelqu’un\",\n",
    "    \"presqu'île\", \"presqu’île\",\n",
    "}\n",
    "\n",
    "def _norm_apo(s: str) -> str:\n",
    "    return (s or \"\").replace(\"’\", \"'\")\n",
    "\n",
    "def split_elisions(text: str, toks: List[Tok]) -> List[Tok]:\n",
    "    out: List[Tok] = []\n",
    "    for tk in toks:\n",
    "        t = tk.text\n",
    "        tl = _norm_apo(t).lower()\n",
    "\n",
    "        if tl in ELISION_EXCEPTIONS:\n",
    "            out.append(tk)\n",
    "            continue\n",
    "\n",
    "        # split si pattern prefix'WORD (ex: d'Air, l'aéroport, n'est, qu'il)\n",
    "        # NOTE: on garde le token \"d'\" (avec apostrophe) comme 1 token.\n",
    "        m = re.match(r\"^([A-Za-zÀ-ÖØ-öø-ÿ]+)(['’])([A-Za-zÀ-ÖØ-öø-ÿ].+)$\", t)\n",
    "        if not m:\n",
    "            out.append(tk)\n",
    "            continue\n",
    "\n",
    "        pref = _norm_apo(m.group(1)).lower()\n",
    "        apo  = m.group(2)\n",
    "        rest = m.group(3)\n",
    "\n",
    "        if pref not in ELISION_PREFIXES:\n",
    "            out.append(tk)\n",
    "            continue\n",
    "\n",
    "        # spans: prefix+apostrophe puis reste\n",
    "        # exemple: \"d'Air\" => \"d'\" et \"Air\"\n",
    "        cut = len(m.group(1)) + 1  # + apostrophe char\n",
    "        out.append(Tok(t[:cut], tk.start, tk.start + cut))\n",
    "        out.append(Tok(rest, tk.start + cut, tk.end))\n",
    "    return out\n",
    "\n",
    "def tokenize_with_spans(text: str) -> List[Tok]:\n",
    "    return split_elisions(text, tokenize_raw(text))\n",
    "\n",
    "def _lower(s: str) -> str:\n",
    "    return (s or \"\").lower()\n",
    "\n",
    "def _is_punct(t: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[“”\\\"«»()\\[\\]{}…,:;.!?¿¡]\", t or \"\"))\n",
    "\n",
    "def _is_number(t: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d+(?:[.,]\\d+)?\", t or \"\")) or bool(re.fullmatch(r\"\\d{1,4}[/-]\\d{1,2}[/-]\\d{1,4}\", t or \"\"))\n",
    "\n",
    "def _is_dash(t: str) -> bool:\n",
    "    return t in (\"-\", \"–\", \"—\")\n",
    "\n",
    "def _is_acronym(t: str) -> bool:\n",
    "    return bool(re.fullmatch(ACRONYM, t or \"\"))\n",
    "\n",
    "def _is_capitalized_word(t: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[A-ZÀ-ÖØ-Þ][A-Za-zÀ-ÖØ-öø-ÿ]+\", t or \"\"))\n",
    "\n",
    "def _has_letters(t: str) -> bool:\n",
    "    return bool(re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", t or \"\"))\n",
    "\n",
    "def _is_hyphenated_word(t: str) -> bool:\n",
    "    return \"-\" in (t or \"\") and bool(re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]-[A-Za-zÀ-ÖØ-öø-ÿ]\", t))\n",
    "\n",
    "# ----------------------------\n",
    "# 4) POS heuristique (NN/NNP/VB/JJ/IN/DT/PRP/CC/RB/CD/RP/PUNCT)\n",
    "# ----------------------------\n",
    "DET = {\n",
    "    \"le\",\"la\",\"les\",\"un\",\"une\",\"des\",\"du\",\"de\",\"au\",\"aux\",\"ce\",\"cet\",\"cette\",\"ces\",\n",
    "    \"mon\",\"ma\",\"mes\",\"ton\",\"ta\",\"tes\",\"son\",\"sa\",\"ses\",\"notre\",\"nos\",\"votre\",\"vos\",\"leur\",\"leurs\"\n",
    "}\n",
    "PREP = {\n",
    "    \"à\",\"a\",\"de\",\"dans\",\"en\",\"sur\",\"sous\",\"chez\",\"vers\",\"avec\",\"sans\",\"pour\",\"par\",\"entre\",\"contre\",\n",
    "    \"selon\",\"depuis\",\"pendant\",\"outre\"  # FIX: outre = souvent préposition\n",
    "}\n",
    "CONJ = {\"et\",\"ou\",\"mais\",\"donc\",\"or\",\"ni\",\"car\",\"que\"}\n",
    "PRON = {\n",
    "    \"je\",\"tu\",\"il\",\"elle\",\"on\",\"nous\",\"vous\",\"ils\",\"elles\",\n",
    "    \"me\",\"te\",\"se\",\"lui\",\"leur\",\"y\",\"en\",\"ce\",\"ça\",\"cela\",\"qui\",\"quoi\",\"dont\",\"où\"\n",
    "}\n",
    "PART = {\"ne\",\"pas\",\"plus\",\"jamais\",\"rien\",\"aucun\",\"aucune\",\"non\"}\n",
    "ADV = {\"très\",\"trop\",\"bien\",\"mal\",\"ici\",\"là\",\"hier\",\"aujourd'hui\",\"aujourd’hui\",\"demain\",\"souvent\",\"parfois\",\"déjà\",\"encore\"}\n",
    "\n",
    "# clitiques split (d', l', j', n', qu', ...)\n",
    "CLITIC_POS_LEMMA = {\n",
    "    \"d'\": (\"IN\",  \"de\"),\n",
    "    \"l'\": (\"DT\",  \"le\"),     # on ajuste plus bas si besoin (PRP)\n",
    "    \"j'\": (\"PRP\", \"je\"),\n",
    "    \"t'\": (\"PRP\", \"te\"),\n",
    "    \"m'\": (\"PRP\", \"me\"),\n",
    "    \"s'\": (\"PRP\", \"se\"),\n",
    "    \"n'\": (\"RP\",  \"ne\"),\n",
    "    \"c'\": (\"PRP\", \"ce\"),\n",
    "    \"qu'\":(\"CC\",  \"que\"),\n",
    "    \"jusqu'\": (\"IN\", \"jusque\"),\n",
    "    \"lorsqu'\": (\"CC\", \"lorsque\"),\n",
    "    \"puisqu'\": (\"CC\", \"puisque\"),\n",
    "}\n",
    "\n",
    "AUX_FORMS = {\n",
    "    # avoir\n",
    "    \"ai\",\"as\",\"a\",\"avons\",\"avez\",\"ont\",\"avais\",\"avait\",\"avions\",\"aviez\",\"avaient\",\n",
    "    \"aurai\",\"auras\",\"aura\",\"aurons\",\"aurez\",\"auront\",\n",
    "    \"eusse\",\"eusses\",\"eût\",\"eussions\",\"eussiez\",\"eussent\",\n",
    "    # être\n",
    "    \"suis\",\"es\",\"est\",\"sommes\",\"êtes\",\"etes\",\"sont\",\"étais\",\"etais\",\"était\",\"etait\",\"étions\",\"etions\",\"étiez\",\"etiez\",\"étaient\",\"etaient\",\n",
    "    \"serai\",\"seras\",\"sera\",\"serons\",\"serez\",\"seront\",\n",
    "    \"fusse\",\"fusses\",\"fût\",\"fussions\",\"fussiez\",\"fussent\",\n",
    "}\n",
    "\n",
    "def guess_pos(token: str, prev_tok: Optional[str], next_tok: Optional[str], prev_pos: Optional[str]) -> str:\n",
    "    t = token or \"\"\n",
    "    tn = _norm_apo(t)\n",
    "    tl = tn.lower()\n",
    "\n",
    "    if _is_punct(t):\n",
    "        return \"PUNCT\"\n",
    "    if _is_dash(t):\n",
    "        return \"PUNCT\"\n",
    "    if _is_number(t):\n",
    "        return \"CD\"\n",
    "\n",
    "    # clitiques (d', l', n', qu'...)\n",
    "    if tl in CLITIC_POS_LEMMA:\n",
    "        # \"l'\" : si suit un verbe/aux => PRP sinon DT\n",
    "        if tl == \"l'\" and next_tok:\n",
    "            n2 = _norm_apo(next_tok).lower()\n",
    "            if n2 in AUX_FORMS:\n",
    "                return \"PRP\"\n",
    "        return CLITIC_POS_LEMMA[tl][0]\n",
    "\n",
    "    if tl in DET:\n",
    "        return \"DT\"\n",
    "    if tl in PREP:\n",
    "        return \"IN\"\n",
    "    if tl in CONJ:\n",
    "        return \"CC\"\n",
    "    if tl in PRON:\n",
    "        return \"PRP\"\n",
    "    if tl in PART:\n",
    "        return \"RP\"\n",
    "    if tl in ADV or tl.endswith(\"ment\"):\n",
    "        return \"RB\"\n",
    "\n",
    "    if _is_acronym(t):\n",
    "        return \"NNP\"\n",
    "\n",
    "    # mots avec tirets : si une partie est capitalisée => NNP probable\n",
    "    if _is_hyphenated_word(t):\n",
    "        parts = [p for p in t.split(\"-\") if p]\n",
    "        if any(_is_acronym(p) or _is_capitalized_word(p) for p in parts):\n",
    "            return \"NNP\"\n",
    "        # sinon souvent NN/JJ ; on garde NN (mais ce n'est plus une \"fragmentation\")\n",
    "        return \"NN\"\n",
    "\n",
    "    # capitalisation : éviter de sur-tag NNP au début si c'est juste \"Le/La/Un\" (déjà géré)\n",
    "    if _is_capitalized_word(t):\n",
    "        return \"NNP\"\n",
    "\n",
    "    # VB : participes passés après auxiliaire (FIX: cru, venu, parti, etc.)\n",
    "    if prev_tok:\n",
    "        p2 = _norm_apo(prev_tok).lower()\n",
    "        if p2 in AUX_FORMS:\n",
    "            if re.search(r\"(é|ée|ées|és|i|ie|ies|is|it|u|ue|ues|us)$\", tl):\n",
    "                return \"VB\"\n",
    "\n",
    "    # VB : terminaisons fréquentes (approximatif)\n",
    "    if re.search(r\"(er|ir|re|oir)$\", tl):\n",
    "        return \"VB\"\n",
    "    if re.search(r\"(ais|ait|aient|ons|ez|ent|era|eront|irai|iras|ira|iront|ront)$\", tl):\n",
    "        return \"VB\"\n",
    "\n",
    "    # JJ : suffixes fréquents\n",
    "    if re.search(r\"(able|ible|eux|euse|al|elle|ien|ienne|ique|if|ive|aux|aire)$\", tl):\n",
    "        return \"JJ\"\n",
    "\n",
    "    # fallback\n",
    "    return \"NN\"\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Lemma (simplemma si dispo, sinon règles)\n",
    "# ----------------------------\n",
    "def lemma_fr(token: str, pos: str) -> str:\n",
    "    t = token or \"\"\n",
    "    tn = _norm_apo(t)\n",
    "    tl = tn.lower()\n",
    "\n",
    "    if _is_punct(t) or _is_dash(t):\n",
    "        return \"∅\"\n",
    "    if _is_number(t):\n",
    "        return t\n",
    "\n",
    "    if tl in CLITIC_POS_LEMMA:\n",
    "        return CLITIC_POS_LEMMA[tl][1]\n",
    "\n",
    "    if SIMPLEMMA_OK:\n",
    "        try:\n",
    "            # simplemma attend souvent minuscules, mais on lui donne le token tel quel\n",
    "            return simplemma.lemmatize(tn, lang=\"fr\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # fallback règles\n",
    "    if pos in (\"NN\", \"NNP\", \"JJ\"):\n",
    "        if tl.endswith(\"aux\") and len(tl) > 4:\n",
    "            return tl[:-3] + \"al\"\n",
    "        if tl.endswith((\"s\",\"x\")) and len(tl) > 3:\n",
    "            return tl[:-1]\n",
    "        return tl\n",
    "\n",
    "    if pos == \"VB\":\n",
    "        # très grossier\n",
    "        if re.search(r\"(ées|és|ée|é)$\", tl):\n",
    "            base = re.sub(r\"(ées|és|ée|é)$\", \"\", tl)\n",
    "            return (base + \"er\") if base else tl\n",
    "        return tl\n",
    "\n",
    "    return tl\n",
    "\n",
    "# ----------------------------\n",
    "# 6) DATE (règles)\n",
    "# ----------------------------\n",
    "MOIS = {\n",
    "    \"janvier\",\"février\",\"fevrier\",\"mars\",\"avril\",\"mai\",\"juin\",\"juillet\",\"août\",\"aout\",\n",
    "    \"septembre\",\"octobre\",\"novembre\",\"décembre\",\"decembre\"\n",
    "}\n",
    "JOURS = {\"lundi\",\"mardi\",\"mercredi\",\"jeudi\",\"vendredi\",\"samedi\",\"dimanche\"}\n",
    "\n",
    "def date_spans(text: str, toks: List[Tok]) -> List[Tuple[int,int,str]]:\n",
    "    spans = []\n",
    "    for m in re.finditer(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "    for m in re.finditer(r\"\\b(19\\d{2}|20\\d{2})\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        w = _norm_apo(toks[i].text).lower()\n",
    "\n",
    "        if w in (\"en\",\"depuis\") and i+1 < len(toks) and re.fullmatch(r\"(19\\d{2}|20\\d{2})\", toks[i+1].text):\n",
    "            spans.append((toks[i].start, toks[i+1].end, \"DATE\"))\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        if w in JOURS:\n",
    "            j = i+1\n",
    "            if j < len(toks) and re.fullmatch(r\"\\d{1,2}\", toks[j].text):\n",
    "                j += 1\n",
    "            if j < len(toks) and _norm_apo(toks[j].text).lower() in MOIS:\n",
    "                j += 1\n",
    "            if j < len(toks) and re.fullmatch(r\"(19\\d{2}|20\\d{2})\", toks[j].text):\n",
    "                j += 1\n",
    "            if j > i+1:\n",
    "                spans.append((toks[i].start, toks[j-1].end, \"DATE\"))\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        if re.fullmatch(r\"\\d{1,2}\", toks[i].text) and i+1 < len(toks) and _norm_apo(toks[i+1].text).lower() in MOIS:\n",
    "            j = i+2\n",
    "            if j < len(toks) and re.fullmatch(r\"(19\\d{2}|20\\d{2})\", toks[j].text):\n",
    "                j += 1\n",
    "            spans.append((toks[i].start, toks[j-1].end, \"DATE\"))\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    out = []\n",
    "    last_end = -1\n",
    "    for a,b,lab in spans:\n",
    "        if a >= last_end:\n",
    "            out.append((a,b,lab))\n",
    "            last_end = b\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 7) NER IA (PER/ORG/LOC) + fallback règles\n",
    "# ----------------------------\n",
    "def load_hf_ner(model_name: str):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.set_num_threads(1)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # IMPORTANT: si un modèle exige sentencepiece, on refuse explicitement\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    except Exception as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"sentencepiece\" in msg:\n",
    "            raise RuntimeError(\"Modèle/tokenizer exige sentencepiece. Choisis un modèle BERT WordPiece (ex: Davlan/bert-base-multilingual-cased-ner-hrl).\")\n",
    "        raise\n",
    "\n",
    "    mdl = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    mdl.eval()\n",
    "\n",
    "    # grouped_entities => spans start/end\n",
    "    ner = pipeline(\"ner\", model=mdl, tokenizer=tok, device=-1, grouped_entities=True)\n",
    "    return ner\n",
    "\n",
    "def hf_spans(ner_pipe, text: str) -> List[Tuple[int,int,str]]:\n",
    "    spans = []\n",
    "    res = ner_pipe(text)\n",
    "    for it in res:\n",
    "        lab = it.get(\"entity_group\") or it.get(\"entity\") or \"\"\n",
    "        lab = lab.replace(\"I-\", \"\").replace(\"B-\", \"\").strip().upper()\n",
    "        if lab in (\"PER\", \"PERSON\"):\n",
    "            lab = \"PERS\"\n",
    "        if lab in (\"ORG\", \"LOC\", \"PERS\"):\n",
    "            a = int(it.get(\"start\", -1))\n",
    "            b = int(it.get(\"end\", -1))\n",
    "            if 0 <= a < b:\n",
    "                spans.append((a,b,lab))\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    out = []\n",
    "    last_end = -1\n",
    "    for a,b,lab in spans:\n",
    "        if a >= last_end:\n",
    "            out.append((a,b,lab))\n",
    "            last_end = b\n",
    "    return out\n",
    "\n",
    "def apply_spans_to_tokens(toks: List[Tok], spans: List[Tuple[int,int,str]], labels: List[str]):\n",
    "    for a,b,lab in spans:\n",
    "        idxs = [i for i,t in enumerate(toks) if not (t.end <= a or t.start >= b)]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        if any(labels[i] != \"O\" for i in idxs):\n",
    "            continue\n",
    "        labels[idxs[0]] = f\"B-{lab}\"\n",
    "        for i in idxs[1:]:\n",
    "            labels[i] = f\"I-{lab}\"\n",
    "\n",
    "# Fallback NER règles (déterministe)\n",
    "ORG_HINT = {\"université\",\"ministère\",\"ministère\",\"groupe\",\"société\",\"compagnie\",\"banque\",\"association\",\"sarl\",\"sas\",\"sa\",\"inc\",\"ltd\"}\n",
    "TITLE_HINT = {\"m.\",\"mme\",\"mlle\",\"monsieur\",\"madame\",\"dr\",\"docteur\",\"prof\",\"pr\"}\n",
    "\n",
    "def ner_rules_spans(text: str, toks: List[Tok]) -> List[Tuple[int,int,str]]:\n",
    "    spans: List[Tuple[int,int,str]] = []\n",
    "\n",
    "    # règle: \"outre-Atlantique\" => LOC (structure)\n",
    "    for tk in toks:\n",
    "        if _is_hyphenated_word(tk.text):\n",
    "            parts = tk.text.split(\"-\")\n",
    "            if len(parts) == 2 and _norm_apo(parts[0]).lower() == \"outre\" and _is_capitalized_word(parts[1]):\n",
    "                spans.append((tk.start, tk.end, \"LOC\"))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        t = toks[i].text\n",
    "        tl = _norm_apo(t).lower()\n",
    "\n",
    "        # personnes: titre + NNP+\n",
    "        if tl in TITLE_HINT and i+1 < len(toks) and (_is_capitalized_word(toks[i+1].text) or _is_acronym(toks[i+1].text)):\n",
    "            a = toks[i].start\n",
    "            j = i+1\n",
    "            while j < len(toks) and (_is_capitalized_word(toks[j].text) or _is_acronym(toks[j].text) or _is_hyphenated_word(toks[j].text)):\n",
    "                j += 1\n",
    "            spans.append((a, toks[j-1].end, \"PERS\"))\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # org: séquence capitalisée contenant un hint (ou acronymes)\n",
    "        if _is_capitalized_word(t) or _is_acronym(t) or _is_hyphenated_word(t):\n",
    "            a = toks[i].start\n",
    "            j = i+1\n",
    "            words = [_norm_apo(t)]\n",
    "            while j < len(toks) and (_is_capitalized_word(toks[j].text) or _is_acronym(toks[j].text) or _is_hyphenated_word(toks[j].text)):\n",
    "                words.append(_norm_apo(toks[j].text))\n",
    "                j += 1\n",
    "            low = \" \".join(words).lower()\n",
    "            if any(h in low for h in ORG_HINT):\n",
    "                spans.append((a, toks[j-1].end, \"ORG\"))\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        # loc: préposition + NNP(+)\n",
    "        if tl in PREP and i+1 < len(toks) and (_is_capitalized_word(toks[i+1].text) or _is_hyphenated_word(toks[i+1].text) or _is_acronym(toks[i+1].text)):\n",
    "            a = toks[i+1].start\n",
    "            j = i+1\n",
    "            while j < len(toks) and (_is_capitalized_word(toks[j].text) or _is_acronym(toks[j].text) or _is_hyphenated_word(toks[j].text)):\n",
    "                j += 1\n",
    "            spans.append((a, toks[j-1].end, \"LOC\"))\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    out = []\n",
    "    last_end = -1\n",
    "    for a,b,lab in spans:\n",
    "        if a >= last_end:\n",
    "            out.append((a,b,lab))\n",
    "            last_end = b\n",
    "    return out\n",
    "\n",
    "def expand_person_titles(toks: List[Tok], labels: List[str]):\n",
    "    for i in range(1, len(toks)):\n",
    "        if labels[i] == \"B-PERS\":\n",
    "            prev = _norm_apo(toks[i-1].text).lower().strip(\".\")\n",
    "            if prev in TITLE_HINT and labels[i-1] == \"O\":\n",
    "                labels[i-1] = \"B-PERS\"\n",
    "                labels[i]   = \"I-PERS\"\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Entities à partir de BIO (avec join FR correct)\n",
    "# ----------------------------\n",
    "NO_SPACE_BEFORE = {\".\", \",\", \";\", \":\", \"!\", \"?\", \"…\", \")\", \"]\", \"}\", \"»\"}\n",
    "NO_SPACE_AFTER  = {\"(\", \"[\", \"{\", \"«\"}\n",
    "\n",
    "def join_fr(tokens: List[str]) -> str:\n",
    "    out = \"\"\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if not out:\n",
    "            out = tok\n",
    "            continue\n",
    "        prev = out[-1] if out else \"\"\n",
    "        if tok in NO_SPACE_BEFORE:\n",
    "            out += tok\n",
    "        elif prev in NO_SPACE_AFTER:\n",
    "            out += tok\n",
    "        elif out.endswith(\"'\") or out.endswith(\"’\"):\n",
    "            out += tok\n",
    "        else:\n",
    "            out += \" \" + tok\n",
    "    return out\n",
    "\n",
    "def entities_from_bio(toks: List[Tok], labels: List[str]) -> Dict[str, List[str]]:\n",
    "    ents: Dict[str, List[str]] = {}\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        lab = labels[i]\n",
    "        if lab == \"O\":\n",
    "            i += 1\n",
    "            continue\n",
    "        m = re.fullmatch(r\"[BI]-(.+)\", lab)\n",
    "        if not m:\n",
    "            i += 1\n",
    "            continue\n",
    "        typ = m.group(1)\n",
    "        j = i + 1\n",
    "        parts = [toks[i].text]\n",
    "        while j < len(toks) and labels[j] == f\"I-{typ}\":\n",
    "            parts.append(toks[j].text)\n",
    "            j += 1\n",
    "        val = join_fr(parts)\n",
    "        ents.setdefault(typ, []).append(val)\n",
    "        i = j\n",
    "    return ents\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Audit \"Réussite/Erreur\" (heuristique, pas vérité terrain)\n",
    "# ----------------------------\n",
    "def audit_pos_lemma(toks: List[Tok], pos: List[str]) -> Tuple[int,int]:\n",
    "    \"\"\"\n",
    "    On marque comme \"erreur\" les tokens alphabetiques taggés NN par pur fallback\n",
    "    (pas précédés d'un DT/IN, pas hyphenés, pas NNP, etc.)\n",
    "    => ça reflète ton problème réel: beaucoup de NN \"par défaut\".\n",
    "    \"\"\"\n",
    "    err = 0\n",
    "    total = 0\n",
    "    for i, tk in enumerate(toks):\n",
    "        if not _has_letters(tk.text):\n",
    "            continue\n",
    "        total += 1\n",
    "        p = pos[i]\n",
    "        prev_p = pos[i-1] if i > 0 else None\n",
    "\n",
    "        # NN suspect si pas de contexte qui l'explique\n",
    "        if p == \"NN\":\n",
    "            if _is_hyphenated_word(tk.text):\n",
    "                continue\n",
    "            if prev_p in (\"DT\",\"IN\"):\n",
    "                continue\n",
    "            # sinon on considère \"fallback\"\n",
    "            err += 1\n",
    "    ok = total - err\n",
    "    return ok, err\n",
    "\n",
    "def audit_ner_structure(toks: List[Tok], labels: List[str]) -> Tuple[int,int]:\n",
    "    \"\"\"\n",
    "    Erreurs structurelles typiques :\n",
    "      - un token avec tiret (outre-Atlantique, France-KLM, Charles-de-Gaulle) coupé en 2 entités\n",
    "    Ici, comme on tokenise sans casser les tirets, ça devrait tendre vers 0.\n",
    "    \"\"\"\n",
    "    # count = nb d'entités \"suspectes\"\n",
    "    suspects = 0\n",
    "    ents = entities_from_bio(toks, labels)\n",
    "    # Heuristique: si une entité contient \"outre - Atlantique\" (avec espaces autour du tiret), suspect\n",
    "    for typ, vals in ents.items():\n",
    "        for v in vals:\n",
    "            if \" - \" in v:\n",
    "                suspects += 1\n",
    "    total_ents = sum(len(v) for v in ents.values())\n",
    "    ok = max(0, total_ents - suspects)\n",
    "    return ok, suspects\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Affichage \"style arabe\"\n",
    "# ----------------------------\n",
    "def print_table(toks: List[Tok], pos: List[str], lem: List[str], max_rows: Optional[int] = None):\n",
    "    if not toks:\n",
    "        return\n",
    "    w_token = max(10, min(28, max(len(t.text) for t in toks)))\n",
    "    view = toks[:max_rows] if max_rows else toks\n",
    "    for i, t in enumerate(view):\n",
    "        print(f\"{t.text:>{w_token}}  {pos[i]:<7} lemma={lem[i]}\")\n",
    "\n",
    "def run_one(text: str, ner_pipe=None):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"INPUT:\", text)\n",
    "\n",
    "    toks = tokenize_with_spans(text)\n",
    "\n",
    "    # POS + lemma\n",
    "    pos: List[str] = []\n",
    "    lem: List[str] = []\n",
    "    for i, tk in enumerate(toks):\n",
    "        prev = toks[i-1].text if i > 0 else None\n",
    "        nxt  = toks[i+1].text if i+1 < len(toks) else None\n",
    "        prev_pos = pos[i-1] if i > 0 else None\n",
    "\n",
    "        p = guess_pos(tk.text, prev, nxt, prev_pos)\n",
    "        pos.append(p)\n",
    "        lem.append(lemma_fr(tk.text, p))\n",
    "\n",
    "    # NER labels\n",
    "    labels = [\"O\"] * len(toks)\n",
    "    ner_mode = \"règles (fallback)\"\n",
    "\n",
    "    # 1) IA spans (PER/ORG/LOC)\n",
    "    if ner_pipe is not None:\n",
    "        try:\n",
    "            spans = hf_spans(ner_pipe, text)\n",
    "            apply_spans_to_tokens(toks, spans, labels)\n",
    "            expand_person_titles(toks, labels)\n",
    "            ner_mode = f\"IA (transformers) : {HF_MODEL_NAME}\"\n",
    "        except Exception as e:\n",
    "            ner_mode = f\"règles (fallback) - NER IA indisponible: {e}\"\n",
    "\n",
    "    # 2) fallback règles PER/ORG/LOC si rien (ou en complément sans écraser)\n",
    "    if all(l == \"O\" for l in labels):\n",
    "        r_sp = ner_rules_spans(text, toks)\n",
    "        apply_spans_to_tokens(toks, r_sp, labels)\n",
    "        ner_mode = \"règles (fallback) : titres/capitalisation/prépositions\"\n",
    "\n",
    "    # 3) DATE rules (ajout sans écraser)\n",
    "    d_sp = date_spans(text, toks)\n",
    "    apply_spans_to_tokens(toks, d_sp, labels)\n",
    "\n",
    "    # Print POS/lemma\n",
    "    print_table(toks, pos, lem, max_rows=None)\n",
    "\n",
    "    # Print NER\n",
    "    print()\n",
    "    print(f\"NER ({ner_mode}) (token, label):\")\n",
    "    pairs = [(toks[i].text, labels[i]) for i in range(len(toks))]\n",
    "    print(pairs)\n",
    "\n",
    "    ents = entities_from_bio(toks, labels)\n",
    "    print(\"Entities:\")\n",
    "    for k in sorted(ents.keys()):\n",
    "        for v in ents[k]:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Audit \"Réussite/Erreur\"\n",
    "    ok1, err1 = audit_pos_lemma(toks, pos)\n",
    "    tot1 = ok1 + err1 if (ok1 + err1) else 1\n",
    "    ok2, err2 = audit_ner_structure(toks, labels)\n",
    "    tot2 = ok2 + err2 if (ok2 + err2) else 1\n",
    "\n",
    "    print(\"\\nAudit (heuristique, pas vérité terrain):\")\n",
    "    print(f\"1) POS+Lemma\")\n",
    "    print(f\"   Réussite : {ok1/tot1*100:.2f} % ({ok1}/{tot1})\")\n",
    "    print(f\"   Erreur   : {err1/tot1*100:.2f} % ({err1}/{tot1})\")\n",
    "    if SIMPLEMMA_OK:\n",
    "        print(\"   Lemma    : simplemma actif\")\n",
    "    else:\n",
    "        print(\"   Lemma    : règles (simplemma absent)\")\n",
    "\n",
    "    print(f\"2) NER (structure)\")\n",
    "    print(f\"   Réussite : {ok2/tot2*100:.2f} % ({ok2}/{tot2})\")\n",
    "    print(f\"   Erreur   : {err2/tot2*100:.2f} % ({err2}/{tot2})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Split multi-sentences (comme ton script arabe)\n",
    "# ----------------------------\n",
    "def split_input_into_sentences(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    dq = re.findall(r\"\\\"([^\\\"]+)\\\"\", s)\n",
    "    gu = re.findall(r\"«([^»]+)»\", s)\n",
    "    parts = [p.strip() for p in (dq + gu) if p.strip()]\n",
    "    if parts:\n",
    "        return parts\n",
    "    # split simple sur ponctuation finale\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+\", s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Main interactive\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ner_pipe = None\n",
    "    if HF_OK:\n",
    "        try:\n",
    "            ner_pipe = load_hf_ner(HF_MODEL_NAME)\n",
    "        except Exception as e:\n",
    "            ner_pipe = None\n",
    "            print(\"[warn] NER IA non chargé -> fallback règles.\")\n",
    "            print(\"       Cause:\", e)\n",
    "    else:\n",
    "        print(\"[info] transformers/torch non dispo -> NER règles.\")\n",
    "        if _HF_IMPORT_ERR is not None:\n",
    "            print(\"       Cause import:\", _HF_IMPORT_ERR)\n",
    "\n",
    "    if not SIMPLEMMA_OK:\n",
    "        print(\"[info] simplemma non dispo -> lemma fallback règles.\")\n",
    "        print_install_help()\n",
    "\n",
    "    print(\"\\nType a French sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\")\n",
    "    while True:\n",
    "        s = input(\"\\nFrench> \").strip()\n",
    "        if not s:\n",
    "            break\n",
    "        for sent in split_input_into_sentences(s):\n",
    "            run_one(sent, ner_pipe=ner_pipe)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e024d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type an English sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Option deps local (sans venv)\n",
    "# ----------------------------\n",
    "LOCAL_DEPS_DIR = os.path.join(os.getcwd(), \".pylibs\")\n",
    "if os.path.isdir(LOCAL_DEPS_DIR) and LOCAL_DEPS_DIR not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_DEPS_DIR)\n",
    "\n",
    "print(\"Python kernel:\", sys.executable)\n",
    "print(\"Local deps dir:\", LOCAL_DEPS_DIR if os.path.isdir(LOCAL_DEPS_DIR) else \"(absent)\")\n",
    "\n",
    "def print_install_help():\n",
    "    py = sys.executable\n",
    "    print(\"\\n[install] Sans venv, en local dans .pylibs (utilise CE python):\")\n",
    "    print(f'  \"{py}\" -m pip install --upgrade --no-user-cfg --target \"{LOCAL_DEPS_DIR}\" nltk')\n",
    "    print(f'  \"{py}\" -m pip install --upgrade --no-user-cfg --target \"{LOCAL_DEPS_DIR}\" torch transformers tokenizers')\n",
    "    print(\"\\nSi tu as: ERROR: Can not combine '--user' and '--target'\")\n",
    "    print(\"  PowerShell (désactive config pip qui force --user):\")\n",
    "    print('    $env:PIP_CONFIG_FILE=\"NUL\"')\n",
    "    print(\"    Remove-Item Env:PIP_USER -ErrorAction SilentlyContinue\")\n",
    "    print(f'    \"{py}\" -m pip install --upgrade --no-user-cfg --target \"{LOCAL_DEPS_DIR}\" nltk')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) NLTK (optionnel): POS + WordNet lemma\n",
    "# ----------------------------\n",
    "NLTK_OK = False\n",
    "_WORDNET_OK = False\n",
    "_NLTK_IMPORT_ERR = None\n",
    "WNL = None\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    NLTK_OK = True\n",
    "except Exception as e:\n",
    "    NLTK_OK = False\n",
    "    _NLTK_IMPORT_ERR = e\n",
    "\n",
    "def _ensure_nltk():\n",
    "    if not NLTK_OK:\n",
    "        return\n",
    "    pkgs = [\n",
    "        (\"punkt\", \"tokenizers/punkt\"),\n",
    "        (\"averaged_perceptron_tagger\", \"taggers/averaged_perceptron_tagger\"),\n",
    "        (\"wordnet\", \"corpora/wordnet\"),\n",
    "        (\"omw-1.4\", \"corpora/omw-1.4\"),\n",
    "    ]\n",
    "    for pkg, probe in pkgs:\n",
    "        try:\n",
    "            nltk.data.find(probe)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] NLTK download failed for {pkg}: {e}\")\n",
    "\n",
    "if NLTK_OK:\n",
    "    _ensure_nltk()\n",
    "    try:\n",
    "        import nltk.corpus\n",
    "        _ = nltk.corpus.wordnet.synsets(\"dog\")\n",
    "        _WORDNET_OK = True\n",
    "    except Exception:\n",
    "        _WORDNET_OK = False\n",
    "\n",
    "if NLTK_OK and _WORDNET_OK:\n",
    "    WNL = WordNetLemmatizer()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Transformers NER (optionnel) SANS sentencepiece\n",
    "# ----------------------------\n",
    "HF_OK = False\n",
    "_HF_IMPORT_ERR = None\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "    HF_OK = True\n",
    "except Exception as e:\n",
    "    HF_OK = False\n",
    "    _HF_IMPORT_ERR = e\n",
    "\n",
    "HF_MODEL_NAME = \"dslim/bert-base-NER\"  # BERT WordPiece => no sentencepiece\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Tokenisation + décomposition contractions (avec offsets)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Tok:\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "    kind: str = \"tok\"          # \"tok\" | \"punct\" | \"contr\"\n",
    "    hint_pos: Optional[str] = None\n",
    "    hint_lemma: Optional[str] = None\n",
    "\n",
    "PUNCT_RE = re.compile(r\"\"\"^[“”\"‘’'()\\[\\]{}…,:;.!?]$\"\"\")\n",
    "\n",
    "def _is_punct(t: str) -> bool:\n",
    "    return bool(PUNCT_RE.match(t))\n",
    "\n",
    "def _is_number(t: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d+(?:[.,]\\d+)?\", t)) or bool(re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", t)) or bool(re.fullmatch(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\", t))\n",
    "\n",
    "MONTHS = r\"(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\"\n",
    "\n",
    "# NOTE: inclut aussi les composés digit-hyphen: 20-year-old\n",
    "TOKEN_RE = re.compile(\n",
    "    rf\"\"\"\n",
    "    (?:\\d{{4}}-\\d{{2}}-\\d{{2}}) |                          # 2026-02-12\n",
    "    (?:\\d{{1,2}}[/-]\\d{{1,2}}[/-]\\d{{2,4}}) |              # 02/12/2026\n",
    "    (?:{MONTHS}\\s+\\d{{1,2}}(?:,\\s*\\d{{2,4}})?) |           # Feb 12, 2026\n",
    "    (?:\\d{{1,2}}\\s+{MONTHS}(?:\\s+\\d{{2,4}})?) |            # 12 Feb 2026\n",
    "    (?:\\d+(?:-[A-Za-z]+)+) |                               # 20-year-old\n",
    "    (?:[A-Za-z]\\.){{2,}} |                                 # U.S. / Ph.D.\n",
    "    (?:\\d+(?:[.,]\\d+)?) |                                  # numbers\n",
    "    (?:[A-Za-z]+(?:[’'][A-Za-z]+)+) |                      # words with apostrophes (candidates contractions)\n",
    "    (?:[A-Za-z]+(?:-[A-Za-z]+)+) |                         # hyphen compounds\n",
    "    (?:[A-Za-z]+) |                                        # plain words\n",
    "    (?:[“”\"‘’'()\\[\\]{{}}…,:;.!?]) |                        # punctuation\n",
    "    (?:\\S)                                                 # fallback 1 char\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "def _lower(s: str) -> str:\n",
    "    return (s or \"\").lower()\n",
    "\n",
    "def _has_apostrophe(s: str) -> bool:\n",
    "    return (\"'\" in s) or (\"’\" in s)\n",
    "\n",
    "def _norm_apos(s: str) -> str:\n",
    "    return (s or \"\").replace(\"’\", \"'\")\n",
    "\n",
    "# special irregular contractions\n",
    "IRREG_BASE = {\n",
    "    \"can't\": \"can\",\n",
    "    \"won't\": \"will\",\n",
    "    \"shan't\": \"shall\",\n",
    "    \"ain't\": \"be\",\n",
    "}\n",
    "\n",
    "# suffix -> expansion (deterministic)\n",
    "SUFFIX_EXPAND = {\n",
    "    \"n't\": (\"not\", \"RB\", \"not\"),\n",
    "    \"'ve\": (\"have\", \"VB\", \"have\"),\n",
    "    \"'re\": (\"are\",  \"VB\", \"be\"),\n",
    "    \"'ll\": (\"will\", \"VB\", \"will\"),\n",
    "    \"'m\":  (\"am\",   \"VB\", \"be\"),\n",
    "    \"'d\":  (\"would\",\"VB\", \"would\"),   # deterministe: would (pas had)\n",
    "    \"'s\":  (\"__S__\", None, None),      # résolu ensuite: POS ou is/has\n",
    "}\n",
    "\n",
    "def _looks_like_word(s: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[A-Za-z]+(?:['’][A-Za-z]+)*\", s)) or bool(re.fullmatch(r\"[A-Za-z]+(?:-[A-Za-z]+)+\", s)) or bool(re.fullmatch(r\"\\d+(?:-[A-Za-z]+)+\", s))\n",
    "\n",
    "def _split_plural_possessive(tok: str, abs_start: int) -> Optional[List[Tok]]:\n",
    "    # Joneses'  -> Joneses + '\n",
    "    t = _norm_apos(tok)\n",
    "    if re.fullmatch(r\"[A-Za-z]+s'\", t) and len(t) >= 3:\n",
    "        base_len = len(t) - 1\n",
    "        base = tok[:-1]  # keep original char\n",
    "        return [\n",
    "            Tok(base, abs_start, abs_start + base_len, kind=\"tok\"),\n",
    "            Tok(\"'\", abs_start + base_len, abs_start + base_len + 1, kind=\"contr\", hint_pos=\"POS\", hint_lemma=\"∅\"),\n",
    "        ]\n",
    "    return None\n",
    "\n",
    "def split_contractions(tok: str, abs_start: int) -> List[Tok]:\n",
    "    \"\"\"\n",
    "    Décompose les contractions, y compris chaînes: shouldn't've.\n",
    "    Ne casse pas les noms type O'Reilly (suffix non reconnu).\n",
    "    \"\"\"\n",
    "    # 1) plural possessive\n",
    "    pp = _split_plural_possessive(tok, abs_start)\n",
    "    if pp is not None:\n",
    "        return pp\n",
    "\n",
    "    raw = tok\n",
    "    t = _norm_apos(raw)\n",
    "\n",
    "    # si pas apostrophe, rien\n",
    "    if \"'\" not in t:\n",
    "        return [Tok(raw, abs_start, abs_start + len(raw), kind=\"tok\")]\n",
    "\n",
    "    # si suffix inconnu (ex: O'Reilly), on garde\n",
    "    # on décide de splitter seulement si on peut matcher un suffix de contraction à la fin.\n",
    "    # (sauf pour chaînes où on va enlever plusieurs suffixes)\n",
    "    cur_text = t\n",
    "    cur_end = abs_start + len(raw)\n",
    "\n",
    "    suffix_toks_rev: List[Tok] = []\n",
    "\n",
    "    # boucle \"peel suffixes\"\n",
    "    while True:\n",
    "        matched = False\n",
    "        for suf in [\"n't\", \"'ve\", \"'re\", \"'ll\", \"'m\", \"'d\", \"'s\"]:\n",
    "            if cur_text.endswith(suf):\n",
    "                matched = True\n",
    "                # span suffix dans le token original\n",
    "                suf_len = len(suf)\n",
    "                suf_start = cur_end - suf_len\n",
    "                suf_end = cur_end\n",
    "\n",
    "                exp_txt, exp_pos, exp_lem = SUFFIX_EXPAND[suf]\n",
    "\n",
    "                suffix_toks_rev.append(\n",
    "                    Tok(exp_txt, suf_start, suf_end, kind=\"contr\", hint_pos=exp_pos, hint_lemma=exp_lem)\n",
    "                )\n",
    "\n",
    "                # strip suffix\n",
    "                cur_end -= suf_len\n",
    "                cur_text = cur_text[:-suf_len]\n",
    "                break\n",
    "        if not matched:\n",
    "            break\n",
    "\n",
    "    # si aucun suffix reconnu, pas contraction\n",
    "    if not suffix_toks_rev:\n",
    "        return [Tok(raw, abs_start, abs_start + len(raw), kind=\"tok\")]\n",
    "\n",
    "    base_raw = raw[: (cur_end - abs_start)]\n",
    "    base_norm = _norm_apos(base_raw)\n",
    "\n",
    "    # cas irréguliers (can't/won't/...)\n",
    "    bn_low = base_norm.lower() + (\"n't\" if t.endswith(\"n't\") and not base_norm.endswith(\"n\") else \"\")\n",
    "    # simplifie: on check l’original complet\n",
    "    full_low = _norm_apos(raw).lower()\n",
    "    if full_low in IRREG_BASE:\n",
    "        # base span = partie avant \"n't\" (souvent \"ca\"/\"wo\"), mais texte = can/will\n",
    "        base_text = IRREG_BASE[full_low]\n",
    "        base_tok = Tok(base_text, abs_start, cur_end, kind=\"contr\", hint_pos=\"VB\", hint_lemma=base_text)\n",
    "    else:\n",
    "        base_tok = Tok(base_raw if base_raw else raw, abs_start, cur_end, kind=\"tok\")\n",
    "\n",
    "    # remettre dans l’ordre: base + suffixes (reverse)\n",
    "    suffix_toks = list(reversed(suffix_toks_rev))\n",
    "    out = [base_tok] + suffix_toks\n",
    "\n",
    "    return out\n",
    "\n",
    "def tokenize_with_spans(text: str) -> List[Tok]:\n",
    "    toks: List[Tok] = []\n",
    "    for m in TOKEN_RE.finditer(text or \"\"):\n",
    "        s = m.group(0)\n",
    "        a, b = m.start(), m.end()\n",
    "\n",
    "        if _is_punct(s):\n",
    "            toks.append(Tok(s, a, b, kind=\"punct\", hint_pos=\"PUNCT\", hint_lemma=\"∅\"))\n",
    "            continue\n",
    "\n",
    "        if _has_apostrophe(s) and _looks_like_word(s):\n",
    "            toks.extend(split_contractions(s, a))\n",
    "        else:\n",
    "            toks.append(Tok(s, a, b, kind=\"tok\"))\n",
    "    return toks\n",
    "\n",
    "# ----------------------------\n",
    "# 4) POS: NLTK si dispo, sinon heuristiques + mapping tags simplifiés\n",
    "# ----------------------------\n",
    "DET = {\"the\",\"a\",\"an\",\"this\",\"that\",\"these\",\"those\",\"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\"}\n",
    "PREP = {\"in\",\"on\",\"at\",\"to\",\"from\",\"of\",\"for\",\"with\",\"without\",\"by\",\"between\",\"among\",\"into\",\"onto\",\"over\",\"under\",\"about\",\"across\",\"through\",\"during\",\"before\",\"after\",\"since\"}\n",
    "CONJ = {\"and\",\"or\",\"but\",\"so\",\"yet\",\"nor\"}\n",
    "PRON = {\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\"who\",\"whom\",\"whose\",\"which\",\"that\"}\n",
    "ADV = {\"very\",\"too\",\"well\",\"badly\",\"here\",\"there\",\"yesterday\",\"today\",\"tomorrow\",\"often\",\"sometimes\",\"already\",\"still\",\"also\"}\n",
    "\n",
    "def guess_pos_heur(token: str, prev_tok: Optional[str], next_tok: Optional[str]) -> str:\n",
    "    tl = _lower(token)\n",
    "\n",
    "    if _is_punct(token):\n",
    "        return \"PUNCT\"\n",
    "    if _is_number(token):\n",
    "        return \"CD\"\n",
    "    if tl in DET:\n",
    "        return \"DT\"\n",
    "    if tl in PREP:\n",
    "        return \"IN\"\n",
    "    if tl in CONJ:\n",
    "        return \"CC\"\n",
    "    if tl in PRON:\n",
    "        return \"PRP\"\n",
    "    if tl in ADV:\n",
    "        return \"RB\"\n",
    "    if tl in {\"not\"}:\n",
    "        return \"RB\"\n",
    "    if tl in {\"would\",\"will\",\"have\",\"has\",\"had\",\"am\",\"are\",\"is\",\"be\",\"been\",\"being\"}:\n",
    "        return \"VB\"\n",
    "\n",
    "    # capitalized -> NNP (sauf articles)\n",
    "    if re.fullmatch(r\"[A-Z][a-z]+(?:['’][A-Za-z]+)?\", token) and tl not in DET and tl not in PRON:\n",
    "        return \"NNP\"\n",
    "\n",
    "    # verb-ish\n",
    "    if re.search(r\"(ing|ed)$\", tl) or re.search(r\"(ize|ise|ate|fy)$\", tl):\n",
    "        return \"VB\"\n",
    "\n",
    "    # adjective-ish\n",
    "    if re.search(r\"(able|ible|ous|ive|al|ic|ish|less|ful)$\", tl):\n",
    "        return \"JJ\"\n",
    "\n",
    "    # noun plural-ish\n",
    "    if tl.endswith(\"s\") and len(tl) > 3:\n",
    "        return \"NN\"\n",
    "\n",
    "    return \"NN\"\n",
    "\n",
    "def map_penn_to_simple(p: str) -> str:\n",
    "    if not p:\n",
    "        return \"NN\"\n",
    "    pu = p.upper()\n",
    "\n",
    "    if pu in {\".\", \",\", \":\", \"``\", \"''\", \"-LRB-\", \"-RRB-\"}:\n",
    "        return \"PUNCT\"\n",
    "\n",
    "    if pu.startswith(\"NNP\") or pu == \"NNPS\":\n",
    "        return \"NNP\"\n",
    "    if pu.startswith(\"NN\"):\n",
    "        return \"NN\"\n",
    "    if pu.startswith(\"VB\") or pu == \"MD\":\n",
    "        return \"VB\"\n",
    "    if pu.startswith(\"JJ\"):\n",
    "        return \"JJ\"\n",
    "    if pu.startswith(\"RB\"):\n",
    "        return \"RB\"\n",
    "    if pu in {\"IN\", \"TO\"}:\n",
    "        return \"IN\"\n",
    "    if pu in {\"DT\", \"PDT\"}:\n",
    "        return \"DT\"\n",
    "    if pu in {\"PRP\", \"PRP$\", \"WP\", \"WP$\"}:\n",
    "        return \"PRP\"\n",
    "    if pu == \"WDT\":\n",
    "        return \"WDT\"\n",
    "    if pu == \"CC\":\n",
    "        return \"CC\"\n",
    "    if pu == \"CD\":\n",
    "        return \"CD\"\n",
    "    if pu == \"RP\":\n",
    "        return \"RP\"\n",
    "    if pu == \"POS\":\n",
    "        return \"POS\"\n",
    "\n",
    "    return \"NN\"\n",
    "\n",
    "def pos_tag_simple(tokens: List[str]) -> Tuple[List[str], str]:\n",
    "    if NLTK_OK:\n",
    "        try:\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            mapped = [map_penn_to_simple(p) for _, p in tagged]\n",
    "            return mapped, \"NLTK (mapped->simple)\"\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    # fallback heuristics\n",
    "    pos = []\n",
    "    for i, t in enumerate(tokens):\n",
    "        prev = tokens[i-1] if i > 0 else None\n",
    "        nxt = tokens[i+1] if i+1 < len(tokens) else None\n",
    "        pos.append(guess_pos_heur(t, prev, nxt))\n",
    "    return pos, \"heuristics\"\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Résolution de __S__ (copule vs possessif)\n",
    "# ----------------------------\n",
    "PRON_SUBJ = {\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"who\",\"that\",\"there\",\"here\",\"what\",\"where\",\"when\",\"how\"}\n",
    "def looks_like_nounish(tok: str) -> bool:\n",
    "    if not tok:\n",
    "        return False\n",
    "    if _is_number(tok):\n",
    "        return True\n",
    "    if re.fullmatch(r\"[A-Z][a-z]+\", tok):\n",
    "        return True\n",
    "    if re.fullmatch(r\"[A-Za-z]+(?:-[A-Za-z]+)+\", tok):\n",
    "        return True\n",
    "    if re.fullmatch(r\"[A-Za-z]+\", tok) and tok.lower() not in DET and tok.lower() not in PREP and tok.lower() not in CONJ:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def resolve_s_contractions(toks: List[Tok]) -> None:\n",
    "    \"\"\"\n",
    "    Modifie en place:\n",
    "      - __S__ -> is (copule) OU 's (POS)\n",
    "    \"\"\"\n",
    "    for i, tk in enumerate(toks):\n",
    "        if tk.text != \"__S__\":\n",
    "            continue\n",
    "        prev = toks[i-1].text if i > 0 else \"\"\n",
    "        nxt = toks[i+1].text if i+1 < len(toks) else \"\"\n",
    "        prev_l = prev.lower()\n",
    "\n",
    "        # Si précédent est pronom sujet -> \"is\"\n",
    "        if prev_l in PRON_SUBJ:\n",
    "            tk.text = \"is\"\n",
    "            tk.hint_pos = \"VB\"\n",
    "            tk.hint_lemma = \"be\"\n",
    "            continue\n",
    "\n",
    "        # Si suivant ressemble à un nom/adjectif (ex: John's car / John's big car) -> possessif\n",
    "        # Heuristique: si suivant est \"nounish\" -> POS\n",
    "        if looks_like_nounish(nxt) or nxt.lower() in DET:\n",
    "            tk.text = \"'s\"\n",
    "            tk.hint_pos = \"POS\"\n",
    "            tk.hint_lemma = \"∅\"\n",
    "            continue\n",
    "\n",
    "        # Sinon -> copule\n",
    "        tk.text = \"is\"\n",
    "        tk.hint_pos = \"VB\"\n",
    "        tk.hint_lemma = \"be\"\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Lemma: WordNet si dispo, sinon règles + hints\n",
    "# ----------------------------\n",
    "def _penn_to_wordnet_simple(pos: str) -> str:\n",
    "    if not pos:\n",
    "        return \"n\"\n",
    "    p = pos.upper()\n",
    "    if p.startswith(\"VB\"):\n",
    "        return \"v\"\n",
    "    if p.startswith(\"JJ\"):\n",
    "        return \"a\"\n",
    "    if p.startswith(\"RB\"):\n",
    "        return \"r\"\n",
    "    return \"n\"\n",
    "\n",
    "def lemma_en(token: str, pos: str) -> str:\n",
    "    if _is_punct(token):\n",
    "        return \"∅\"\n",
    "    if _is_number(token):\n",
    "        return token\n",
    "\n",
    "    tl = token.lower()\n",
    "\n",
    "    # possessive marker tokens\n",
    "    if token in {\"'s\", \"'\"}:\n",
    "        return \"∅\"\n",
    "\n",
    "    # WordNet lemmatizer\n",
    "    if WNL is not None:\n",
    "        try:\n",
    "            return WNL.lemmatize(tl, _penn_to_wordnet_simple(pos))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # fallback rules\n",
    "    if pos == \"NN\" and tl.endswith(\"ies\") and len(tl) > 4:\n",
    "        return tl[:-3] + \"y\"\n",
    "    if pos == \"NN\" and tl.endswith(\"s\") and not tl.endswith(\"ss\") and len(tl) > 3:\n",
    "        return tl[:-1]\n",
    "\n",
    "    if pos == \"VB\":\n",
    "        if tl.endswith(\"ing\") and len(tl) > 5:\n",
    "            base = tl[:-3]\n",
    "            if len(base) >= 2 and base[-1] == base[-2]:\n",
    "                base = base[:-1]\n",
    "            return base\n",
    "        if tl.endswith(\"ed\") and len(tl) > 4:\n",
    "            base = tl[:-2]\n",
    "            return base\n",
    "        return tl\n",
    "\n",
    "    return tl\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Fix \"Buffalo\" (verbe) déterministe\n",
    "# ----------------------------\n",
    "def fix_buffalo_pos(tokens: List[str], pos: List[str], lem: List[str]) -> None:\n",
    "    idxs = [i for i,t in enumerate(tokens) if t.lower() == \"buffalo\"]\n",
    "    if len(idxs) < 4:\n",
    "        return  # on évite de sur-corriger les phrases normales\n",
    "\n",
    "    for i in range(1, len(tokens)-1):\n",
    "        if tokens[i].lower() != \"buffalo\":\n",
    "            continue\n",
    "        # \"slot verbe\" : entouré de noms/proper nouns\n",
    "        if pos[i] in {\"NN\",\"NNP\"} and pos[i-1] in {\"NN\",\"NNP\"} and pos[i+1] in {\"NN\",\"NNP\"}:\n",
    "            pos[i] = \"VB\"\n",
    "            lem[i] = \"buffalo\"\n",
    "\n",
    "# ----------------------------\n",
    "# 8) DATE spans (règles) + NER IA + fixes bruit Buffalo\n",
    "# ----------------------------\n",
    "def date_spans(text: str) -> List[Tuple[int,int,str]]:\n",
    "    spans = []\n",
    "    if not text:\n",
    "        return spans\n",
    "\n",
    "    for m in re.finditer(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "    for m in re.finditer(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "    for m in re.finditer(r\"\\b(19\\d{2}|20\\d{2})\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "    for m in re.finditer(rf\"\\b{MONTHS}\\s+\\d{{1,2}}(?:,\\s*\\d{{2,4}})?\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "    for m in re.finditer(rf\"\\b\\d{{1,2}}\\s+{MONTHS}(?:\\s+\\d{{2,4}})?\\b\", text):\n",
    "        spans.append((m.start(), m.end(), \"DATE\"))\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    out = []\n",
    "    last_end = -1\n",
    "    for a,b,lab in spans:\n",
    "        if a >= last_end:\n",
    "            out.append((a,b,lab))\n",
    "            last_end = b\n",
    "    return out\n",
    "\n",
    "def load_hf_ner(model_name: str):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.set_num_threads(1)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    mdl.eval()\n",
    "\n",
    "    ner = pipeline(\n",
    "        \"ner\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        device=-1,\n",
    "        grouped_entities=True\n",
    "    )\n",
    "    return ner\n",
    "\n",
    "def _norm_ner_label(raw: str) -> str:\n",
    "    r = (raw or \"\").upper().strip()\n",
    "    r = r.replace(\"I-\", \"\").replace(\"B-\", \"\")\n",
    "    if r in (\"PER\", \"PERSON\"):\n",
    "        return \"PERS\"\n",
    "    if r in (\"ORG\", \"LOC\", \"MISC\", \"PERS\"):\n",
    "        return r\n",
    "    return r or \"O\"\n",
    "\n",
    "def hf_spans(ner_pipe, text: str) -> List[Tuple[int,int,str]]:\n",
    "    spans = []\n",
    "    res = ner_pipe(text)\n",
    "    for it in res:\n",
    "        lab = it.get(\"entity_group\") or it.get(\"entity\") or \"\"\n",
    "        lab = _norm_ner_label(lab)\n",
    "        if lab in (\"PERS\",\"ORG\",\"LOC\",\"MISC\"):\n",
    "            a = int(it.get(\"start\", -1))\n",
    "            b = int(it.get(\"end\", -1))\n",
    "            if 0 <= a < b:\n",
    "                spans.append((a,b,lab))\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    out = []\n",
    "    last_end = -1\n",
    "    for a,b,lab in spans:\n",
    "        if a >= last_end:\n",
    "            out.append((a,b,lab))\n",
    "            last_end = b\n",
    "    return out\n",
    "\n",
    "def apply_spans_to_tokens(toks: List[Tok], spans: List[Tuple[int,int,str]], labels: List[str]):\n",
    "    for a,b,lab in spans:\n",
    "        idxs = [i for i,t in enumerate(toks) if not (t.end <= a or t.start >= b)]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        if any(labels[i] != \"O\" for i in idxs):\n",
    "            continue\n",
    "        labels[idxs[0]] = f\"B-{lab}\"\n",
    "        for i in idxs[1:]:\n",
    "            labels[i] = f\"I-{lab}\"\n",
    "\n",
    "def enforce_bio(labels: List[str]) -> List[str]:\n",
    "    out = labels[:]\n",
    "    for i in range(len(out)):\n",
    "        if out[i].startswith(\"I-\"):\n",
    "            typ = out[i][2:]\n",
    "            if i == 0 or out[i-1] == \"O\" or (out[i-1].startswith((\"B-\",\"I-\")) and out[i-1][2:] != typ):\n",
    "                out[i] = \"B-\" + typ\n",
    "    return out\n",
    "\n",
    "def fix_buffalo_ner(tokens: List[str], labels: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Débruite:\n",
    "      - Buffalo (majuscule) taggué MISC -> LOC\n",
    "      - buffalo (minuscule) taggué MISC -> O\n",
    "    \"\"\"\n",
    "    out = labels[:]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok == \"Buffalo\" and out[i].endswith(\"MISC\"):\n",
    "            out[i] = out[i].replace(\"MISC\", \"LOC\")\n",
    "        if tok == \"buffalo\" and out[i].endswith(\"MISC\"):\n",
    "            out[i] = \"O\"\n",
    "    return enforce_bio(out)\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Entities depuis BIO\n",
    "# ----------------------------\n",
    "def _join_entity_tokens(parts: List[str]) -> str:\n",
    "    s = \" \".join(parts)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\(\\s+\", \"(\", s)\n",
    "    s = re.sub(r\"\\s+\\)\", \")\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def entities_from_bio(tokens: List[str], labels: List[str]) -> Dict[str, List[str]]:\n",
    "    ents: Dict[str, List[str]] = {}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        lab = labels[i]\n",
    "        if not lab.startswith(\"B-\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        typ = lab[2:]\n",
    "        j = i + 1\n",
    "        parts = [tokens[i]]\n",
    "        while j < len(tokens) and labels[j] == f\"I-{typ}\":\n",
    "            parts.append(tokens[j])\n",
    "            j += 1\n",
    "        val = _join_entity_tokens([p for p in parts if p not in {\"'\"}])\n",
    "        if val:\n",
    "            ents.setdefault(typ, []).append(val)\n",
    "        i = j\n",
    "    return ents\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Affichage \"style arabe\"\n",
    "# ----------------------------\n",
    "def print_table(tokens: List[str], pos: List[str], lem: List[str], max_rows: Optional[int] = None):\n",
    "    w_token = max(10, min(30, max(len(t) for t in tokens) if tokens else 10))\n",
    "    rows = range(len(tokens)) if max_rows is None else range(min(len(tokens), max_rows))\n",
    "    for i in rows:\n",
    "        print(f\"{tokens[i]:>{w_token}}  {pos[i]:<7} lemma={lem[i]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Fallback NER (si HF indispo): capitalized sequences + dates\n",
    "# ----------------------------\n",
    "def fallback_ner_labels(tokens: List[str], pos: List[str]) -> List[str]:\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    # simple: suite de mots capitalisés -> MISC, après prépositions -> LOC\n",
    "    preps = {\"in\",\"at\",\"from\",\"to\",\"near\",\"around\",\"within\",\"outside\",\"inside\",\"of\"}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if re.fullmatch(r\"[A-Z][a-z]+\", tokens[i]):\n",
    "            j = i + 1\n",
    "            while j < len(tokens) and re.fullmatch(r\"[A-Z][a-z]+\", tokens[j]):\n",
    "                j += 1\n",
    "            typ = \"MISC\"\n",
    "            if i > 0 and tokens[i-1].lower() in preps:\n",
    "                typ = \"LOC\"\n",
    "            labels[i] = f\"B-{typ}\"\n",
    "            for k in range(i+1, j):\n",
    "                labels[k] = f\"I-{typ}\"\n",
    "            i = j\n",
    "            continue\n",
    "        i += 1\n",
    "\n",
    "    return enforce_bio(labels)\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Runner\n",
    "# ----------------------------\n",
    "def run_one(text: str, ner_pipe=None):\n",
    "    print(\"=\" * 90)\n",
    "    print(\"INPUT:\", text)\n",
    "\n",
    "    toks = tokenize_with_spans(text)\n",
    "    resolve_s_contractions(toks)\n",
    "\n",
    "    tokens = [t.text for t in toks]\n",
    "\n",
    "    # POS\n",
    "    pos, pos_mode = pos_tag_simple(tokens)\n",
    "\n",
    "    # appliquer hints de tokenisation (not/have/would/POS...)\n",
    "    for i, tk in enumerate(toks):\n",
    "        if tk.hint_pos is not None:\n",
    "            pos[i] = tk.hint_pos\n",
    "\n",
    "    # Lemma\n",
    "    lem = []\n",
    "    lemma_mode = \"fallback rules\"\n",
    "    if WNL is not None:\n",
    "        lemma_mode = \"NLTK WordNetLemmatizer\"\n",
    "    for i, tk in enumerate(toks):\n",
    "        if tk.hint_lemma is not None:\n",
    "            lem.append(tk.hint_lemma)\n",
    "        else:\n",
    "            lem.append(lemma_en(tk.text, pos[i]))\n",
    "\n",
    "    # Fix Buffalo POS\n",
    "    fix_buffalo_pos(tokens, pos, lem)\n",
    "\n",
    "    # Print POS/lemma table\n",
    "    print_table(tokens, pos, lem, max_rows=None)\n",
    "\n",
    "    # NER\n",
    "    labels = [\"O\"] * len(toks)\n",
    "    ner_mode = \"fallback (rules)\"\n",
    "\n",
    "    if ner_pipe is not None:\n",
    "        try:\n",
    "            spans = hf_spans(ner_pipe, text)\n",
    "            apply_spans_to_tokens(toks, spans, labels)\n",
    "            ner_mode = f\"IA (transformers): {HF_MODEL_NAME}\"\n",
    "        except Exception as e:\n",
    "            ner_mode = f\"fallback (rules) - NER IA indisponible: {e}\"\n",
    "            labels = fallback_ner_labels(tokens, pos)\n",
    "    else:\n",
    "        labels = fallback_ner_labels(tokens, pos)\n",
    "\n",
    "    # DATE rules (n’écrase pas une entité existante)\n",
    "    d_sp = date_spans(text)\n",
    "    apply_spans_to_tokens(toks, d_sp, labels)\n",
    "    labels = enforce_bio(labels)\n",
    "\n",
    "    # Fix bruit Buffalo\n",
    "    labels = fix_buffalo_ner(tokens, labels)\n",
    "\n",
    "    # Print NER output\n",
    "    print()\n",
    "    print(f\"NER ({ner_mode}) (token, label):\")\n",
    "    print(list(zip(tokens, labels)))\n",
    "\n",
    "    ents = entities_from_bio(tokens, labels)\n",
    "    print(\"Entities:\")\n",
    "    for k in sorted(ents.keys()):\n",
    "        for v in ents[k]:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Audit (heuristique, pas une mesure de justesse)\n",
    "    total = max(1, len(tokens))\n",
    "    nn_like = sum(1 for p in pos if p in (\"NN\", \"NNP\"))\n",
    "    contr = sum(1 for t in toks if t.kind == \"contr\" or t.text in {\"not\",\"have\",\"would\",\"is\",\"'s\",\"'\"} )\n",
    "    print()\n",
    "    print(\"Audit (heuristique, pas une mesure de justesse):\")\n",
    "    print(f\"  tokens total     : {len(tokens)}\")\n",
    "    print(f\"  POS mode         : {pos_mode}\")\n",
    "    print(f\"  lemma mode       : {lemma_mode}\")\n",
    "    print(f\"  NN/NNP ratio     : {nn_like}/{len(pos)} = {nn_like/total*100:.2f} %\")\n",
    "    print(f\"  contraction tokens (approx): {contr} ({contr/total*100:.2f} %)\")\n",
    "\n",
    "def split_input_into_sentences(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    dq = re.findall(r\"\\\"([^\\\"]+)\\\"\", s)\n",
    "    gu = re.findall(r\"«([^»]+)»\", s)\n",
    "    parts = [p.strip() for p in (dq + gu) if p.strip()]\n",
    "    if parts:\n",
    "        return parts\n",
    "\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+\", s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def main():\n",
    "    ner_pipe = None\n",
    "    if HF_OK:\n",
    "        try:\n",
    "            ner_pipe = load_hf_ner(HF_MODEL_NAME)\n",
    "        except Exception as e:\n",
    "            ner_pipe = None\n",
    "            print(\"[warn] NER IA non chargé, fallback rules. Cause:\", e)\n",
    "    else:\n",
    "        print(\"[info] transformers/torch non dispo -> NER fallback rules.\")\n",
    "        if _HF_IMPORT_ERR is not None:\n",
    "            print(\"       Cause import:\", _HF_IMPORT_ERR)\n",
    "\n",
    "    if not NLTK_OK:\n",
    "        print(\"[info] nltk non dispo -> POS+lemma heuristiques.\")\n",
    "        if _NLTK_IMPORT_ERR is not None:\n",
    "            print(\"       Cause import:\", _NLTK_IMPORT_ERR)\n",
    "        print_install_help()\n",
    "    else:\n",
    "        if not _WORDNET_OK:\n",
    "            print(\"[info] NLTK OK mais WordNet pas dispo -> lemma fallback rules.\")\n",
    "\n",
    "    print()\n",
    "    print(\"Type an English sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\")\n",
    "    while True:\n",
    "        s = input(\"\\nEnglish> \").strip()\n",
    "        if not s:\n",
    "            break\n",
    "        for sent in split_input_into_sentences(s):\n",
    "            run_one(sent, ner_pipe=ner_pipe)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
