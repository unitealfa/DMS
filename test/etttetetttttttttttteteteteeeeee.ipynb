{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9e27ff",
   "metadata": {},
   "source": [
    "## lire le document de quelle type il est et si cest une image ou contien du text dans sont code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2dc739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'documents/francais.docx',\n",
       "  'ext': '.docx',\n",
       "  'mime': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
       "  'label': 'Word document (DOCX)',\n",
       "  'content': 'text'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List, Dict, Any\n",
    "\n",
    "\n",
    "# Saisie possible:\n",
    "# INPUT_FILE = \"a.pdf, b.docx, c.png\"\n",
    "# INPUT_FILE = [\"a.pdf\", \"b.docx\", \"c.png\"]\n",
    "INPUT_FILE: Optional[Union[str, Sequence[str]]] = (\n",
    "    # \"epsteanpdf.pdf, epsteain22.pdf, testexcel.xlsx, testword.docx, image2tab.webp, contras-14page.pdf, signettab.png\"\n",
    "    # \"contras-14page.pdf, testword.docx, testexcel.xlsx, signettab.png, image2tab.webp\"\n",
    "    \"documents/francais.docx\"\n",
    ")\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1     # 1 caractère => \"text\"\n",
    "MIN_CHARS_PDF = 30       # seuil de texte extrait\n",
    "PDF_MAX_PAGES = 3        # on teste les N premières pages\n",
    "\n",
    "# Dossiers de recherche si un nom est donné sans chemin (utile en notebook)\n",
    "SEARCH_DIRS = [\n",
    "    os.getcwd(),\n",
    "    \"/mnt/data\",  # utile dans l'environnement ChatGPT\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "# ----------------- input parsing -----------------\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    \"\"\"Retourne toujours une liste. Supporte une string avec virgules (CSV).\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s:\n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Résout un chemin:\n",
    "    - si p existe tel quel -> retourne p\n",
    "    - sinon essaie SEARCH_DIRS + basename(p)\n",
    "    - sinon retourne None (introuvable)\n",
    "    \"\"\"\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ----------------- format detection -----------------\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    # ZIP containers (DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB/ZIP)\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                # EPUB\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                # Office OpenXML\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                # OpenDocument\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    # Ancien Office (OLE2)\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "# ----------------- text vs image_only -----------------\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    \"\"\"Compte du texte dans du XML (éléments + fallback simple).\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    \"\"\"\n",
    "    DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB\n",
    "    True si on trouve au moins MIN_CHARS_OFFICE caractères.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                # corps\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                # headers/footers (souvent du texte “isolé”)\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                if \"content.xml\" in names:\n",
    "                    return _xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE\n",
    "                return False\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    \"\"\"Retourne PdfReader depuis pypdf ou PyPDF2, ou None si indisponible.\"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    PDF:\n",
    "    - True si extract_text() produit assez de caractères, OU si fonts / opérateurs texte présents.\n",
    "    - Si aucune lib PDF n'est dispo: fallback binaire (cherche /Font ou opérateurs BT/Tj).\n",
    "    \"\"\"\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        # fallback binaire: moins fiable, mais évite de renvoyer faux systématique\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)  # 2MB max\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "\n",
    "        extracted_score = 0\n",
    "        saw_font = False\n",
    "        saw_text_ops = False\n",
    "\n",
    "        for page in pages:\n",
    "            # 1) extraction texte\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "\n",
    "            # 2) fonts dans resources\n",
    "            try:\n",
    "                res = page.get(\"/Resources\") or {}\n",
    "                font = res.get(\"/Font\")\n",
    "                if font:\n",
    "                    saw_font = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # 3) opérateurs texte dans stream\n",
    "            try:\n",
    "                contents = page.get_contents()\n",
    "                if contents is None:\n",
    "                    continue\n",
    "                if hasattr(contents, \"get_data\"):\n",
    "                    data = contents.get_data()\n",
    "                else:\n",
    "                    data = b\"\".join(c.get_data() for c in contents)  # type: ignore\n",
    "                if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                    saw_text_ops = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return saw_font or saw_text_ops\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    \"\"\"Retourne seulement: 'text' ou 'image_only'.\"\"\"\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    # Images => image_only\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    # Formats texte compressés (Office/ODF/EPUB)\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    # Tout le reste => image_only (car tu veux 2 états)\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "def analyze_many_two_states(input_file: Optional[Union[str, Sequence[str]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sortie:\n",
    "      [{\"path\": ..., \"ext\": ..., \"mime\": ..., \"label\": ..., \"content\": \"text|image_only\"}, ...]\n",
    "    Ignore les fichiers introuvables.\n",
    "    \"\"\"\n",
    "    raw_paths = normalize_input_files(input_file)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for raw in raw_paths:\n",
    "        p = resolve_path(raw)\n",
    "        if p is None:\n",
    "            continue\n",
    "\n",
    "        ft = detect_path_type(p)\n",
    "        out.append({\n",
    "            \"path\": p,\n",
    "            \"ext\": ft.ext,\n",
    "            \"mime\": ft.mime,\n",
    "            \"label\": ft.label,\n",
    "            \"content\": content_kind_two_states(p, ft),\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test\n",
    "analyze_many_two_states(INPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d63c5",
   "metadata": {},
   "source": [
    "### si image faire passer sur un pretraitemetn lamelirer sinon un document avce text dans sont code source aallors pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99d5904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] content='text' -> c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "\n",
    "\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    np = None\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In notebooks __file__ is undefined; fall back to current working directory.\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_LANG = \"fra\"\n",
    "DEFAULT_CONTRAST = 1.5\n",
    "DEFAULT_SHARPNESS = 1.2\n",
    "DEFAULT_BRIGHTNESS = 1.0\n",
    "DEFAULT_UPSCALE = 1.5\n",
    "DEFAULT_DPI = 300\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1\n",
    "MIN_CHARS_PDF = 30\n",
    "PDF_MAX_PAGES = 3\n",
    "SEARCH_DIRS = [os.getcwd(), \"/mnt/data\"]  # utile en notebook\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s: \n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return os.path.abspath(p)\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return os.path.abspath(alt)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                return (\"content.xml\" in names) and (_xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE)\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    PdfReader = _get_pdf_reader()\n",
    "\n",
    "    if PdfReader is None:\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "        extracted_score = 0\n",
    "\n",
    "        for page in pages:\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "# --------- ROUTAGE ---------\n",
    "ORIGINAL_INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "_raw_items = normalize_input_files(ORIGINAL_INPUT_FILE)\n",
    "\n",
    "IMAGE_ONLY_FILES: List[str] = []\n",
    "TEXT_FILES: List[str] = []\n",
    "MISSING_FILES: List[str] = []\n",
    "\n",
    "for item in _raw_items:\n",
    "    p = resolve_path(item)\n",
    "    if p is None:\n",
    "        MISSING_FILES.append(item)\n",
    "        continue\n",
    "\n",
    "    ft = detect_path_type(p)\n",
    "    kind = content_kind_two_states(p, ft)\n",
    "\n",
    "    if kind == \"image_only\":\n",
    "        IMAGE_ONLY_FILES.append(p)\n",
    "    else:\n",
    "        TEXT_FILES.append(p)\n",
    "        print(f\"[skip] content='text' -> {p}\")\n",
    "\n",
    "# IMPORTANT: ton code OCR (cellule suivante) reste inchangé, il lira INPUT_FILE ici\n",
    "INPUT_FILE = IMAGE_ONLY_FILES\n",
    "\n",
    "if MISSING_FILES:\n",
    "    print(\"[missing] fichiers introuvables:\")\n",
    "    for m in MISSING_FILES:\n",
    "        print(\" -\", m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SHOW_PREPROCESSED = True   #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhanceOptions:\n",
    "    contrast: float = DEFAULT_CONTRAST\n",
    "    sharpness: float = DEFAULT_SHARPNESS\n",
    "    brightness: float = DEFAULT_BRIGHTNESS\n",
    "    upscale: float = DEFAULT_UPSCALE\n",
    "    gamma: Optional[float] = None  # gamma correction; <1 brightens darks, >1 darkens\n",
    "    pad: int = 0  # pixels to pad around the image\n",
    "    median: Optional[int] = None  # kernel size for median filter (odd int, e.g., 3)\n",
    "    unsharp_radius: Optional[float] = None  # e.g., 1.0\n",
    "    unsharp_percent: int = 150\n",
    "    invert: bool = False\n",
    "    autocontrast_cutoff: Optional[int] = None  # 0-100; percentage to clip for autocontrast\n",
    "    equalize: bool = False  # histogram equalization\n",
    "    auto_rotate: bool = False  # attempt orientation detection + rotate\n",
    "    otsu: bool = False  # auto-threshold with Otsu (requires numpy)\n",
    "    threshold: Optional[int] = None  # 0-255; if set, applies a binary threshold\n",
    "\n",
    "\n",
    "def build_config(\n",
    "    oem: Optional[int],\n",
    "    psm: Optional[int],\n",
    "    base_flags: Iterable[str],\n",
    "    dpi: Optional[int],\n",
    "    tessdata_dir: Optional[Path],\n",
    "    user_words: Optional[Path],\n",
    "    user_patterns: Optional[Path],\n",
    ") -> str:\n",
    "    parts: List[str] = []\n",
    "    if oem is not None:\n",
    "        parts.append(f\"--oem {oem}\")\n",
    "    if psm is not None:\n",
    "        parts.append(f\"--psm {psm}\")\n",
    "    if dpi is not None:\n",
    "        parts.append(f\"--dpi {dpi}\")\n",
    "    if tessdata_dir is not None:\n",
    "        parts.append(f'--tessdata-dir \"{tessdata_dir}\"')\n",
    "    if user_words is not None:\n",
    "        parts.append(f'--user-words \"{user_words}\"')\n",
    "    if user_patterns is not None:\n",
    "        parts.append(f'--user-patterns \"{user_patterns}\"')\n",
    "    parts.extend(base_flags)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def ensure_environment(lang: str) -> None:\n",
    "    try:\n",
    "        _ = pytesseract.get_tesseract_version()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        sys.exit(\"Tesseract binary not found on PATH. Install it and its language data.\")\n",
    "    if lang:\n",
    "        try:\n",
    "            available = set(pytesseract.get_languages(config=\"\"))\n",
    "            requested = set(lang.split(\"+\"))\n",
    "            missing = requested - available\n",
    "            if missing:\n",
    "                print(\n",
    "                    f\"Warning: missing languages: {', '.join(sorted(missing))}. \"\n",
    "                    f\"Available: {', '.join(sorted(available))}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "        except pytesseract.TesseractError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def auto_rotate_if_needed(img: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    if not enhance.auto_rotate:\n",
    "        return img\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        angle = None\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                try:\n",
    "                    angle = int(line.split(\":\")[1].strip())\n",
    "                except ValueError:\n",
    "                    angle = None\n",
    "                break\n",
    "        if angle is not None and angle % 360 != 0:\n",
    "            return img.rotate(-angle, expand=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    img = image.convert(\"L\")\n",
    "    img = auto_rotate_if_needed(img, enhance)\n",
    "\n",
    "    if enhance.invert:\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    if enhance.pad and enhance.pad > 0:\n",
    "        img = ImageOps.expand(img, border=enhance.pad, fill=255)\n",
    "\n",
    "    if enhance.autocontrast_cutoff is not None:\n",
    "        cutoff = max(0, min(100, enhance.autocontrast_cutoff))\n",
    "        img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "\n",
    "    if enhance.equalize:\n",
    "        img = ImageOps.equalize(img)\n",
    "\n",
    "    if enhance.upscale and enhance.upscale != 1.0:\n",
    "        w, h = img.size\n",
    "        img = img.resize((int(w * enhance.upscale), int(h * enhance.upscale)), Image.LANCZOS)\n",
    "\n",
    "    if enhance.gamma and enhance.gamma > 0:\n",
    "        inv_gamma = 1.0 / enhance.gamma\n",
    "        lut = [pow(x / 255.0, inv_gamma) * 255 for x in range(256)]\n",
    "        img = img.point(lut)\n",
    "\n",
    "    if enhance.brightness and enhance.brightness != 1.0:\n",
    "        img = ImageEnhance.Brightness(img).enhance(enhance.brightness)\n",
    "\n",
    "    if enhance.contrast and enhance.contrast != 1.0:\n",
    "        img = ImageEnhance.Contrast(img).enhance(enhance.contrast)\n",
    "\n",
    "    if enhance.sharpness and enhance.sharpness != 1.0:\n",
    "        img = ImageEnhance.Sharpness(img).enhance(enhance.sharpness)\n",
    "\n",
    "    if enhance.unsharp_radius:\n",
    "        img = img.filter(\n",
    "            ImageFilter.UnsharpMask(\n",
    "                radius=enhance.unsharp_radius,\n",
    "                percent=enhance.unsharp_percent,\n",
    "                threshold=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if enhance.median and enhance.median > 1 and enhance.median % 2 == 1:\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=enhance.median))\n",
    "\n",
    "    if enhance.threshold is not None:\n",
    "        thr = max(0, min(255, enhance.threshold))\n",
    "        img = img.point(lambda p, t=thr: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "    elif enhance.otsu and np is not None:\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        hist, _ = np.histogram(arr, bins=256, range=(0, 256))\n",
    "        total = arr.size\n",
    "        sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "        sum_b = 0.0\n",
    "        w_b = 0.0\n",
    "        max_var = 0.0\n",
    "        threshold = 0\n",
    "\n",
    "        for i in range(256):\n",
    "            w_b += hist[i]\n",
    "            if w_b == 0:\n",
    "                continue\n",
    "            w_f = total - w_b\n",
    "            if w_f == 0:\n",
    "                break\n",
    "            sum_b += i * hist[i]\n",
    "            m_b = sum_b / w_b\n",
    "            m_f = (sum_total - sum_b) / w_f\n",
    "            var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "            if var_between > max_var:\n",
    "                max_var = var_between\n",
    "                threshold = i\n",
    "\n",
    "        img = img.point(lambda p, t=threshold: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-l\", \"--lang\", default=DEFAULT_LANG)\n",
    "    parser.add_argument(\"--oem\", type=int, choices=range(0, 4), default=None)\n",
    "    parser.add_argument(\"--psm\", type=int, choices=range(0, 14), default=None)\n",
    "    parser.add_argument(\"--dpi\", type=int, default=DEFAULT_DPI)\n",
    "    parser.add_argument(\"--tessdata-dir\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-words\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-patterns\", type=Path, default=None)\n",
    "    parser.add_argument(\"--whitelist\", type=str, default=None)\n",
    "    parser.add_argument(\"--blacklist\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--contrast\", type=float, default=DEFAULT_CONTRAST)\n",
    "    parser.add_argument(\"--sharpness\", type=float, default=DEFAULT_SHARPNESS)\n",
    "    parser.add_argument(\"--brightness\", type=float, default=DEFAULT_BRIGHTNESS)\n",
    "    parser.add_argument(\"--upscale\", type=float, default=DEFAULT_UPSCALE)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--pad\", type=int, default=0)\n",
    "    parser.add_argument(\"--threshold\", type=int, default=None)\n",
    "    parser.add_argument(\"--median\", type=int, default=None)\n",
    "    parser.add_argument(\"--unsharp-radius\", type=float, default=None)\n",
    "    parser.add_argument(\"--unsharp-percent\", type=int, default=150)\n",
    "    parser.add_argument(\"--invert\", action=\"store_true\")\n",
    "    parser.add_argument(\"--autocontrast-cutoff\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--auto-rotate\", action=\"store_true\")\n",
    "    parser.add_argument(\"--otsu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"*\",\n",
    "        default=[],\n",
    "        metavar=\"CFG\",\n",
    "        help=\"Additional configuration flags passed verbatim to tesseract (e.g., -c foo=bar).\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args(list(argv) if argv is not None else [])\n",
    "\n",
    "\n",
    "#  Exécution Cellule 1 (jusqu’à l’affichage) \n",
    "\n",
    "args = parse_args()\n",
    "ensure_environment(args.lang)\n",
    "\n",
    "enhance = EnhanceOptions(\n",
    "    contrast=args.contrast,\n",
    "    sharpness=args.sharpness,\n",
    "    brightness=args.brightness,\n",
    "    upscale=args.upscale,\n",
    "    gamma=args.gamma,\n",
    "    pad=args.pad,\n",
    "    median=args.median,\n",
    "    unsharp_radius=args.unsharp_radius,\n",
    "    unsharp_percent=args.unsharp_percent,\n",
    "    invert=args.invert,\n",
    "    autocontrast_cutoff=args.autocontrast_cutoff,\n",
    "    equalize=args.equalize,\n",
    "    auto_rotate=args.auto_rotate,\n",
    "    otsu=args.otsu,\n",
    "    threshold=args.threshold,\n",
    ")\n",
    "\n",
    "config_flags: List[str] = list(args.config)\n",
    "\n",
    "# AJOUTE ÇA :\n",
    "config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "\n",
    "if args.whitelist:\n",
    "    config_flags.append(f\"-c tessedit_char_whitelist={args.whitelist}\")\n",
    "if args.blacklist:\n",
    "    config_flags.append(f\"-c tessedit_char_blacklist={args.blacklist}\")\n",
    "\n",
    "\n",
    "def _normalize_input_files(val):\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        items = list(val)\n",
    "    else:\n",
    "        items = [val]\n",
    "\n",
    "    out = []\n",
    "    for item in items:\n",
    "        if item is None:\n",
    "            continue\n",
    "        if isinstance(item, Path):\n",
    "            out.append(str(item))\n",
    "            continue\n",
    "        s = str(item).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if \",\" in s:\n",
    "            parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "            out.extend(parts)\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# Backwards-compatible alias (older cell name)\n",
    "_normalize_input_file = _normalize_input_files\n",
    "\n",
    "# Safeguard if INPUT_FILE cell not executed yet\n",
    "INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "\n",
    "\n",
    "def _load_images_from_path(path: Path, dpi: int):\n",
    "    if path.suffix.lower() == \".pdf\":\n",
    "        try:\n",
    "            from pdf2image import convert_from_path\n",
    "        except Exception:\n",
    "            sys.exit(\n",
    "                \"pdf2image is not available. Install it and Poppler to read PDF files.\"\n",
    "            )\n",
    "        try:\n",
    "            return convert_from_path(str(path), dpi=dpi)\n",
    "        except Exception as exc:\n",
    "            sys.exit(f\"PDF conversion failed for {path}: {exc}\")\n",
    "    # default: image file (supports multi-page TIFF)\n",
    "    img = Image.open(path)\n",
    "    n_frames = getattr(img, \"n_frames\", 1)\n",
    "    if n_frames and n_frames > 1:\n",
    "        images = []\n",
    "        for i in range(n_frames):\n",
    "            try:\n",
    "                img.seek(i)\n",
    "            except Exception:\n",
    "                break\n",
    "            images.append(img.copy())\n",
    "        return images\n",
    "    return [img]\n",
    "\n",
    "\n",
    "input_items = _normalize_input_files(INPUT_FILE)\n",
    "if not input_items:\n",
    "    print(\"[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\")\n",
    "    DOCS = []\n",
    "else:\n",
    "    DOCS = []\n",
    "    for item in input_items:\n",
    "        path = Path(item)\n",
    "        if not path.is_absolute():\n",
    "            path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "        print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "        dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "        images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "        if len(images) == 1:\n",
    "            original = images[0]\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "        else:\n",
    "            total = len(images)\n",
    "            for idx, original in enumerate(images, start=1):\n",
    "                prepped = preprocess_image(original, enhance)\n",
    "                DOCS.append({\n",
    "                    \"path\": path,\n",
    "                    \"original\": original,\n",
    "                    \"prepped\": prepped,\n",
    "                    \"page_index\": idx,\n",
    "                    \"page_count\": total\n",
    "                })\n",
    "\n",
    "\n",
    "DOCS = []\n",
    "for item in input_items:\n",
    "    path = Path(item)\n",
    "    if not path.is_absolute():\n",
    "        path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "    if not path.exists():\n",
    "        sys.exit(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "    print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "    dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "    images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "    if len(images) == 1:\n",
    "        original = images[0]\n",
    "        prepped = preprocess_image(original, enhance)\n",
    "        DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "    else:\n",
    "        total = len(images)\n",
    "        for idx, original in enumerate(images, start=1):\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\n",
    "                \"path\": path,\n",
    "                \"original\": original,\n",
    "                \"prepped\": prepped,\n",
    "                \"page_index\": idx,\n",
    "                \"page_count\": total\n",
    "            })\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "for doc in DOCS:\n",
    "    original = doc[\"original\"]\n",
    "    prepped = doc[\"prepped\"]\n",
    "    path = doc[\"path\"]\n",
    "\n",
    "    display(original.convert(\"RGB\") if original.mode not in (\"RGB\",\"L\") else original)\n",
    "\n",
    "    if \"SHOW_PREPROCESSED\" not in globals() or SHOW_PREPROCESSED:\n",
    "        display(prepped.convert(\"RGB\") if prepped.mode not in (\"RGB\",\"L\") else prepped)\n",
    "\n",
    "# Keep globals aligned with the last document for backwards compatibility.\n",
    "if DOCS:\n",
    "    path = DOCS[-1][\"path\"]\n",
    "    original = DOCS[-1][\"original\"]\n",
    "    prepped = DOCS[-1][\"prepped\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bb258",
   "metadata": {},
   "source": [
    "### si image passer sur teseract ou document extraire sont contenue apartire de sont contenue code a la fin les deux => input txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0db4b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[doc] francais.docx | content=text | extraction=native:docx:xml | pages=1\n",
      "[page 1/1]\n",
      "Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE:\n",
    "# Cette cellule suppose que la cellule précédente a déjà exécuté:\n",
    "# - la détection/routage (TEXT_FILES / IMAGE_ONLY_FILES / INPUT_FILE)\n",
    "# - le preprocess + affichage (DOCS avec \"prepped\")\n",
    "# Donc ici on fait:\n",
    "# 1) OCR Tesseract UNIQUEMENT sur DOCS (images -> [info])\n",
    "# 2) Extraction NATIVE (sans OCR) sur TEXT_FILES (-> [skip] content='text')\n",
    "#\n",
    "# Objectif print:\n",
    "# - 1 seule fois, à la fin\n",
    "# - affiche: fichier, nb pages, puis texte de chaque page\n",
    "\n",
    "import uuid\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# ==================== Réglage PRINT ====================\n",
    "# False => aucune sortie pendant OCR/native\n",
    "# True  => debug pendant extraction (à éviter si tu veux 1 seul print)\n",
    "PRINT_DURING_EXTRACTION = False\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (flags tesseract pour espaces/tables) --------------------\n",
    "if \"config_flags\" in globals():\n",
    "    if \"-c preserve_interword_spaces=1\" not in config_flags:\n",
    "        config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "    if \"-c textord_tabfind_find_tables=1\" not in config_flags:\n",
    "        config_flags.append(\"-c textord_tabfind_find_tables=1\")\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (reconstruction layout via TSV) --------------------\n",
    "def _median(values):\n",
    "    values = sorted(values)\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    mid = n // 2\n",
    "    if n % 2 == 1:\n",
    "        return values[mid]\n",
    "    return (values[mid - 1] + values[mid]) / 2.0\n",
    "\n",
    "def _estimate_char_metrics(data: dict):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    ws = data.get(\"width\", [])\n",
    "    hs = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        w = int(ws[i]) if i < len(ws) else 0\n",
    "        h = int(hs[i]) if i < len(hs) else 0\n",
    "        if h > 0:\n",
    "            heights.append(h)\n",
    "\n",
    "        L = len(s)\n",
    "        if w > 0 and L > 0:\n",
    "            widths.append(w / float(L))\n",
    "\n",
    "    char_w = _median(widths) or 10.0\n",
    "    line_h = _median(heights) or 20.0\n",
    "\n",
    "    if char_w <= 1:\n",
    "        char_w = 10.0\n",
    "    if line_h <= 1:\n",
    "        line_h = 20.0\n",
    "\n",
    "    return float(char_w), float(line_h)\n",
    "\n",
    "def _render_layout_from_data(data: dict, img_w: int, img_h: int) -> str:\n",
    "    char_w, line_h = _estimate_char_metrics(data)\n",
    "    line_tol = max(6.0, line_h * 0.55)\n",
    "\n",
    "    items = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    lefts = data.get(\"left\", [])\n",
    "    tops = data.get(\"top\", [])\n",
    "    widths = data.get(\"width\", [])\n",
    "    heights = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        l = int(lefts[i]) if i < len(lefts) else 0\n",
    "        tp = int(tops[i]) if i < len(tops) else 0\n",
    "        w = int(widths[i]) if i < len(widths) else 0\n",
    "        h = int(heights[i]) if i < len(heights) else 0\n",
    "\n",
    "        items.append({\"text\": s, \"left\": l, \"top\": tp, \"right\": l + w, \"height\": h})\n",
    "\n",
    "    items.sort(key=lambda x: (x[\"top\"], x[\"left\"]))\n",
    "\n",
    "    lines = []\n",
    "    for it in items:\n",
    "        placed = False\n",
    "        if lines and abs(it[\"top\"] - lines[-1][\"top\"]) <= line_tol:\n",
    "            lines[-1][\"words\"].append(it)\n",
    "            lines[-1][\"top\"] = min(lines[-1][\"top\"], it[\"top\"])\n",
    "            placed = True\n",
    "        if not placed:\n",
    "            for ln in reversed(lines):\n",
    "                if abs(it[\"top\"] - ln[\"top\"]) <= line_tol:\n",
    "                    ln[\"words\"].append(it)\n",
    "                    ln[\"top\"] = min(ln[\"top\"], it[\"top\"])\n",
    "                    placed = True\n",
    "                    break\n",
    "        if not placed:\n",
    "            lines.append({\"top\": it[\"top\"], \"words\": [it]})\n",
    "\n",
    "    lines.sort(key=lambda ln: ln[\"top\"])\n",
    "\n",
    "    out_lines = []\n",
    "    prev_row = None\n",
    "\n",
    "    for ln in lines:\n",
    "        words = sorted(ln[\"words\"], key=lambda x: x[\"left\"])\n",
    "        row = int(round(ln[\"top\"] / line_h)) if line_h > 0 else 0\n",
    "        if prev_row is not None:\n",
    "            gap = row - prev_row\n",
    "            if gap > 1:\n",
    "                for _ in range(gap - 1):\n",
    "                    out_lines.append(\"\")\n",
    "        prev_row = row\n",
    "\n",
    "        line_str = \"\"\n",
    "        cursor = 0\n",
    "        for w in words:\n",
    "            col = int(round(w[\"left\"] / char_w)) if char_w > 0 else 0\n",
    "            if col < 0:\n",
    "                col = 0\n",
    "\n",
    "            if cursor == 0 and not line_str:\n",
    "                if col > 0:\n",
    "                    line_str += \" \" * col\n",
    "                    cursor = col\n",
    "            else:\n",
    "                needed = col - cursor\n",
    "                if needed <= 0:\n",
    "                    needed = 1\n",
    "                line_str += \" \" * needed\n",
    "                cursor += needed\n",
    "\n",
    "            line_str += w[\"text\"]\n",
    "            cursor += len(w[\"text\"])\n",
    "\n",
    "        out_lines.append(line_str)\n",
    "\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "# -------------------- OCR --------------------\n",
    "config = build_config(\n",
    "    args.oem,\n",
    "    args.psm,\n",
    "    config_flags,\n",
    "    args.dpi,\n",
    "    args.tessdata_dir,\n",
    "    args.user_words,\n",
    "    args.user_patterns,\n",
    ")\n",
    "\n",
    "if \"DOCS\" not in globals():\n",
    "    DOCS = []\n",
    "\n",
    "def _basename(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return Path(val).name\n",
    "    except Exception:\n",
    "        s = str(val)\n",
    "        return s.replace(\"\\\\\", \"/\").split(\"/\")[-1]\n",
    "\n",
    "# If DOCS is a list of pages (legacy), group into document-level objects\n",
    "if DOCS and isinstance(DOCS[0], dict) and \"pages\" not in DOCS[0]:\n",
    "    groups = {}\n",
    "    for i, page in enumerate(DOCS, start=1):\n",
    "        raw = str(page.get(\"path\") or \"batch\")\n",
    "        key = f\"{raw}::p{page.get('page_index') or i}\"\n",
    "        groups.setdefault(key, []).append(page)\n",
    "\n",
    "    packed = []\n",
    "    for key, pages in groups.items():\n",
    "        pages_sorted = sorted(pages, key=lambda p: int(p.get(\"page_index\") or 0)) if pages else []\n",
    "\n",
    "        source_files = [_basename(p.get(\"path\")) for p in pages_sorted if _basename(p.get(\"path\"))]\n",
    "        source_files = list(dict.fromkeys(source_files))\n",
    "\n",
    "        filename = source_files[0] if len(source_files) == 1 else (_basename(key) or \"batch\")\n",
    "\n",
    "        doc = {\"doc_id\": str(uuid.uuid4()), \"filename\": filename, \"source_files\": source_files, \"pages\": []}\n",
    "        page_index = 1\n",
    "        for p in pages_sorted:\n",
    "            idx = int(p.get(\"page_index\") or page_index)\n",
    "            src_path = p.get(\"path\")\n",
    "            doc[\"pages\"].append({\n",
    "                \"page_index\": idx,\n",
    "                \"image\": p.get(\"original\"),\n",
    "                \"prepped\": p.get(\"prepped\"),\n",
    "                \"source_path\": src_path,\n",
    "                \"source_file\": _basename(src_path)\n",
    "            })\n",
    "            page_index += 1\n",
    "        doc[\"page_count_total\"] = len(doc[\"pages\"])\n",
    "        packed.append(doc)\n",
    "\n",
    "    DOCS = packed\n",
    "\n",
    "# Ensure doc-level metadata consistency (even if DOCS already has pages)\n",
    "for doc in DOCS:\n",
    "    pages = doc.get(\"pages\", []) or []\n",
    "    for i, page in enumerate(pages, start=1):\n",
    "        if not page.get(\"page_index\"):\n",
    "            page[\"page_index\"] = i\n",
    "        if not page.get(\"source_file\"):\n",
    "            src_path = page.get(\"source_path\") or page.get(\"path\")\n",
    "            page[\"source_file\"] = _basename(src_path)\n",
    "\n",
    "    doc[\"page_count_total\"] = len(pages)\n",
    "\n",
    "    if not doc.get(\"source_files\"):\n",
    "        source_files = [p.get(\"source_file\") for p in pages if p.get(\"source_file\")]\n",
    "        doc[\"source_files\"] = list(dict.fromkeys(source_files))\n",
    "\n",
    "    if not doc.get(\"filename\"):\n",
    "        if len(doc.get(\"source_files\", [])) == 1:\n",
    "            doc[\"filename\"] = doc[\"source_files\"][0]\n",
    "        elif len(doc.get(\"source_files\", [])) > 1:\n",
    "            doc[\"filename\"] = \"batch\"\n",
    "\n",
    "for doc in DOCS:\n",
    "    pages_text = []\n",
    "    for page in doc.get(\"pages\", []):\n",
    "        prepped = page.get(\"prepped\")\n",
    "        if prepped is None:\n",
    "            raise RuntimeError(\"prepped image missing. Run the input/preprocess cell first.\")\n",
    "\n",
    "        data = pytesseract.image_to_data(prepped, lang=args.lang, config=config, output_type=Output.DICT)\n",
    "        w, h = prepped.size\n",
    "        OCR_TEXT = _render_layout_from_data(data, w, h)\n",
    "\n",
    "        page[\"ocr_text\"] = OCR_TEXT\n",
    "        pages_text.append(OCR_TEXT)\n",
    "\n",
    "        if PRINT_DURING_EXTRACTION:\n",
    "            src = page.get(\"source_file\") or _basename(page.get(\"source_path\")) or \"\"\n",
    "            total = doc.get(\"page_count_total\", 1)\n",
    "            print(f\"[ocr] {doc.get('filename')} | file={src} | page {page.get('page_index')}/{total}\")\n",
    "            print(OCR_TEXT)\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "    doc[\"pages_text\"] = pages_text\n",
    "    doc[\"ocr_text\"] = \"\\n\\n\".join(pages_text)\n",
    "\n",
    "# Backwards compatibility\n",
    "if DOCS:\n",
    "    OCR_TEXT = DOCS[-1].get(\"ocr_text\", \"\")\n",
    "\n",
    "# -------------------- EXTRACTION NATIVE POUR [skip] (TEXT_FILES) --------------------\n",
    "def _get_pdf_reader_with_name():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader, \"pypdf\"\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader, \"PyPDF2\"\n",
    "        except ImportError:\n",
    "            return None, \"none\"\n",
    "\n",
    "# AJOUT MINIMAL: tenter un mode \"layout\" si dispo, sinon fallback normal\n",
    "def _pdf_extract_text_preserve_layout(page) -> str:\n",
    "    try:\n",
    "        return page.extract_text(extraction_mode=\"layout\") or \"\"\n",
    "    except TypeError:\n",
    "        return page.extract_text() or \"\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            return page.extract_text() or \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def _docx_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.findall(\".//w:p\", ns):\n",
    "        line_parts = []\n",
    "        for node in p.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                line_parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}tab\"):\n",
    "                line_parts.append(\"\\t\")\n",
    "            elif tag.endswith(\"}br\") or tag.endswith(\"}cr\"):\n",
    "                line_parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(line_parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _pptx_slide_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\n",
    "        \"a\": \"http://schemas.openxmlformats.org/drawingml/2006/main\",\n",
    "        \"p\": \"http://schemas.openxmlformats.org/presentationml/2006/main\",\n",
    "    }\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for para in root.findall(\".//a:p\", ns):\n",
    "        parts = []\n",
    "        for node in para.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}br\"):\n",
    "                parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _xlsx_col_to_index(col_letters: str) -> int:\n",
    "    n = 0\n",
    "    for ch in col_letters:\n",
    "        if \"A\" <= ch <= \"Z\":\n",
    "            n = n * 26 + (ord(ch) - ord(\"A\") + 1)\n",
    "    return n\n",
    "\n",
    "def _xlsx_shared_strings(xml_bytes: bytes) -> list:\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    out = []\n",
    "    for si in root.findall(\".//s:si\", ns):\n",
    "        parts = []\n",
    "        for t in si.findall(\".//s:t\", ns):\n",
    "            parts.append(t.text if t.text is not None else \"\")\n",
    "        out.append(\"\".join(parts))\n",
    "    return out\n",
    "\n",
    "def _xlsx_sheet_to_text(sheet_xml: bytes, shared: list) -> str:\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    root = ET.fromstring(sheet_xml)\n",
    "\n",
    "    lines = []\n",
    "    for row in root.findall(\".//s:row\", ns):\n",
    "        cells = row.findall(\"./s:c\", ns)\n",
    "        row_map = {}\n",
    "        max_col = 0\n",
    "\n",
    "        for c in cells:\n",
    "            r = c.get(\"r\") or \"\"\n",
    "            col_letters = \"\".join([ch for ch in r if ch.isalpha()]).upper()\n",
    "            col_idx = _xlsx_col_to_index(col_letters) if col_letters else 0\n",
    "            if col_idx > max_col:\n",
    "                max_col = col_idx\n",
    "\n",
    "            cell_type = c.get(\"t\")\n",
    "            v = c.find(\"./s:v\", ns)\n",
    "            is_node = c.find(\"./s:is\", ns)\n",
    "\n",
    "            val = \"\"\n",
    "            if cell_type == \"s\" and v is not None and v.text is not None:\n",
    "                try:\n",
    "                    val = shared[int(v.text)]\n",
    "                except Exception:\n",
    "                    val = v.text\n",
    "            elif cell_type == \"inlineStr\" and is_node is not None:\n",
    "                parts = []\n",
    "                for t in is_node.findall(\".//s:t\", ns):\n",
    "                    parts.append(t.text if t.text is not None else \"\")\n",
    "                val = \"\".join(parts)\n",
    "            else:\n",
    "                if v is not None and v.text is not None:\n",
    "                    val = v.text\n",
    "\n",
    "            row_map[col_idx] = val\n",
    "\n",
    "        if max_col <= 0:\n",
    "            lines.append(\"\")\n",
    "        else:\n",
    "            parts = []\n",
    "            for i in range(1, max_col + 1):\n",
    "                parts.append(row_map.get(i, \"\"))\n",
    "            lines.append(\"\\t\".join(parts))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _odf_content_to_text(xml_bytes: bytes) -> str:\n",
    "    ns_text = \"urn:oasis:names:tc:opendocument:xmlns:text:1.0\"\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    def walk(node):\n",
    "        pieces = []\n",
    "        if node.text is not None:\n",
    "            pieces.append(node.text)\n",
    "\n",
    "        for child in list(node):\n",
    "            tag = child.tag\n",
    "            if tag == f\"{{{ns_text}}}s\":\n",
    "                c = child.get(f\"{{{ns_text}}}c\") or child.get(\"c\") or \"1\"\n",
    "                try:\n",
    "                    pieces.append(\" \" * int(c))\n",
    "                except Exception:\n",
    "                    pieces.append(\" \")\n",
    "            else:\n",
    "                pieces.append(walk(child))\n",
    "\n",
    "            if child.tail is not None:\n",
    "                pieces.append(child.tail)\n",
    "        return \"\".join(pieces)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.iter():\n",
    "        if p.tag == f\"{{{ns_text}}}p\":\n",
    "            out_lines.append(walk(p))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _html_bytes_to_text_preserve(b: bytes) -> str:\n",
    "    b = re.sub(rb\"(?i)<br\\s*/?>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"(?i)</p\\s*>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "    try:\n",
    "        return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return str(b)\n",
    "\n",
    "def extract_text_native(path: str) -> dict:\n",
    "    ft = detect_path_type(path)  # défini dans cellule précédente\n",
    "    ext = ft.ext.lower()\n",
    "    filename = Path(path).name\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        PdfReader, backend = _get_pdf_reader_with_name()\n",
    "        if PdfReader is not None:\n",
    "            reader = PdfReader(path)\n",
    "            pages = reader.pages\n",
    "            pages_text = []\n",
    "            total = len(pages)\n",
    "\n",
    "            for i, page in enumerate(pages, start=1):\n",
    "                # MODIF MINIMALE: extraction \"layout\" si dispo => garde mieux espaces/sauts de ligne\n",
    "                txt = _pdf_extract_text_preserve_layout(page)\n",
    "                pages_text.append(txt)\n",
    "                if PRINT_DURING_EXTRACTION:\n",
    "                    print(f\"[native:{backend}] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": f\"native:pdf:{backend}\",\n",
    "                \"text\": full,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "\n",
    "        # Fallback pdfminer\n",
    "        try:\n",
    "            from pdfminer.high_level import extract_text  # type: ignore\n",
    "            full = extract_text(path) or \"\"\n",
    "            pages = full.split(\"\\f\")\n",
    "            pages_text = [p for p in pages]  # garder brut\n",
    "            total = len(pages_text)\n",
    "\n",
    "            if PRINT_DURING_EXTRACTION:\n",
    "                for i, txt in enumerate(pages_text, start=1):\n",
    "                    print(f\"[native:pdfminer] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full2 = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:pdfminer\",\n",
    "                \"text\": full2,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:none\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "            }\n",
    "\n",
    "    # Office/OpenDocument/EPUB\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = z.namelist()\n",
    "\n",
    "                if ext == \".docx\":\n",
    "                    parts = []\n",
    "                    if \"word/document.xml\" in names:\n",
    "                        parts.append(_docx_xml_to_text(z.read(\"word/document.xml\")))\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                        if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(parts)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:docx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],      # docx: pas de \"pages\" fiables => 1 bloc\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".xlsx\":\n",
    "                    shared = []\n",
    "                    if \"xl/sharedStrings.xml\" in names:\n",
    "                        try:\n",
    "                            shared = _xlsx_shared_strings(z.read(\"xl/sharedStrings.xml\"))\n",
    "                        except Exception:\n",
    "                            shared = []\n",
    "\n",
    "                    sheet_files = [nm for nm in names if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\")]\n",
    "                    sheet_files_sorted = sorted(sheet_files)\n",
    "\n",
    "                    pages_text = []\n",
    "                    total = len(sheet_files_sorted)\n",
    "                    for nm in sheet_files_sorted:\n",
    "                        sheet_text = _xlsx_sheet_to_text(z.read(nm), shared)\n",
    "                        pages_text.append(sheet_text)\n",
    "\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:xlsx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # sheets = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext == \".pptx\":\n",
    "                    slides = [nm for nm in names if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\")]\n",
    "                    slides_sorted = sorted(slides)\n",
    "                    pages_text = []\n",
    "                    total = len(slides_sorted)\n",
    "                    for nm in slides_sorted:\n",
    "                        pages_text.append(_pptx_slide_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:pptx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # slides = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                    text = \"\"\n",
    "                    if \"content.xml\" in names:\n",
    "                        text = _odf_content_to_text(z.read(\"content.xml\"))\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": f\"native:{ext[1:]}:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".epub\":\n",
    "                    htmls = [nm for nm in names if nm.lower().endswith((\".xhtml\", \".html\", \".htm\"))]\n",
    "                    htmls_sorted = sorted(htmls)\n",
    "                    pages_text = []\n",
    "                    total = len(htmls_sorted)\n",
    "                    for nm in htmls_sorted:\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            b = b\"\"\n",
    "                        pages_text.append(_html_bytes_to_text_preserve(b))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:epub:html\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:zip:error\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": str(uuid.uuid4()),\n",
    "        \"filename\": filename,\n",
    "        \"source_path\": path,\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": \"native:unsupported\",\n",
    "        \"text\": \"\",\n",
    "        \"pages_text\": [\"\"],\n",
    "        \"page_count_total\": 1,\n",
    "    }\n",
    "\n",
    "# TEXT_FILES vient de la cellule précédente (celle qui a fait les [skip])\n",
    "TEXT_DOCS: List[dict] = []\n",
    "if \"TEXT_FILES\" not in globals():\n",
    "    TEXT_FILES = []\n",
    "\n",
    "for p in TEXT_FILES:\n",
    "    try:\n",
    "        TEXT_DOCS.append(extract_text_native(p))\n",
    "    except Exception as e:\n",
    "        TEXT_DOCS.append({\n",
    "            \"doc_id\": str(uuid.uuid4()),\n",
    "            \"filename\": Path(p).name,\n",
    "            \"source_path\": p,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": \"native:error\",\n",
    "            \"text\": \"\",\n",
    "            \"pages_text\": [\"\"],\n",
    "            \"page_count_total\": 1,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "# -------------------- SORTIE FINALE (OCR + NATIVE) --------------------\n",
    "FINAL_DOCS: List[dict] = []\n",
    "\n",
    "# OCR docs (images)\n",
    "for d in DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"image_only\",\n",
    "        \"extraction\": \"ocr:tesseract\",\n",
    "        \"text\": d.get(\"ocr_text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "# Native docs (text)\n",
    "for d in TEXT_DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or [d.get(\"text\") or \"\"]\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": d.get(\"extraction\"),\n",
    "        \"text\": d.get(\"text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "for d in FINAL_DOCS:\n",
    "    filename = d.get(\"filename\")\n",
    "    content = d.get(\"content\")\n",
    "    extraction = d.get(\"extraction\")\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    total = int(d.get(\"page_count_total\") or len(pages_text) or 1)\n",
    "\n",
    "    print(f\"[doc] {filename} | content={content} | extraction={extraction} | pages={total}\")\n",
    "\n",
    "    if not pages_text:\n",
    "        print(\"\")\n",
    "        print(\"\\n\" + (\"-\" * 120) + \"\\n\")\n",
    "        continue\n",
    "\n",
    "    for i, txt in enumerate(pages_text, start=1):\n",
    "        print(f\"[page {i}/{total}]\")\n",
    "        print(txt if txt is not None else \"\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097c827",
   "metadata": {},
   "source": [
    "### Tokenisation \"layout\" (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c54db6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "[doc] francais.docx\n",
      "  doc_id       : 96d0060a-70ad-4515-90c7-ed5efebc86d4\n",
      "  content      : text\n",
      "  extraction   : native:docx:xml\n",
      "  pages_total  : 1\n",
      "  chars_total  : 1706\n",
      "  recompose_ok : False\n",
      "  paths:\n",
      "    - c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[page 1/1] source_path=c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\francais.docx\n",
      "  lang         : fr\n",
      "  chars        : 1706\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  sentences_layout: 1 chunks total | sentences=1 | noise=0 | showing 1/1 (filter_is_sentence=True, fallback=False, min_nonspace=12)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[sent 1/1] page=1 start=0 end=1706 line=1 col=0 chars=1706 nonspace=1442 is_noise=False is_sentence=True layout=table\n",
      "Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# ==================== Réglages ====================\n",
    "TARGET = None\n",
    "\n",
    "PRINT_SENTENCES = True\n",
    "MAX_SENTENCES_PREVIEW = 80   # None => imprime tout\n",
    "PRINT_REPR = False           # True => debug espaces invisibles via repr(chunk)\n",
    "\n",
    "MIN_SENTENCE_NONSPACE = 12\n",
    "PRINT_ONLY_SENTENCES = True\n",
    "PRINT_PAGE_TEXT = False\n",
    "\n",
    "# ==================== NLTK data ====================\n",
    "def _ensure_nltk():\n",
    "    for pkg, probe in ((\"punkt\", \"tokenizers/punkt\"), (\"punkt_tab\", \"tokenizers/punkt_tab\")):\n",
    "        try:\n",
    "            nltk.data.find(probe)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] NLTK download failed for {pkg}: {e}\")\n",
    "\n",
    "_ensure_nltk()\n",
    "\n",
    "# ==================== Détection langue (simple) ====================\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "# ==================== Sentence split \"layout\" (fallback) ====================\n",
    "_AR_END_RE = re.compile(r\"([.!?؟]+)(\\s+|$)\", flags=re.UNICODE)\n",
    "\n",
    "def split_ar_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    last = 0\n",
    "    for m in _AR_END_RE.finditer(text):\n",
    "        end = m.end()\n",
    "        chunks.append(text[last:end])\n",
    "        last = end\n",
    "    if last < len(text):\n",
    "        chunks.append(text[last:])\n",
    "    return chunks\n",
    "\n",
    "def _load_punkt_pickle(lang_pickle_name: str):\n",
    "    p = nltk.data.find(f\"tokenizers/punkt/{lang_pickle_name}.pickle\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def split_punkt_layout(text: str, lang_pickle_name: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    tok = _load_punkt_pickle(lang_pickle_name)\n",
    "    spans = list(tok.span_tokenize(text))\n",
    "    if not spans:\n",
    "        return [text]\n",
    "    starts = [0] + [spans[i][0] for i in range(1, len(spans))]\n",
    "    ends = [spans[i+1][0] for i in range(len(spans)-1)] + [len(text)]\n",
    "    return [text[starts[i]:ends[i]] for i in range(len(ends))]\n",
    "\n",
    "def sentence_chunks_layout(text: str, lang: str):\n",
    "    lang = (lang or \"\").lower()\n",
    "    if lang.startswith(\"ar\"):\n",
    "        return split_ar_layout(text)\n",
    "    if lang.startswith(\"fr\"):\n",
    "        return split_punkt_layout(text, \"french\")\n",
    "    if lang.startswith(\"en\"):\n",
    "        return split_punkt_layout(text, \"english\")\n",
    "    return split_punkt_layout(text, \"english\")\n",
    "\n",
    "# ==================== Split sections/alinéas (layout-preserving) ====================\n",
    "def _iter_line_spans(text: str):\n",
    "    if not text:\n",
    "        return\n",
    "    start = 0\n",
    "    for m in re.finditer(r\"\\n\", text):\n",
    "        end = m.end()\n",
    "        yield start, end\n",
    "        start = end\n",
    "    if start < len(text):\n",
    "        yield start, len(text)\n",
    "\n",
    "def _collapse_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def _mask_digits(s: str) -> str:\n",
    "    return re.sub(r\"\\d\", \"#\", s)\n",
    "\n",
    "_NUM_SIMPLE_RE = re.compile(r\"(?i)^[ \\t]*\\(?\\d{1,3}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ALPHA_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[a-z]\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ROMAN_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[ivxlcdm]{1,8}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_NUM_MULTI_RE  = re.compile(r\"^[ \\t]*\\d{1,3}(?:\\.\\d{1,3})+[ \\t]*(?:[.)])?[ \\t]+(?=\\S)\")\n",
    "\n",
    "_KEYWORD_STRONG_RE = re.compile(r\"(?i)^[ \\t]*(article|section|chapitre|chapter|part)\\b\")\n",
    "_KEYWORD_WEAK_HEADING_RE = re.compile(\n",
    "    r\"\"\"(?ix)^[ \\t]*\n",
    "    (schedule|exhibit|appendix|annexe|annex)\n",
    "    [ \\t]+\n",
    "    ([A-Z0-9]{1,8}|[ivxlcdm]{1,8}|\\d{1,3})\n",
    "    [ \\t]*\n",
    "    (?:[:\\-–—][ \\t]*\\S.*)?\n",
    "    [ \\t]*$\n",
    "    \"\"\"\n",
    ")\n",
    "_SEP_RE = re.compile(r\"^[ \\t]*[-_]{4,}[ \\t]*$\")\n",
    "\n",
    "_LABEL_ONLY_RE = re.compile(\n",
    "    r\"(?is)^[ \\t]*\"\n",
    "    r\"(?:\\(?\\d{1,3}\\)?|\\(?[a-z]\\)?|\\(?[ivxlcdm]{1,8}\\)?)\"\n",
    "    r\"[ \\t]*[.)][ \\t]*$\"\n",
    ")\n",
    "\n",
    "def _is_section_start_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _KEYWORD_STRONG_RE.match(s):\n",
    "        return True\n",
    "    if _KEYWORD_WEAK_HEADING_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_SIMPLE_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_MULTI_RE.match(s):\n",
    "        label = _collapse_ws(s).split(\" \", 1)[0]\n",
    "        parts = label.split(\".\")\n",
    "        if len(parts) >= 2 and parts[-1] in (\"00\", \"000\"):\n",
    "            return False\n",
    "        return True\n",
    "    if _ALPHA_RE.match(s) or _ROMAN_RE.match(s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _merge_label_only(chunks):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        if i + 1 < len(chunks) and _LABEL_ONLY_RE.match(chunks[i]):\n",
    "            out.append(chunks[i] + chunks[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(chunks[i])\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def split_sections_layout(text: str, allow_alpha_roman: bool = True):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = {0}\n",
    "    for ls, le in _iter_line_spans(text):\n",
    "        line = text[ls:le]\n",
    "        if _is_section_start_line(line):\n",
    "            if not allow_alpha_roman:\n",
    "                s = line.rstrip(\"\\n\")\n",
    "                if (\n",
    "                    _KEYWORD_STRONG_RE.match(s)\n",
    "                    or _KEYWORD_WEAK_HEADING_RE.match(s)\n",
    "                    or _NUM_SIMPLE_RE.match(s)\n",
    "                    or _NUM_MULTI_RE.match(s)\n",
    "                ):\n",
    "                    starts.add(ls)\n",
    "            else:\n",
    "                starts.add(ls)\n",
    "\n",
    "    starts = sorted(starts)\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        a, b = starts[i], starts[i+1]\n",
    "        if a != b:\n",
    "            chunks.append(text[a:b])\n",
    "    chunks.append(text[starts[-1]:])\n",
    "\n",
    "    return _merge_label_only(chunks)\n",
    "\n",
    "_PARA_BREAK_RE = re.compile(r\"(?:\\n[ \\t]*){2,}\")\n",
    "\n",
    "def split_paragraphs_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = [0]\n",
    "    for m in _PARA_BREAK_RE.finditer(text):\n",
    "        starts.append(m.end())\n",
    "    starts = sorted(set(starts))\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "    out = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        out.append(text[starts[i]:starts[i+1]])\n",
    "    out.append(text[starts[-1]:])\n",
    "    return out\n",
    "\n",
    "def chunk_layout_universal(text: str, lang: str):\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    lines = [text[ls:le].rstrip(\"\\n\") for ls, le in _iter_line_spans(text)]\n",
    "    num_kw_hits = 0\n",
    "    alpha_roman_hits = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        if (\n",
    "            _KEYWORD_STRONG_RE.match(ln)\n",
    "            or _KEYWORD_WEAK_HEADING_RE.match(ln)\n",
    "            or _NUM_SIMPLE_RE.match(ln)\n",
    "            or _NUM_MULTI_RE.match(ln)\n",
    "        ):\n",
    "            num_kw_hits += 1\n",
    "        elif _ALPHA_RE.match(ln) or _ROMAN_RE.match(ln):\n",
    "            alpha_roman_hits += 1\n",
    "\n",
    "    is_structured = (num_kw_hits >= 2) or (alpha_roman_hits >= 3)\n",
    "\n",
    "    if is_structured:\n",
    "        chunks = split_sections_layout(text, allow_alpha_roman=True)\n",
    "        if len(chunks) > 1:\n",
    "            return chunks\n",
    "\n",
    "    paras = split_paragraphs_layout(text)\n",
    "    if len(paras) > 1:\n",
    "        return paras\n",
    "\n",
    "    return sentence_chunks_layout(text, lang)\n",
    "\n",
    "# ======================================================================\n",
    "#  MULTI-COLONNES (général, robuste) + TABLE (inchangé)\n",
    "#  + micro-table: interpréter les headers multi-col comme un \"table chunk\"\n",
    "# ======================================================================\n",
    "\n",
    "GAP_MIN_OCR = 10\n",
    "GAP_MIN_NATIVE = 6\n",
    "\n",
    "MERGE_COL_DIST_OCR = 22\n",
    "MERGE_COL_DIST_NATIVE = 16\n",
    "\n",
    "MICROTABLE_MAX_ROWS = 30\n",
    "MICROTABLE_MIN_DENS = 0.25\n",
    "MICROTABLE_MIN_MULTIROW = 2\n",
    "\n",
    "TABLE_HINT_RE = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    \\b(\n",
    "        qt[ée]|\n",
    "        désignation|designation|\n",
    "        prix|\n",
    "        montan?t|\n",
    "        r[ée]f[ée]rence|reference|\n",
    "        description|\n",
    "        quantit[ée]|\n",
    "        p\\.?\\s*unitaire|\n",
    "        valeur|\n",
    "        total\\s*ht|total|\n",
    "        tva|vat\n",
    "    )\\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d+(?:[.,]\\d+)?\")\n",
    "DEC_RE = re.compile(r\"\\d+[.,]\\d+\")\n",
    "\n",
    "def _space_runs_ge(s: str, n: int):\n",
    "    return [(m.start(), m.end()) for m in re.finditer(r\"[ ]{%d,}\" % n, s or \"\")]\n",
    "\n",
    "def _has_big_gap(s: str, gap_min: int, min_count: int = 1) -> bool:\n",
    "    return len(_space_runs_ge(s, gap_min)) >= min_count\n",
    "\n",
    "def _num_tokens(s: str) -> int:\n",
    "    return len(NUM_RE.findall(s or \"\"))\n",
    "\n",
    "def _dec_tokens(s: str) -> int:\n",
    "    return len(DEC_RE.findall(s or \"\"))\n",
    "\n",
    "def _is_table_line(line: str, gap_min: int) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return False\n",
    "    if s.count(\"\\t\") >= 2:\n",
    "        return True\n",
    "    if TABLE_HINT_RE.search(s):\n",
    "        return True\n",
    "    if _has_big_gap(s, gap_min, min_count=2):\n",
    "        if _num_tokens(s) >= 3:\n",
    "            return True\n",
    "        if _dec_tokens(s) >= 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _cluster_centers(values, tol=2, min_hits=1):\n",
    "    if not values:\n",
    "        return []\n",
    "    xs = sorted(values)\n",
    "    clusters = []\n",
    "    cur = [xs[0]]\n",
    "    for v in xs[1:]:\n",
    "        if abs(v - cur[-1]) <= tol:\n",
    "            cur.append(v)\n",
    "        else:\n",
    "            clusters.append(cur)\n",
    "            cur = [v]\n",
    "    clusters.append(cur)\n",
    "\n",
    "    centers = []\n",
    "    for c in clusters:\n",
    "        if len(c) >= min_hits:\n",
    "            c2 = sorted(c)\n",
    "            centers.append(c2[len(c2)//2])\n",
    "    return sorted(set(centers))\n",
    "\n",
    "def _upper_ratio(s: str) -> float:\n",
    "    letters = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s or \"\")\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for ch in letters if ch.isupper())\n",
    "    return upp / max(1, len(letters))\n",
    "\n",
    "def _sep_spans(line: str, gap_min: int):\n",
    "    s = line or \"\"\n",
    "    spans = []\n",
    "    for m in re.finditer(r\"\\t+\", s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    for m in re.finditer(r\"[ ]{%d,}\" % gap_min, s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    if not spans:\n",
    "        return []\n",
    "    spans.sort()\n",
    "    merged = [spans[0]]\n",
    "    for a, b in spans[1:]:\n",
    "        la, lb = merged[-1]\n",
    "        if a <= lb:\n",
    "            merged[-1] = (la, max(lb, b))\n",
    "        else:\n",
    "            merged.append((a, b))\n",
    "    return merged\n",
    "\n",
    "def _line_segments_by_gaps(line: str, gap_min: int):\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return []\n",
    "    seps = _sep_spans(s, gap_min)\n",
    "    segs = []\n",
    "    prev = 0\n",
    "    cuts = seps + [(len(s), len(s))]\n",
    "    for a, b in cuts:\n",
    "        if a < prev:\n",
    "            continue\n",
    "        chunk = s[prev:a]\n",
    "        m1 = re.search(r\"\\S\", chunk)\n",
    "        if m1:\n",
    "            l = m1.start()\n",
    "            r = len(chunk.rstrip(\" \\t\"))\n",
    "            text = chunk[l:r]\n",
    "            segs.append({\"x\": prev + l, \"a\": prev + l, \"b\": prev + r, \"text\": text})\n",
    "        prev = b\n",
    "    return segs\n",
    "\n",
    "def _looks_like_title_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\").strip()\n",
    "    if not s or len(s) > 50:\n",
    "        return False\n",
    "    if _SEP_RE.match(s):\n",
    "        return False\n",
    "    if _is_section_start_line(line):\n",
    "        return True\n",
    "    if _upper_ratio(s) >= 0.85 and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s) and not re.search(r\"\\d\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_multicol_candidate_line(line: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _is_table_line(line, gap_min):\n",
    "        return False\n",
    "\n",
    "    segs = _line_segments_by_gaps(s, gap_min)\n",
    "    if len(segs) >= 3:\n",
    "        return True\n",
    "    if len(segs) == 2:\n",
    "        if is_ocr:\n",
    "            return True\n",
    "        if _has_big_gap(s, gap_min, 1):\n",
    "            return True\n",
    "        if re.search(r\"[:#№°/\\\\\\-–—]\", s) or re.search(r\"\\d\", s):\n",
    "            return True\n",
    "        if _upper_ratio(s) >= 0.70:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "_KV_GENERIC_RE = re.compile(r\"^\\s*(?P<k>[^:]{1,80}?)\\s{2,}(?P<v>\\S.+?)\\s*$\")\n",
    "\n",
    "def _looks_like_header_pair(k: str, v: str) -> bool:\n",
    "    k2 = (k or \"\").strip()\n",
    "    v2 = (v or \"\").strip()\n",
    "    if not k2 or not v2:\n",
    "        return False\n",
    "    if len(k2) <= 25 and len(v2) <= 25 and _upper_ratio(k2) >= 0.85 and _upper_ratio(v2) >= 0.85:\n",
    "        if not re.search(r\"\\d\", k2 + v2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _looks_like_addressish(line: str) -> bool:\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if re.search(r\"(rue|route|avenue|bd|boulevard|street|st\\.|road|zip|code\\s*postal|bp)\", s, flags=re.I):\n",
    "        return True\n",
    "    if len(s) >= 10 and not s.endswith(\":\") and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _normalize_kv_generic(text: str) -> str:\n",
    "    out = []\n",
    "    for raw in (text or \"\").splitlines():\n",
    "        line = raw.rstrip(\"\\n\")\n",
    "        if not line.strip():\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if \":\" in line:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if _looks_like_addressish(line):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        m = _KV_GENERIC_RE.match(line)\n",
    "        if not m:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        k = _collapse_ws(m.group(\"k\"))\n",
    "        v = _collapse_ws(m.group(\"v\"))\n",
    "        if _looks_like_header_pair(k, v):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if not re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", k):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        out.append(f\"{k}: {v}\" if v else k)\n",
    "    return \"\\n\".join(out) + (\"\\n\" if (text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _strip_sep_lines(block_text: str) -> str:\n",
    "    if not block_text:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for ln in (block_text or \"\").splitlines():\n",
    "        if _SEP_RE.match(ln.strip()):\n",
    "            continue\n",
    "        out.append(ln.rstrip())\n",
    "    txt = \"\\n\".join(out).rstrip()\n",
    "    return txt + (\"\\n\" if (block_text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _assign_to_centers(x: int, centers, tol: int):\n",
    "    if not centers:\n",
    "        return 0\n",
    "    best_i = 0\n",
    "    best_d = abs(x - centers[0])\n",
    "    for i in range(1, len(centers)):\n",
    "        d = abs(x - centers[i])\n",
    "        if d < best_d:\n",
    "            best_d = d\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def _merge_close_columns(centers, row_cells, merge_dist: int):\n",
    "    i = 0\n",
    "    while i < len(centers) - 1:\n",
    "        if (centers[i+1] - centers[i]) <= merge_dist:\n",
    "            both = 0\n",
    "            alone_next = 0\n",
    "            for r in row_cells:\n",
    "                hi = i in r\n",
    "                hj = (i+1) in r\n",
    "                if hj and hi:\n",
    "                    both += 1\n",
    "                elif hj and not hi:\n",
    "                    alone_next += 1\n",
    "            if both >= 1 and alone_next <= max(1, int(0.2 * (both + alone_next))):\n",
    "                for r in row_cells:\n",
    "                    if (i+1) in r:\n",
    "                        t2, sp2 = r.pop(i+1)\n",
    "                        if i in r:\n",
    "                            t1, sp1 = r[i]\n",
    "                            r[i] = ((t1 + \"  \" + t2).strip(), sp1 + sp2)\n",
    "                        else:\n",
    "                            r[i] = (t2, sp2)\n",
    "\n",
    "                centers.pop(i+1)\n",
    "\n",
    "                for r in row_cells:\n",
    "                    ks = sorted([k for k in r.keys() if k > i+1])\n",
    "                    for k in ks:\n",
    "                        r[k-1] = r.pop(k)\n",
    "                continue\n",
    "        i += 1\n",
    "    return centers, row_cells\n",
    "\n",
    "def _is_grid_like(row_cells, col_count: int):\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if not rows:\n",
    "        return False\n",
    "    n_rows = len(rows)\n",
    "    if n_rows > 5:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / n_rows\n",
    "    return dens >= 0.70\n",
    "\n",
    "def _is_micro_table_like(row_cells, col_count: int) -> bool:\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if len(rows) < 2:\n",
    "        return False\n",
    "    if len(rows) > MICROTABLE_MAX_ROWS:\n",
    "        return False\n",
    "    multi = sum(1 for r in rows if len(r) >= 2)\n",
    "    if multi < MICROTABLE_MIN_MULTIROW:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / max(1, len(rows))\n",
    "    return dens >= MICROTABLE_MIN_DENS\n",
    "\n",
    "def _transpose_or_group_multicol(block_text: str, abs_start: int, gap_min: int, is_ocr: bool):\n",
    "    lines = []\n",
    "    segs_by_line = []\n",
    "\n",
    "    for ls, le in _iter_line_spans(block_text):\n",
    "        line_full = block_text[ls:le]\n",
    "        s = line_full[:-1] if line_full.endswith(\"\\n\") else line_full\n",
    "\n",
    "        lines.append((ls, le, line_full, s))\n",
    "\n",
    "        if _SEP_RE.match(s.strip()):\n",
    "            segs_by_line.append([])\n",
    "            continue\n",
    "\n",
    "        segs = _line_segments_by_gaps(s, gap_min)\n",
    "        segs = [g for g in segs if g.get(\"text\", \"\").strip()]\n",
    "        segs_by_line.append(segs)\n",
    "\n",
    "    xs = []\n",
    "    for segs in segs_by_line:\n",
    "        for g in segs:\n",
    "            txt = g[\"text\"].strip()\n",
    "            if len(txt) == 1 and txt in (\":\", \"|\", \"-\", \"_\"):\n",
    "                continue\n",
    "            xs.append(int(g[\"x\"]))\n",
    "\n",
    "    if not xs:\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    tol_cluster = 3 if is_ocr else 2\n",
    "    centers = _cluster_centers(xs, tol=tol_cluster, min_hits=1)\n",
    "    min_x = min(xs)\n",
    "    if min_x not in centers:\n",
    "        centers = sorted([min_x] + centers)\n",
    "    centers = centers[:8]\n",
    "\n",
    "    tol_assign = 6 if is_ocr else 4\n",
    "    row_cells = []\n",
    "    for (ls, le, line_full, s), segs in zip(lines, segs_by_line):\n",
    "        r = {}\n",
    "        for g in segs:\n",
    "            ci = _assign_to_centers(int(g[\"x\"]), centers, tol_assign)\n",
    "            a = abs_start + ls + int(g[\"a\"])\n",
    "            b = abs_start + ls + int(g[\"b\"])\n",
    "            txt = g[\"text\"].strip()\n",
    "\n",
    "            if ci in r:\n",
    "                t0, sp0 = r[ci]\n",
    "                r[ci] = ((t0 + \" \" + txt).strip(), sp0 + [(a, b)])\n",
    "            else:\n",
    "                r[ci] = (txt, [(a, b)])\n",
    "        row_cells.append(r)\n",
    "\n",
    "    merge_dist = MERGE_COL_DIST_OCR if is_ocr else MERGE_COL_DIST_NATIVE\n",
    "    centers, row_cells = _merge_close_columns(centers, row_cells, merge_dist=merge_dist)\n",
    "    col_count = len(centers)\n",
    "\n",
    "    if _is_micro_table_like(row_cells, col_count):\n",
    "        table_rows = []\n",
    "        for (ls, le, line_full, s) in lines:\n",
    "            table_rows.append({\"text\": line_full, \"spans\": [(abs_start + ls, abs_start + le)]})\n",
    "\n",
    "        table_cells = []\n",
    "        for r in row_cells:\n",
    "            row = []\n",
    "            for ci in range(col_count):\n",
    "                if ci in r:\n",
    "                    t, sp = r[ci]\n",
    "                    row.append({\"col\": ci, \"text\": t, \"spans\": [(int(a), int(b)) for a, b in sp if b > a]})\n",
    "                else:\n",
    "                    row.append({\"col\": ci, \"text\": \"\", \"spans\": []})\n",
    "            table_cells.append(row)\n",
    "\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"header\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "            \"table_rows\": table_rows,\n",
    "            \"table_cells\": table_cells,\n",
    "            \"header_source\": \"micro_multicol\",\n",
    "            \"column_centers\": centers,\n",
    "        }]\n",
    "\n",
    "    if _is_grid_like(row_cells, col_count):\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"multicol_grid\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    col_items = []\n",
    "    for ci in range(col_count):\n",
    "        out_lines = []\n",
    "        spans = []\n",
    "        for r in row_cells:\n",
    "            if ci in r:\n",
    "                t, sp = r[ci]\n",
    "                out_lines.append(t)\n",
    "                spans.extend(sp)\n",
    "            else:\n",
    "                out_lines.append(\"\")\n",
    "\n",
    "        while out_lines and not out_lines[0].strip():\n",
    "            out_lines.pop(0)\n",
    "        while out_lines and not out_lines[-1].strip():\n",
    "            out_lines.pop()\n",
    "\n",
    "        compact = []\n",
    "        blank = 0\n",
    "        for ln in out_lines:\n",
    "            if not ln.strip():\n",
    "                blank += 1\n",
    "                if blank <= 1:\n",
    "                    compact.append(\"\")\n",
    "            else:\n",
    "                blank = 0\n",
    "                compact.append(ln)\n",
    "\n",
    "        txt = \"\\n\".join(compact).rstrip() + (\"\\n\" if block_text.endswith(\"\\n\") else \"\")\n",
    "        txt = _normalize_kv_generic(txt)\n",
    "\n",
    "        if not _collapse_ws(txt).strip():\n",
    "            continue\n",
    "\n",
    "        if spans:\n",
    "            st = min(a for a, _ in spans)\n",
    "            en = max(b for _, b in spans)\n",
    "        else:\n",
    "            st = abs_start\n",
    "            en = abs_start + len(block_text)\n",
    "\n",
    "        col_items.append({\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(int(a), int(b)) for (a, b) in spans if b > a],\n",
    "            \"start\": st,\n",
    "            \"end\": en,\n",
    "            \"layout_kind\": \"multicol_col\",\n",
    "            \"col_index\": ci,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        })\n",
    "\n",
    "    if not col_items:\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    return col_items\n",
    "\n",
    "def _looks_like_paragraphish(line_full: str) -> bool:\n",
    "    s = (line_full or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if len(s) >= 120:\n",
    "        words = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", s)\n",
    "        if len(words) >= 10 and not _has_big_gap(s, 6, 1):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _is_address_continuation_line(line_full: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line_full or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return True\n",
    "    if _SEP_RE.match(st):\n",
    "        return True\n",
    "    if _is_table_line(line_full, gap_min):\n",
    "        return False\n",
    "    if TABLE_HINT_RE.search(st):\n",
    "        return False\n",
    "    if _is_section_start_line(line_full):\n",
    "        return False\n",
    "    if _dec_tokens(st) > 0:\n",
    "        return False\n",
    "    if _num_tokens(st) > (4 if is_ocr else 6):\n",
    "        return False\n",
    "    if re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", st) or _AR_RE.search(st):\n",
    "        return True\n",
    "    if re.match(r\"^\\d{4,6}$\", st):\n",
    "        return True\n",
    "    if re.search(r\"[@+/,-]\", st) and len(st) <= 120:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _collect_table_block(lines, start_i, gap_min):\n",
    "    n = len(lines)\n",
    "    i = start_i\n",
    "    blank_run = 0\n",
    "    seen_data = 0\n",
    "    collected = []\n",
    "\n",
    "    # FIX: reconnaître une \"wrap line\" indépendamment de la ligne précédente (utile pour chaîner wrap+wrap)\n",
    "    def _looks_like_wrap_line(s_raw: str) -> bool:\n",
    "        if not s_raw:\n",
    "            return False\n",
    "        # doit être indenté (comme dans ton exemple)\n",
    "        if not re.match(r\"^[ \\t]{2,}\\S\", s_raw):\n",
    "            return False\n",
    "        s_l = s_raw.lstrip(\" \\t\")\n",
    "        # pas une ligne structurante, pas de structure table\n",
    "        if _is_section_start_line(s_raw):\n",
    "            return False\n",
    "        if s_l.count(\"\\t\") >= 2:\n",
    "            return False\n",
    "        if _has_big_gap(s_l, gap_min, min_count=1):\n",
    "            return False\n",
    "        # très peu de signaux numériques (évite d'absorber TOTAL/TVA/etc)\n",
    "        if _dec_tokens(s_l) != 0:\n",
    "            return False\n",
    "        if _num_tokens(s_l) > 1:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    while i < n:\n",
    "        line_full, ls, le = lines[i]\n",
    "        s = line_full.rstrip(\"\\n\")\n",
    "\n",
    "        if not s.strip():\n",
    "            blank_run += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        is_tbl = _is_table_line(line_full, gap_min)\n",
    "\n",
    "        if is_tbl:\n",
    "            blank_run = 0\n",
    "            if _dec_tokens(s) >= 1 or _num_tokens(s) >= 3 or TABLE_HINT_RE.search(s):\n",
    "                seen_data += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # FIX: accepter wrap, y compris wrap qui suit wrap (pas seulement table_line)\n",
    "        if seen_data >= 1 and _looks_like_wrap_line(s):\n",
    "            prev_nonblank = None\n",
    "            for plf, _, _ in reversed(collected):\n",
    "                if plf.strip():\n",
    "                    prev_nonblank = plf.rstrip(\"\\n\")\n",
    "                    break\n",
    "            # on continue si la ligne précédente est soit une ligne de table, soit déjà une wrap\n",
    "            if prev_nonblank and (_is_table_line(prev_nonblank, gap_min) or _looks_like_wrap_line(prev_nonblank)):\n",
    "                blank_run = 0\n",
    "                collected.append((line_full, ls, le))\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        if seen_data >= 2 and blank_run >= 2:\n",
    "            break\n",
    "        if seen_data >= 1 and blank_run >= 1:\n",
    "            break\n",
    "        break\n",
    "\n",
    "    while collected and not collected[-1][0].strip():\n",
    "        collected.pop()\n",
    "\n",
    "    return collected, i\n",
    "\n",
    "def _make_span_item(page_text, spans, text_override, kind, meta=None):\n",
    "    spans2 = [(int(a), int(b)) for (a, b) in (spans or []) if b > a]\n",
    "    if spans2:\n",
    "        st = min(a for a, _ in spans2)\n",
    "        en = max(b for _, b in spans2)\n",
    "    else:\n",
    "        st = 0\n",
    "        en = 0\n",
    "    it = {\"text\": text_override, \"spans\": spans2, \"start\": st, \"end\": en, \"layout_kind\": kind}\n",
    "    if meta:\n",
    "        it.update(meta)\n",
    "    return it\n",
    "\n",
    "def layout_items(page_text: str, lang: str, extraction: str = \"\"):\n",
    "    if not page_text:\n",
    "        return []\n",
    "\n",
    "    is_ocr = str(extraction or \"\").startswith(\"ocr:\")\n",
    "    gap_min = GAP_MIN_OCR if is_ocr else GAP_MIN_NATIVE\n",
    "\n",
    "    lines = []\n",
    "    for ls, le in _iter_line_spans(page_text):\n",
    "        lines.append((page_text[ls:le], ls, le))\n",
    "\n",
    "    items = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    def _starts_table(i0):\n",
    "        return _is_table_line(lines[i0][0], gap_min)\n",
    "\n",
    "    def _starts_multicol(i0):\n",
    "        return _is_multicol_candidate_line(lines[i0][0], gap_min=gap_min, is_ocr=is_ocr)\n",
    "\n",
    "    while i < n:\n",
    "        if _starts_table(i):\n",
    "            collected, j = _collect_table_block(lines, i, gap_min=gap_min)\n",
    "            if collected:\n",
    "                a0 = collected[0][1]\n",
    "                b0 = collected[-1][2]\n",
    "                block_text = page_text[a0:b0]\n",
    "                table_rows = [{\"text\": lf, \"spans\": [(lls, lle)]} for (lf, lls, lle) in collected]\n",
    "                items.append(_make_span_item(\n",
    "                    page_text,\n",
    "                    spans=[(a0, b0)],\n",
    "                    text_override=block_text,\n",
    "                    kind=\"table\",\n",
    "                    meta={\"table_rows\": table_rows}\n",
    "                ))\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        if _starts_multicol(i):\n",
    "            start = i\n",
    "\n",
    "            if start - 1 >= 0:\n",
    "                prev_line = lines[start - 1][0]\n",
    "                if _looks_like_title_line(prev_line) and not _starts_table(start - 1):\n",
    "                    start -= 1\n",
    "\n",
    "            j = i\n",
    "            saw_any = False\n",
    "            blank_run = 0\n",
    "            noncol_inside = 0\n",
    "\n",
    "            MAX_INBLOCK_BLANK = 6\n",
    "            MAX_INBLOCK_LINES = 140\n",
    "            MAX_NONCOL_INSIDE = 25\n",
    "            weak_gap = max(3, gap_min - (3 if is_ocr else 2))\n",
    "\n",
    "            while j < n and (j - start) < MAX_INBLOCK_LINES:\n",
    "                if _starts_table(j):\n",
    "                    break\n",
    "\n",
    "                lf, lls, lle = lines[j]\n",
    "                ss = lf.rstrip(\"\\n\")\n",
    "\n",
    "                if not ss.strip() or _SEP_RE.match(ss.strip()):\n",
    "                    blank_run += 1\n",
    "                    j += 1\n",
    "                    if saw_any and blank_run >= MAX_INBLOCK_BLANK:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                blank_run = 0\n",
    "\n",
    "                if _starts_multicol(j):\n",
    "                    saw_any = True\n",
    "                    noncol_inside = 0\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                if saw_any and noncol_inside < MAX_NONCOL_INSIDE:\n",
    "                    if _is_address_continuation_line(lf, gap_min=gap_min, is_ocr=is_ocr) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    if _has_big_gap(ss, weak_gap, min_count=1) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "\n",
    "                break\n",
    "\n",
    "            end = j if j > i else i + 1\n",
    "\n",
    "            a0 = lines[start][1]\n",
    "            b0 = lines[end-1][2] if end-1 >= start else lines[start][2]\n",
    "            block_text = page_text[a0:b0]\n",
    "\n",
    "            items.extend(_transpose_or_group_multicol(block_text, abs_start=a0, gap_min=gap_min, is_ocr=is_ocr))\n",
    "\n",
    "            i = end\n",
    "            continue\n",
    "\n",
    "        start = i\n",
    "        j = i\n",
    "        while j < n:\n",
    "            if _starts_table(j) or _starts_multicol(j):\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        a0 = lines[start][1]\n",
    "        b0 = lines[j-1][2] if j-1 >= start else lines[start][2]\n",
    "        plain_text = page_text[a0:b0]\n",
    "\n",
    "        chunks = chunk_layout_universal(plain_text, lang)\n",
    "        pos = 0\n",
    "        for ch in chunks:\n",
    "            ca = a0 + pos\n",
    "            cb = ca + len(ch)\n",
    "            pos += len(ch)\n",
    "            items.append(_make_span_item(page_text, spans=[(ca, cb)], text_override=ch, kind=\"plain\"))\n",
    "\n",
    "        i = j if j > start else i + 1\n",
    "\n",
    "    def _k(it):\n",
    "        if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\"):\n",
    "            return (it.get(\"block_start\", it.get(\"start\", 0)), it.get(\"col_index\", 0) if it.get(\"col_index\") is not None else -1)\n",
    "        return (it.get(\"start\", 0), 0)\n",
    "\n",
    "    items.sort(key=_k)\n",
    "    return items\n",
    "\n",
    "# ==================== Noise detection (audit) ====================\n",
    "_NOISE_LINE_RE = re.compile(\n",
    "    r\"(?i)^\\s*(sample|confidential|draft)\\s*$|\"\n",
    "    r\"^\\s*page\\s+\\d+\\s*(?:of|/)\\s*\\d+\\s*$|\"\n",
    "    r\"^\\s*\\d+\\s*(?:of|/)\\s*\\d+\\s*$\"\n",
    ")\n",
    "\n",
    "def build_noise_keys_for_doc(pages_text):\n",
    "    if not pages_text:\n",
    "        return set()\n",
    "    page_count = len(pages_text)\n",
    "    if page_count < 3:\n",
    "        return set()\n",
    "    min_pages = max(3, int(math.ceil(page_count * 0.30)))\n",
    "\n",
    "    counts = {}\n",
    "    counts_masked = {}\n",
    "\n",
    "    for txt in pages_text:\n",
    "        seen = set()\n",
    "        seen_m = set()\n",
    "        for ls, le in _iter_line_spans(txt or \"\"):\n",
    "            line = (txt[ls:le]).rstrip(\"\\n\")\n",
    "            key = _collapse_ws(line).lower()\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            if _SEP_RE.match(key) or _NOISE_LINE_RE.match(line):\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                continue\n",
    "\n",
    "            mkey = _mask_digits(key)\n",
    "\n",
    "            if key not in seen:\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                seen.add(key)\n",
    "            if mkey not in seen_m:\n",
    "                counts_masked[mkey] = counts_masked.get(mkey, 0) + 1\n",
    "                seen_m.add(mkey)\n",
    "\n",
    "    noise_keys = set()\n",
    "    for k, c in counts.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(k)\n",
    "    for mk, c in counts_masked.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(mk)\n",
    "\n",
    "    return noise_keys\n",
    "\n",
    "def chunk_is_noise(chunk_text: str, noise_keys: set) -> bool:\n",
    "    if not chunk_text:\n",
    "        return True\n",
    "\n",
    "    has_nonempty = False\n",
    "    for ls, le in _iter_line_spans(chunk_text):\n",
    "        line = chunk_text[ls:le].rstrip(\"\\n\")\n",
    "        st = line.strip()\n",
    "        if not st:\n",
    "            continue\n",
    "        if _SEP_RE.match(st):\n",
    "            continue\n",
    "\n",
    "        has_nonempty = True\n",
    "        key = _collapse_ws(line).lower()\n",
    "        mkey = _mask_digits(key)\n",
    "\n",
    "        if _NOISE_LINE_RE.match(line):\n",
    "            continue\n",
    "        if key in noise_keys or mkey in noise_keys:\n",
    "            continue\n",
    "\n",
    "        return False\n",
    "\n",
    "    return True if has_nonempty else True\n",
    "\n",
    "# ==================== Helpers emplacement (page) ====================\n",
    "_WS_RE = re.compile(r\"\\s+\", flags=re.UNICODE)\n",
    "\n",
    "def _nonspace_len(s: str) -> int:\n",
    "    return len(_WS_RE.sub(\"\", s or \"\"))\n",
    "\n",
    "def _line_col_from_offset(text: str, off: int):\n",
    "    if off < 0:\n",
    "        off = 0\n",
    "    if off > len(text):\n",
    "        off = len(text)\n",
    "    line = text.count(\"\\n\", 0, off) + 1\n",
    "    last_nl = text.rfind(\"\\n\", 0, off)\n",
    "    col = off if last_nl < 0 else (off - last_nl - 1)\n",
    "    return line, col\n",
    "\n",
    "# ==================== Metadonnées depuis DOCS / TEXT_DOCS ====================\n",
    "def _safe_str(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _unique_keep_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def _pdf_extract_pages_text(path: str):\n",
    "    PdfReader = _get_pdf_reader()  # défini dans ta cellule d'extraction\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        out = []\n",
    "        for p in reader.pages:\n",
    "            out.append(p.extract_text() or \"\")\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _pdf_page_count(path: str):\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        return len(PdfReader(path).pages)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ==================== Vérifier FINAL_DOCS ====================\n",
    "if \"FINAL_DOCS\" not in globals() or not isinstance(FINAL_DOCS, list):\n",
    "    raise RuntimeError(\"FINAL_DOCS not found. Exécute d'abord la cellule précédente (celle qui imprime FINAL PRINT).\")\n",
    "\n",
    "# ==================== Construire une structure DOC -> PAGES ====================\n",
    "DOC_PACK = []\n",
    "\n",
    "# 1) OCR: DOCS (si dispo)\n",
    "if \"DOCS\" in globals() and isinstance(DOCS, list):\n",
    "    for d in DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        pages = d.get(\"pages\", []) or []\n",
    "        page_count_total = d.get(\"page_count_total\") if d.get(\"page_count_total\") else len(pages)\n",
    "\n",
    "        paths = []\n",
    "        for p in pages:\n",
    "            sp = p.get(\"source_path\") or p.get(\"path\")\n",
    "            if sp:\n",
    "                paths.append(_safe_str(sp))\n",
    "        paths = _unique_keep_order(paths)\n",
    "\n",
    "        pages_out = []\n",
    "        for p in pages:\n",
    "            pages_out.append({\n",
    "                \"page_index\": int(p.get(\"page_index\") or 1),\n",
    "                \"text\": p.get(\"ocr_text\") or \"\",\n",
    "                \"source_path\": _safe_str(p.get(\"source_path\") or p.get(\"path\") or \"\"),\n",
    "            })\n",
    "        pages_out.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"image_only\",\n",
    "            \"extraction\": \"ocr:tesseract\",\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 2) NATIVE: TEXT_DOCS (si dispo)\n",
    "if \"TEXT_DOCS\" in globals() and isinstance(TEXT_DOCS, list):\n",
    "    for d in TEXT_DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        extraction = d.get(\"extraction\") or \"native:unknown\"\n",
    "        sp = d.get(\"source_path\") or \"\"\n",
    "        paths = _unique_keep_order([_safe_str(sp)]) if sp else []\n",
    "        full_text = d.get(\"text\") or \"\"\n",
    "\n",
    "        pages_out = []\n",
    "        page_count_total = d.get(\"page_count_total\", None)\n",
    "        pages_text = d.get(\"pages_text\", None)\n",
    "\n",
    "        if pages_text is not None and isinstance(pages_text, list) and len(pages_text) > 0:\n",
    "            page_count_total = page_count_total or len(pages_text)\n",
    "            for i2, txt in enumerate(pages_text, start=1):\n",
    "                pages_out.append({\n",
    "                    \"page_index\": i2,\n",
    "                    \"text\": txt or \"\",\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "        else:\n",
    "            if sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "                pages_text2 = _pdf_extract_pages_text(sp)\n",
    "                if pages_text2:\n",
    "                    page_count_total = page_count_total or len(pages_text2)\n",
    "                    for i2, txt in enumerate(pages_text2, start=1):\n",
    "                        pages_out.append({\n",
    "                            \"page_index\": i2,\n",
    "                            \"text\": txt or \"\",\n",
    "                            \"source_path\": _safe_str(sp),\n",
    "                        })\n",
    "                else:\n",
    "                    pages_out.append({\n",
    "                        \"page_index\": 1,\n",
    "                        \"text\": full_text,\n",
    "                        \"source_path\": _safe_str(sp),\n",
    "                    })\n",
    "                    page_count_total = page_count_total or 1\n",
    "            else:\n",
    "                pages_out.append({\n",
    "                    \"page_index\": 1,\n",
    "                    \"text\": full_text,\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "                page_count_total = page_count_total or 1\n",
    "\n",
    "        if page_count_total is None and sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "            pc = _pdf_page_count(sp)\n",
    "            if pc is not None:\n",
    "                page_count_total = pc\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": extraction,\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 3) Fallback à FINAL_DOCS\n",
    "if not DOC_PACK:\n",
    "    for d in FINAL_DOCS:\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": d.get(\"doc_id\"),\n",
    "            \"filename\": d.get(\"filename\") or \"unknown\",\n",
    "            \"content\": d.get(\"content\"),\n",
    "            \"extraction\": d.get(\"extraction\"),\n",
    "            \"paths\": [],\n",
    "            \"page_count_total\": 1,\n",
    "            \"pages\": [{\"page_index\": 1, \"text\": d.get(\"text\") or \"\", \"source_path\": \"\"}],\n",
    "        })\n",
    "\n",
    "# ==================== Tokeniser: construire TOK_DOCS ====================\n",
    "TOK_DOCS = []\n",
    "\n",
    "for doc in DOC_PACK:\n",
    "    doc_id = doc.get(\"doc_id\")\n",
    "    filename = doc.get(\"filename\") or \"unknown\"\n",
    "    extraction = doc.get(\"extraction\") or \"\"\n",
    "    content_type = doc.get(\"content\")\n",
    "    paths = doc.get(\"paths\") or []\n",
    "    page_count_total = doc.get(\"page_count_total\")\n",
    "\n",
    "    pages_text_for_noise = [(p.get(\"text\") or \"\") for p in (doc.get(\"pages\") or [])]\n",
    "    noise_keys = build_noise_keys_for_doc(pages_text_for_noise)\n",
    "\n",
    "    pages_tok = []\n",
    "    doc_chars_total = 0\n",
    "    recompose_ok_doc = True\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        page_index = int(pg.get(\"page_index\") or 1)\n",
    "        page_text = pg.get(\"text\") or \"\"\n",
    "        doc_chars_total += len(page_text)\n",
    "\n",
    "        lang = detect_lang(page_text)\n",
    "\n",
    "        items = layout_items(page_text, lang, extraction=extraction)\n",
    "        recompose_ok = False if any(it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\") for it in items) else True\n",
    "        if not recompose_ok:\n",
    "            recompose_ok_doc = False\n",
    "\n",
    "        sent_items = []\n",
    "        for it in items:\n",
    "            chunk = it[\"text\"]\n",
    "            start = int(it.get(\"start\", 0))\n",
    "            end = int(it.get(\"end\", start + len(chunk)))\n",
    "\n",
    "            line, col = _line_col_from_offset(page_text, start)\n",
    "            nonspace = _nonspace_len(chunk)\n",
    "\n",
    "            is_noise = chunk_is_noise(chunk, noise_keys)\n",
    "\n",
    "            if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\"):\n",
    "                is_sentence = (not is_noise) and (nonspace >= 1)\n",
    "            else:\n",
    "                is_sentence = (nonspace >= MIN_SENTENCE_NONSPACE) and (not is_noise)\n",
    "\n",
    "            sent_items.append({\n",
    "                \"text\": chunk,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"line\": line,\n",
    "                \"col\": col,\n",
    "                \"chars\": len(chunk),\n",
    "                \"nonspace\": nonspace,\n",
    "                \"is_noise\": is_noise,\n",
    "                \"is_sentence\": is_sentence,\n",
    "                \"spans\": it.get(\"spans\", []),\n",
    "                \"layout_kind\": it.get(\"layout_kind\", \"plain\"),\n",
    "                \"col_index\": it.get(\"col_index\", None),\n",
    "                \"table_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_cells\": it.get(\"table_cells\", None),\n",
    "                \"header_source\": it.get(\"header_source\", None),\n",
    "            })\n",
    "\n",
    "        pages_tok.append({\n",
    "            \"page_index\": page_index,\n",
    "            \"source_path\": pg.get(\"source_path\") or \"\",\n",
    "            \"lang\": lang,\n",
    "            \"chars\": len(page_text),\n",
    "            \"recompose_ok\": recompose_ok,\n",
    "            \"sentences_layout\": sent_items,\n",
    "            \"page_text\": page_text,\n",
    "        })\n",
    "\n",
    "    pages_tok.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "    TOK_DOCS.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": filename,\n",
    "        \"paths\": paths,\n",
    "        \"page_count_total\": page_count_total,\n",
    "        \"content\": content_type,\n",
    "        \"extraction\": extraction,\n",
    "        \"pages\": pages_tok,\n",
    "        \"chars_total\": doc_chars_total,\n",
    "        \"recompose_ok\": recompose_ok_doc,\n",
    "    })\n",
    "\n",
    "def _sort_key(x):\n",
    "    p = (x.get(\"paths\") or [\"\"])[0]\n",
    "    return (x.get(\"filename\") or \"\", str(p))\n",
    "\n",
    "TOK_DOCS.sort(key=_sort_key)\n",
    "\n",
    "TOK_BY_ID = {d[\"doc_id\"]: d for d in TOK_DOCS if d.get(\"doc_id\")}\n",
    "TOK_BY_FILENAME = {}\n",
    "for d in TOK_DOCS:\n",
    "    TOK_BY_FILENAME.setdefault(d[\"filename\"], []).append(d)\n",
    "\n",
    "def _select_doc(target):\n",
    "    if target is None:\n",
    "        return TOK_DOCS\n",
    "    if isinstance(target, int):\n",
    "        if 0 <= target < len(TOK_DOCS):\n",
    "            return [TOK_DOCS[target]]\n",
    "        raise IndexError(f\"TARGET index out of range: {target} (0..{len(TOK_DOCS)-1})\")\n",
    "    if isinstance(target, str):\n",
    "        t = target.strip()\n",
    "        if t in TOK_BY_ID:\n",
    "            return [TOK_BY_ID[t]]\n",
    "        if t in TOK_BY_FILENAME:\n",
    "            return TOK_BY_FILENAME[t]\n",
    "        hits = []\n",
    "        for d in TOK_DOCS:\n",
    "            if t.lower() in (d.get(\"filename\",\"\").lower()):\n",
    "                hits.append(d)\n",
    "                continue\n",
    "            for p in d.get(\"paths\") or []:\n",
    "                if t.lower() in str(p).lower():\n",
    "                    hits.append(d)\n",
    "                    break\n",
    "        if hits:\n",
    "            return hits\n",
    "        raise ValueError(f\"No document matches TARGET='{target}' (by doc_id/filename/path).\")\n",
    "    raise TypeError(\"TARGET must be None, int, or str\")\n",
    "\n",
    "def print_one_doc(doc):\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"[doc] {doc['filename']}\")\n",
    "    print(f\"  doc_id       : {doc.get('doc_id')}\")\n",
    "    print(f\"  content      : {doc.get('content')}\")\n",
    "    print(f\"  extraction   : {doc.get('extraction')}\")\n",
    "    print(f\"  pages_total  : {doc.get('page_count_total')}\")\n",
    "    print(f\"  chars_total  : {doc.get('chars_total')}\")\n",
    "    print(f\"  recompose_ok : {doc.get('recompose_ok')}\")\n",
    "    print(\"  paths:\")\n",
    "    if doc.get(\"paths\"):\n",
    "        for p in doc[\"paths\"]:\n",
    "            print(f\"    - {p}\")\n",
    "    else:\n",
    "        print(\"    - (unknown)\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    if not PRINT_SENTENCES:\n",
    "        return\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        print(f\"[page {pg['page_index']}/{doc.get('page_count_total') or '?'}] source_path={pg.get('source_path')}\")\n",
    "        print(f\"  lang         : {pg.get('lang')}\")\n",
    "        print(f\"  chars        : {pg.get('chars')}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        if PRINT_PAGE_TEXT:\n",
    "            print(pg.get(\"page_text\") or \"\")\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "        sent_items = pg.get(\"sentences_layout\") or []\n",
    "\n",
    "        total_all = len(sent_items)\n",
    "        total_noise = sum(1 for s in sent_items if s.get(\"is_noise\"))\n",
    "        total_sentence = sum(1 for s in sent_items if s.get(\"is_sentence\"))\n",
    "\n",
    "        if PRINT_ONLY_SENTENCES:\n",
    "            view = [s for s in sent_items if s.get(\"is_sentence\")]\n",
    "        else:\n",
    "            view = list(sent_items)\n",
    "\n",
    "        fallback_used = False\n",
    "        if PRINT_ONLY_SENTENCES and not view and total_all > 0:\n",
    "            view = list(sent_items)\n",
    "            fallback_used = True\n",
    "\n",
    "        total_view = len(view)\n",
    "        show = total_view if MAX_SENTENCES_PREVIEW is None else min(total_view, MAX_SENTENCES_PREVIEW)\n",
    "\n",
    "        print(\n",
    "            f\"  sentences_layout: {total_all} chunks total | \"\n",
    "            f\"sentences={total_sentence} | noise={total_noise} | \"\n",
    "            f\"showing {show}/{total_view} \"\n",
    "            f\"(filter_is_sentence={PRINT_ONLY_SENTENCES}, fallback={fallback_used}, min_nonspace={MIN_SENTENCE_NONSPACE})\"\n",
    "        )\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        for i2 in range(show):\n",
    "            s = view[i2]\n",
    "            chunk = s[\"text\"]\n",
    "            print(\n",
    "                f\"[sent {i2+1}/{total_view}] page={pg['page_index']} start={s['start']} end={s['end']} \"\n",
    "                f\"line={s['line']} col={s['col']} chars={s['chars']} nonspace={s['nonspace']} \"\n",
    "                f\"is_noise={s.get('is_noise')} is_sentence={s['is_sentence']} layout={s.get('layout_kind')}\"\n",
    "            )\n",
    "            print(chunk, end=\"\" if chunk.endswith(\"\\n\") else \"\\n\")\n",
    "            if PRINT_REPR:\n",
    "                print(\"repr:\", repr(chunk))\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        if MAX_SENTENCES_PREVIEW is not None and total_view > show:\n",
    "            print(f\"... {total_view - show} chunks restants non affichés (MAX_SENTENCES_PREVIEW={MAX_SENTENCES_PREVIEW})\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# ==================== Exécution ====================\n",
    "selected = _select_doc(TARGET)\n",
    "\n",
    "if not selected:\n",
    "    print(\"[info] Aucun document à traiter.\")\n",
    "else:\n",
    "    for doc in selected:\n",
    "        print_one_doc(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b2b4a",
   "metadata": {},
   "source": [
    "### attribue une catégorie grammaticale // jeu d’étiquettes NN, NNS, VB, VBD, JJ ...///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10970649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps: (absent)\n",
      "Type an Arabic sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n",
      "\n",
      "========================================================================================================================\n",
      "RUN EN (engcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "##########################################################################################\n",
      "DOC=englais.docx | page=1 | sent=0 | lang=en\n",
      "##########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "INPUT: On February 14, 2026, at 07:35 a.m., MaghrebRail announced from Algiers-Centre a new modernization phase for the commuter line linking Bab Ezzouar to Blida via El Harrach and Birtouta.\n",
      "               On  IN      lemma=on\n",
      "February 14, 2026  NNP     lemma=february 14, 2026\n",
      "                ,  PUNCT   lemma=∅\n",
      "               at  IN      lemma=at\n",
      "               07  CD      lemma=07\n",
      "                :  PUNCT   lemma=∅\n",
      "               35  CD      lemma=35\n",
      "             a.m.  NN      lemma=a.m.\n",
      "                ,  PUNCT   lemma=∅\n",
      "      MaghrebRail  NNP     lemma=maghrebrail\n",
      "        announced  VB      lemma=announce\n",
      "             from  IN      lemma=from\n",
      "   Algiers-Centre  NNP     lemma=algiers-centre\n",
      "                a  DT      lemma=a\n",
      "              new  JJ      lemma=new\n",
      "    modernization  NN      lemma=modernization\n",
      "            phase  NN      lemma=phase\n",
      "              for  IN      lemma=for\n",
      "              the  DT      lemma=the\n",
      "         commuter  NN      lemma=commuter\n",
      "             line  NN      lemma=line\n",
      "          linking  VB      lemma=link\n",
      "              Bab  NNP     lemma=bab\n",
      "          Ezzouar  NNP     lemma=ezzouar\n",
      "               to  IN      lemma=to\n",
      "            Blida  NNP     lemma=blida\n",
      "              via  IN      lemma=via\n",
      "               El  NNP     lemma=el\n",
      "          Harrach  NNP     lemma=harrach\n",
      "              and  CC      lemma=and\n",
      "         Birtouta  NNP     lemma=birtouta\n",
      "                .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): dslim/bert-base-NER) (token, label):\n",
      "[('On', 'O'), ('February 14, 2026', 'B-DATE'), (',', 'O'), ('at', 'O'), ('07', 'O'), (':', 'O'), ('35', 'O'), ('a.m.', 'O'), (',', 'O'), ('MaghrebRail', 'B-ORG'), ('announced', 'O'), ('from', 'O'), ('Algiers-Centre', 'B-LOC'), ('a', 'O'), ('new', 'O'), ('modernization', 'O'), ('phase', 'O'), ('for', 'O'), ('the', 'O'), ('commuter', 'O'), ('line', 'O'), ('linking', 'O'), ('Bab', 'B-LOC'), ('Ezzouar', 'I-LOC'), ('to', 'O'), ('Blida', 'B-LOC'), ('via', 'O'), ('El', 'B-LOC'), ('Harrach', 'I-LOC'), ('and', 'O'), ('Birtouta', 'B-LOC'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: February 14, 2026\n",
      "  LOC: Algiers-Centre\n",
      "  LOC: Bab Ezzouar\n",
      "  LOC: Blida\n",
      "  LOC: El Harrach\n",
      "  LOC: Birtouta\n",
      "  ORG: MaghrebRail\n",
      "\n",
      "Audit (heuristique, pas une mesure de justesse):\n",
      "  tokens total     : 32\n",
      "  POS mode         : NLTK (mapped->simple)\n",
      "  lemma mode       : NLTK WordNetLemmatizer\n",
      "  NN/NNP ratio     : 14/32 = 43.75 %\n",
      "  contraction tokens (approx): 0 (0.00 %)\n",
      "\n",
      "##########################################################################################\n",
      "DOC=englais.docx | page=1 | sent=1 | lang=en\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: According to an internal memo signed by Samir Ould-Kaci (Chief Operations Officer), work should start on March 3, 2026 and last “18 to 24 months”, with an estimated budget of DZD 1.8 billion (about $13.4 million), 35% financed by a private consortium led by NorthGate Infrastructure and supported by engineers from Lyon, Milan, and Hamburg.\n",
      "     According  VB      lemma=accord\n",
      "            to  IN      lemma=to\n",
      "            an  DT      lemma=an\n",
      "      internal  JJ      lemma=internal\n",
      "          memo  NN      lemma=memo\n",
      "        signed  VB      lemma=sign\n",
      "            by  IN      lemma=by\n",
      "         Samir  NNP     lemma=samir\n",
      "     Ould-Kaci  NNP     lemma=ould-kaci\n",
      "             (  PUNCT   lemma=∅\n",
      "         Chief  NNP     lemma=chief\n",
      "    Operations  NNP     lemma=operation\n",
      "       Officer  NNP     lemma=officer\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "          work  NN      lemma=work\n",
      "        should  VB      lemma=should\n",
      "         start  VB      lemma=start\n",
      "            on  IN      lemma=on\n",
      " March 3, 2026  NNP     lemma=march 3, 2026\n",
      "           and  CC      lemma=and\n",
      "          last  JJ      lemma=last\n",
      "             “  JJ      lemma=“\n",
      "            18  CD      lemma=18\n",
      "            to  IN      lemma=to\n",
      "            24  CD      lemma=24\n",
      "        months  NN      lemma=month\n",
      "             ”  RB      lemma=”\n",
      "             ,  PUNCT   lemma=∅\n",
      "          with  IN      lemma=with\n",
      "            an  DT      lemma=an\n",
      "     estimated  JJ      lemma=estimated\n",
      "        budget  NN      lemma=budget\n",
      "            of  IN      lemma=of\n",
      "           DZD  NNP     lemma=dzd\n",
      "           1.8  CD      lemma=1.8\n",
      "       billion  CD      lemma=billion\n",
      "             (  PUNCT   lemma=∅\n",
      "         about  IN      lemma=about\n",
      "             $  NN      lemma=$\n",
      "          13.4  CD      lemma=13.4\n",
      "       million  CD      lemma=million\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "            35  CD      lemma=35\n",
      "             %  PUNCT   lemma=∅\n",
      "      financed  VB      lemma=finance\n",
      "            by  IN      lemma=by\n",
      "             a  DT      lemma=a\n",
      "       private  JJ      lemma=private\n",
      "    consortium  NN      lemma=consortium\n",
      "           led  VB      lemma=lead\n",
      "            by  IN      lemma=by\n",
      "     NorthGate  NNP     lemma=northgate\n",
      "Infrastructure  NNP     lemma=infrastructure\n",
      "           and  CC      lemma=and\n",
      "     supported  VB      lemma=support\n",
      "            by  IN      lemma=by\n",
      "     engineers  NN      lemma=engineer\n",
      "          from  IN      lemma=from\n",
      "          Lyon  NNP     lemma=lyon\n",
      "             ,  PUNCT   lemma=∅\n",
      "         Milan  NNP     lemma=milan\n",
      "             ,  PUNCT   lemma=∅\n",
      "           and  CC      lemma=and\n",
      "       Hamburg  NNP     lemma=hamburg\n",
      "             .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): dslim/bert-base-NER) (token, label):\n",
      "[('According', 'O'), ('to', 'O'), ('an', 'O'), ('internal', 'O'), ('memo', 'O'), ('signed', 'O'), ('by', 'O'), ('Samir', 'B-PERS'), ('Ould-Kaci', 'B-PERS'), ('(', 'O'), ('Chief', 'O'), ('Operations', 'O'), ('Officer', 'O'), (')', 'O'), (',', 'O'), ('work', 'O'), ('should', 'O'), ('start', 'O'), ('on', 'O'), ('March 3, 2026', 'B-DATE'), ('and', 'O'), ('last', 'O'), ('“', 'O'), ('18', 'O'), ('to', 'O'), ('24', 'O'), ('months', 'O'), ('”', 'O'), (',', 'O'), ('with', 'O'), ('an', 'O'), ('estimated', 'O'), ('budget', 'O'), ('of', 'O'), ('DZD', 'B-MISC'), ('1.8', 'O'), ('billion', 'O'), ('(', 'O'), ('about', 'O'), ('$', 'O'), ('13.4', 'O'), ('million', 'O'), (')', 'O'), (',', 'O'), ('35', 'O'), ('%', 'O'), ('financed', 'O'), ('by', 'O'), ('a', 'O'), ('private', 'O'), ('consortium', 'O'), ('led', 'O'), ('by', 'O'), ('NorthGate', 'B-ORG'), ('Infrastructure', 'I-ORG'), ('and', 'O'), ('supported', 'O'), ('by', 'O'), ('engineers', 'O'), ('from', 'O'), ('Lyon', 'B-LOC'), (',', 'O'), ('Milan', 'B-LOC'), (',', 'O'), ('and', 'O'), ('Hamburg', 'B-LOC'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: March 3, 2026\n",
      "  LOC: Lyon\n",
      "  LOC: Milan\n",
      "  LOC: Hamburg\n",
      "  MISC: DZD\n",
      "  ORG: NorthGate Infrastructure\n",
      "  PERS: Samir\n",
      "  PERS: Ould-Kaci\n",
      "\n",
      "Audit (heuristique, pas une mesure de justesse):\n",
      "  tokens total     : 67\n",
      "  POS mode         : NLTK (mapped->simple)\n",
      "  lemma mode       : NLTK WordNetLemmatizer\n",
      "  NN/NNP ratio     : 19/67 = 28.36 %\n",
      "  contraction tokens (approx): 0 (0.00 %)\n",
      "\n",
      "##########################################################################################\n",
      "DOC=englais.docx | page=1 | sent=2 | lang=en\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: The memo also mentions a research partnership with the Houari Boumédiène University of Science and Technology and a pilot safety dashboard tracking KPIs such as on-time rate, incident reports, and peak-hour load during Ramadan and match days at Baraki Stadium.\n",
      "        The  DT      lemma=the\n",
      "       memo  NN      lemma=memo\n",
      "       also  RB      lemma=also\n",
      "   mentions  VB      lemma=mention\n",
      "          a  DT      lemma=a\n",
      "   research  NN      lemma=research\n",
      "partnership  NN      lemma=partnership\n",
      "       with  IN      lemma=with\n",
      "        the  DT      lemma=the\n",
      "     Houari  NNP     lemma=houari\n",
      "       Boum  NNP     lemma=boum\n",
      "          é  NNP     lemma=é\n",
      "         di  NN      lemma=di\n",
      "          è  NNP     lemma=è\n",
      "         ne  NN      lemma=ne\n",
      " University  NNP     lemma=university\n",
      "         of  IN      lemma=of\n",
      "    Science  NNP     lemma=science\n",
      "        and  CC      lemma=and\n",
      " Technology  NNP     lemma=technology\n",
      "        and  CC      lemma=and\n",
      "          a  DT      lemma=a\n",
      "      pilot  NN      lemma=pilot\n",
      "     safety  NN      lemma=safety\n",
      "  dashboard  NN      lemma=dashboard\n",
      "   tracking  VB      lemma=track\n",
      "       KPIs  NNP     lemma=kpis\n",
      "       such  JJ      lemma=such\n",
      "         as  IN      lemma=a\n",
      "    on-time  JJ      lemma=on-time\n",
      "       rate  NN      lemma=rate\n",
      "          ,  PUNCT   lemma=∅\n",
      "   incident  NN      lemma=incident\n",
      "    reports  NN      lemma=report\n",
      "          ,  PUNCT   lemma=∅\n",
      "        and  CC      lemma=and\n",
      "  peak-hour  JJ      lemma=peak-hour\n",
      "       load  NN      lemma=load\n",
      "     during  IN      lemma=during\n",
      "    Ramadan  NNP     lemma=ramadan\n",
      "        and  CC      lemma=and\n",
      "      match  VB      lemma=match\n",
      "       days  NN      lemma=day\n",
      "         at  IN      lemma=at\n",
      "     Baraki  NNP     lemma=baraki\n",
      "    Stadium  NNP     lemma=stadium\n",
      "          .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): dslim/bert-base-NER) (token, label):\n",
      "[('The', 'O'), ('memo', 'O'), ('also', 'O'), ('mentions', 'O'), ('a', 'O'), ('research', 'O'), ('partnership', 'O'), ('with', 'O'), ('the', 'O'), ('Houari', 'B-ORG'), ('Boum', 'O'), ('é', 'O'), ('di', 'O'), ('è', 'O'), ('ne', 'O'), ('University', 'O'), ('of', 'O'), ('Science', 'O'), ('and', 'O'), ('Technology', 'O'), ('and', 'O'), ('a', 'O'), ('pilot', 'O'), ('safety', 'O'), ('dashboard', 'O'), ('tracking', 'O'), ('KPIs', 'O'), ('such', 'O'), ('as', 'O'), ('on-time', 'O'), ('rate', 'O'), (',', 'O'), ('incident', 'O'), ('reports', 'O'), (',', 'O'), ('and', 'O'), ('peak-hour', 'O'), ('load', 'O'), ('during', 'O'), ('Ramadan', 'B-MISC'), ('and', 'O'), ('match', 'O'), ('days', 'O'), ('at', 'O'), ('Baraki', 'B-LOC'), ('Stadium', 'I-LOC'), ('.', 'O')]\n",
      "Entities:\n",
      "  LOC: Baraki Stadium\n",
      "  MISC: Ramadan\n",
      "  ORG: Houari\n",
      "\n",
      "Audit (heuristique, pas une mesure de justesse):\n",
      "  tokens total     : 47\n",
      "  POS mode         : NLTK (mapped->simple)\n",
      "  lemma mode       : NLTK WordNetLemmatizer\n",
      "  NN/NNP ratio     : 24/47 = 51.06 %\n",
      "  contraction tokens (approx): 0 (0.00 %)\n",
      "\n",
      "##########################################################################################\n",
      "DOC=englais.docx | page=1 | sent=3 | lang=en\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: Meanwhile, residents posted mixed reactions on social media: “We need fewer delays and better security,” wrote Amina B., while Mourad, a shop owner near the station, worried that roadworks would disrupt deliveries; one comment even claimed “the meeting is postponed” and another replied “we’ll adapt.” For stress-testing NLP tools, the text includes tricky tokens like UUIDs (550e8400-e29b-41d4-a716-446655440000), Unix paths (/var/log/nginx/access.log), scientific notation (3.2e-4, 2×10−3), hashtags (#DevOps, #AI), handles (@support, @team-lead), URLs (https://example.org/api/v1/users?id=42), and emails (first.last+test@domain.dz) — all of which can confuse POS tagging, lemmatization, and NER if normalization is too aggressive.\n",
      "     Meanwhile  RB      lemma=meanwhile\n",
      "             ,  PUNCT   lemma=∅\n",
      "     residents  NN      lemma=resident\n",
      "        posted  VB      lemma=post\n",
      "         mixed  JJ      lemma=mixed\n",
      "     reactions  NN      lemma=reaction\n",
      "            on  IN      lemma=on\n",
      "        social  JJ      lemma=social\n",
      "         media  NN      lemma=medium\n",
      "             :  PUNCT   lemma=∅\n",
      "             “  NN      lemma=“\n",
      "            We  PRP     lemma=we\n",
      "          need  VB      lemma=need\n",
      "         fewer  JJ      lemma=few\n",
      "        delays  NN      lemma=delay\n",
      "           and  CC      lemma=and\n",
      "        better  JJ      lemma=good\n",
      "      security  NN      lemma=security\n",
      "             ,  PUNCT   lemma=∅\n",
      "             ”  NNP     lemma=”\n",
      "         wrote  VB      lemma=write\n",
      "         Amina  NNP     lemma=amina\n",
      "             B  NNP     lemma=b\n",
      "             .  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "         while  IN      lemma=while\n",
      "        Mourad  NNP     lemma=mourad\n",
      "             ,  PUNCT   lemma=∅\n",
      "             a  DT      lemma=a\n",
      "          shop  NN      lemma=shop\n",
      "         owner  NN      lemma=owner\n",
      "          near  IN      lemma=near\n",
      "           the  DT      lemma=the\n",
      "       station  NN      lemma=station\n",
      "             ,  PUNCT   lemma=∅\n",
      "       worried  VB      lemma=worry\n",
      "          that  IN      lemma=that\n",
      "     roadworks  NN      lemma=roadworks\n",
      "         would  VB      lemma=would\n",
      "       disrupt  VB      lemma=disrupt\n",
      "    deliveries  NN      lemma=delivery\n",
      "             ;  PUNCT   lemma=∅\n",
      "           one  CD      lemma=one\n",
      "       comment  NN      lemma=comment\n",
      "          even  RB      lemma=even\n",
      "       claimed  VB      lemma=claim\n",
      "             “  IN      lemma=“\n",
      "           the  DT      lemma=the\n",
      "       meeting  NN      lemma=meeting\n",
      "            is  VB      lemma=be\n",
      "     postponed  VB      lemma=postpone\n",
      "             ”  NNP     lemma=”\n",
      "           and  CC      lemma=and\n",
      "       another  DT      lemma=another\n",
      "       replied  JJ      lemma=replied\n",
      "             “  NN      lemma=“\n",
      "            we  PRP     lemma=we\n",
      "          will  VB      lemma=will\n",
      "         adapt  VB      lemma=adapt\n",
      "             .  PUNCT   lemma=∅\n",
      "             ”  VB      lemma=”\n",
      "           For  IN      lemma=for\n",
      "stress-testing  JJ      lemma=stress-testing\n",
      "           NLP  NNP     lemma=nlp\n",
      "         tools  NN      lemma=tool\n",
      "             ,  PUNCT   lemma=∅\n",
      "           the  DT      lemma=the\n",
      "          text  NN      lemma=text\n",
      "      includes  VB      lemma=include\n",
      "        tricky  JJ      lemma=tricky\n",
      "        tokens  NN      lemma=token\n",
      "          like  IN      lemma=like\n",
      "         UUIDs  NNP     lemma=uuids\n",
      "             (  PUNCT   lemma=∅\n",
      "           550  CD      lemma=550\n",
      "             e  RB      lemma=e\n",
      "        8400-e  JJ      lemma=8400-e\n",
      "            29  CD      lemma=29\n",
      "             b  NN      lemma=b\n",
      "             -  PUNCT   lemma=∅\n",
      "            41  CD      lemma=41\n",
      "             d  JJ      lemma=d\n",
      "           4-a  JJ      lemma=4-a\n",
      "           716  CD      lemma=716\n",
      "             -  PUNCT   lemma=∅\n",
      "  446655440000  CD      lemma=446655440000\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "          Unix  NNP     lemma=unix\n",
      "         paths  NN      lemma=path\n",
      "             (  PUNCT   lemma=∅\n",
      "             /  PUNCT   lemma=∅\n",
      "           var  NN      lemma=var\n",
      "             /  PUNCT   lemma=∅\n",
      "           log  NN      lemma=log\n",
      "             /  PUNCT   lemma=∅\n",
      "         nginx  NN      lemma=nginx\n",
      "             /  PUNCT   lemma=∅\n",
      "        access  NN      lemma=access\n",
      "             .  PUNCT   lemma=∅\n",
      "           log  NN      lemma=log\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "    scientific  JJ      lemma=scientific\n",
      "      notation  NN      lemma=notation\n",
      "             (  PUNCT   lemma=∅\n",
      "           3.2  CD      lemma=3.2\n",
      "             e  NN      lemma=e\n",
      "             -  PUNCT   lemma=∅\n",
      "             4  CD      lemma=4\n",
      "             ,  PUNCT   lemma=∅\n",
      "             2  CD      lemma=2\n",
      "             ×  NN      lemma=×\n",
      "            10  CD      lemma=10\n",
      "             −  NN      lemma=−\n",
      "             3  CD      lemma=3\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "      hashtags  NN      lemma=hashtags\n",
      "             (  PUNCT   lemma=∅\n",
      "             #  NN      lemma=#\n",
      "        DevOps  NNP     lemma=devops\n",
      "             ,  PUNCT   lemma=∅\n",
      "             #  NN      lemma=#\n",
      "            AI  NNP     lemma=ai\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "       handles  NN      lemma=handle\n",
      "             (  PUNCT   lemma=∅\n",
      "             @  PUNCT   lemma=∅\n",
      "       support  NN      lemma=support\n",
      "             ,  PUNCT   lemma=∅\n",
      "             @  PUNCT   lemma=∅\n",
      "     team-lead  NN      lemma=team-lead\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "          URLs  NNP     lemma=url\n",
      "             (  PUNCT   lemma=∅\n",
      "         https  NN      lemma=http\n",
      "             :  PUNCT   lemma=∅\n",
      "             /  PUNCT   lemma=∅\n",
      "             /  PUNCT   lemma=∅\n",
      "       example  NN      lemma=example\n",
      "             .  PUNCT   lemma=∅\n",
      "           org  VB      lemma=org\n",
      "             /  PUNCT   lemma=∅\n",
      "           api  NN      lemma=api\n",
      "             /  PUNCT   lemma=∅\n",
      "             v  VB      lemma=v\n",
      "             1  CD      lemma=1\n",
      "             /  PUNCT   lemma=∅\n",
      "         users  NN      lemma=user\n",
      "             ?  PUNCT   lemma=∅\n",
      "            id  JJ      lemma=id\n",
      "             =  PUNCT   lemma=∅\n",
      "            42  CD      lemma=42\n",
      "             )  PUNCT   lemma=∅\n",
      "             ,  PUNCT   lemma=∅\n",
      "           and  CC      lemma=and\n",
      "        emails  NN      lemma=email\n",
      "             (  PUNCT   lemma=∅\n",
      "         first  RB      lemma=first\n",
      "             .  PUNCT   lemma=∅\n",
      "          last  JJ      lemma=last\n",
      "             +  PUNCT   lemma=∅\n",
      "          test  NN      lemma=test\n",
      "             @  PUNCT   lemma=∅\n",
      "        domain  NN      lemma=domain\n",
      "             .  PUNCT   lemma=∅\n",
      "            dz  NN      lemma=dz\n",
      "             )  PUNCT   lemma=∅\n",
      "             —  PUNCT   lemma=∅\n",
      "           all  DT      lemma=all\n",
      "            of  IN      lemma=of\n",
      "         which  WDT     lemma=which\n",
      "           can  VB      lemma=can\n",
      "       confuse  VB      lemma=confuse\n",
      "           POS  NNP     lemma=po\n",
      "       tagging  NN      lemma=tagging\n",
      "             ,  PUNCT   lemma=∅\n",
      " lemmatization  NN      lemma=lemmatization\n",
      "             ,  PUNCT   lemma=∅\n",
      "           and  CC      lemma=and\n",
      "           NER  NNP     lemma=ner\n",
      "            if  IN      lemma=if\n",
      " normalization  NN      lemma=normalization\n",
      "            is  VB      lemma=be\n",
      "           too  RB      lemma=too\n",
      "    aggressive  JJ      lemma=aggressive\n",
      "             .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): dslim/bert-base-NER) (token, label):\n",
      "[('Meanwhile', 'O'), (',', 'O'), ('residents', 'O'), ('posted', 'O'), ('mixed', 'O'), ('reactions', 'O'), ('on', 'O'), ('social', 'O'), ('media', 'O'), (':', 'O'), ('“', 'O'), ('We', 'O'), ('need', 'O'), ('fewer', 'O'), ('delays', 'O'), ('and', 'O'), ('better', 'O'), ('security', 'O'), (',', 'O'), ('”', 'O'), ('wrote', 'O'), ('Amina', 'B-PERS'), ('B', 'O'), ('.', 'O'), (',', 'O'), ('while', 'O'), ('Mourad', 'B-PERS'), (',', 'O'), ('a', 'O'), ('shop', 'O'), ('owner', 'O'), ('near', 'O'), ('the', 'O'), ('station', 'O'), (',', 'O'), ('worried', 'O'), ('that', 'O'), ('roadworks', 'O'), ('would', 'O'), ('disrupt', 'O'), ('deliveries', 'O'), (';', 'O'), ('one', 'O'), ('comment', 'O'), ('even', 'O'), ('claimed', 'O'), ('“', 'O'), ('the', 'O'), ('meeting', 'O'), ('is', 'O'), ('postponed', 'O'), ('”', 'O'), ('and', 'O'), ('another', 'O'), ('replied', 'O'), ('“', 'O'), ('we', 'O'), ('will', 'O'), ('adapt', 'O'), ('.', 'O'), ('”', 'O'), ('For', 'O'), ('stress-testing', 'O'), ('NLP', 'B-MISC'), ('tools', 'O'), (',', 'O'), ('the', 'O'), ('text', 'O'), ('includes', 'O'), ('tricky', 'O'), ('tokens', 'O'), ('like', 'O'), ('UUIDs', 'B-MISC'), ('(', 'O'), ('550', 'O'), ('e', 'O'), ('8400-e', 'O'), ('29', 'O'), ('b', 'O'), ('-', 'O'), ('41', 'O'), ('d', 'O'), ('4-a', 'O'), ('716', 'O'), ('-', 'O'), ('446655440000', 'O'), (')', 'O'), (',', 'O'), ('Unix', 'B-MISC'), ('paths', 'O'), ('(', 'O'), ('/', 'O'), ('var', 'O'), ('/', 'O'), ('log', 'O'), ('/', 'O'), ('nginx', 'O'), ('/', 'O'), ('access', 'O'), ('.', 'O'), ('log', 'O'), (')', 'O'), (',', 'O'), ('scientific', 'O'), ('notation', 'O'), ('(', 'O'), ('3.2', 'O'), ('e', 'O'), ('-', 'O'), ('4', 'O'), (',', 'O'), ('2', 'O'), ('×', 'O'), ('10', 'O'), ('−', 'O'), ('3', 'O'), (')', 'O'), (',', 'O'), ('hashtags', 'O'), ('(', 'O'), ('#', 'O'), ('DevOps', 'O'), (',', 'O'), ('#', 'O'), ('AI', 'O'), (')', 'O'), (',', 'O'), ('handles', 'O'), ('(', 'O'), ('@', 'O'), ('support', 'O'), (',', 'O'), ('@', 'O'), ('team-lead', 'O'), (')', 'O'), (',', 'O'), ('URLs', 'O'), ('(', 'O'), ('https', 'O'), (':', 'O'), ('/', 'O'), ('/', 'O'), ('example', 'O'), ('.', 'O'), ('org', 'O'), ('/', 'O'), ('api', 'O'), ('/', 'O'), ('v', 'O'), ('1', 'O'), ('/', 'O'), ('users', 'O'), ('?', 'O'), ('id', 'O'), ('=', 'O'), ('42', 'O'), (')', 'O'), (',', 'O'), ('and', 'O'), ('emails', 'O'), ('(', 'O'), ('first', 'O'), ('.', 'O'), ('last', 'O'), ('+', 'O'), ('test', 'O'), ('@', 'O'), ('domain', 'O'), ('.', 'O'), ('dz', 'O'), (')', 'O'), ('—', 'O'), ('all', 'O'), ('of', 'O'), ('which', 'O'), ('can', 'O'), ('confuse', 'O'), ('POS', 'B-ORG'), ('tagging', 'O'), (',', 'O'), ('lemmatization', 'O'), (',', 'O'), ('and', 'O'), ('NER', 'O'), ('if', 'O'), ('normalization', 'O'), ('is', 'O'), ('too', 'O'), ('aggressive', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  MISC: NLP\n",
      "  MISC: UUIDs\n",
      "  MISC: Unix\n",
      "  ORG: POS\n",
      "  PERS: Amina\n",
      "  PERS: Mourad\n",
      "\n",
      "Audit (heuristique, pas une mesure de justesse):\n",
      "  tokens total     : 190\n",
      "  POS mode         : NLTK (mapped->simple)\n",
      "  lemma mode       : NLTK WordNetLemmatizer\n",
      "  NN/NNP ratio     : 58/190 = 30.53 %\n",
      "  contraction tokens (approx): 4 (2.11 %)\n",
      "\n",
      "========================================================================================================================\n",
      "RUN FR (frcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "##########################################################################################\n",
      "DOC=francais.docx | page=1 | sent=0 | lang=fr\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: Le 14 février 2026, à 07:35, MaghrebRail a annoncé depuis Alger-Centre une nouvelle phase de modernisation de la ligne de banlieue reliant Bab Ezzouar à Blida via El Harrach et Birtouta, en s’appuyant sur une note interne signée par Samir Ould-Kaci (Chief Operations Officer) indiquant un démarrage le 3 mars 2026 et une durée estimée de 18 à 24 mois, pour un budget de 1,8 milliard DZD (≈ 13,4 M$) dont 35% financés par un consortium privé mené par NorthGate Infrastructure, avec l’appui d’ingénieurs basés à Lyon, Milan et Hambourg, et un partenariat de recherche avec l’USTHB (Université des Sciences et de la Technologie Houari-Boumédiène) afin de déployer un tableau de bord sécurité suivant des KPI comme le taux de ponctualité, les incidents, et la charge en heures de pointe pendant le Ramadan et les jours de match au stade de Baraki; sur les réseaux sociaux, les réactions étaient contrastées — « On veut moins de retards et plus de sécurité », écrit Amina B., tandis que Mourad, commerçant près de la gare, craint que les travaux perturbent les livraisons — et, pour pousser ton pipeline POS/lemmatisation/NER dans ses retranchements, le même texte mélange des tokens “difficiles” comme un UUID 550e8400-e29b-41d4-a716-446655440000, un chemin Unix /var/log/nginx/access.log, des notations scientifiques 3.2e-4 et 2×10−3, des hashtags #DevOps et #IA, des handles @support et @team-lead, une URL https://example.org/api/v1/users?id=42, un e-mail prenom.nom+test@domaine.dz, une référence de ticket #A-9981, ainsi que des termes semi-techniques (SLA, logs, export CSV, RGPD) et de la ponctuation atypique (« », —, %, :, /, +) qui peuvent faire dérailler la normalisation si elle est trop agressive.\n",
      "                              Le  DT      lemma=le\n",
      "                              14  CD      lemma=14\n",
      "                         février  NNP     lemma=février\n",
      "                            2026  CD      lemma=2026\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               à  IN      lemma=à\n",
      "                              07  CD      lemma=07\n",
      "                               :  PUNCT   lemma=∅\n",
      "                              35  CD      lemma=35\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                     MaghrebRail  NNP     lemma=MaghrebRail\n",
      "                               a  VB      lemma=avoir\n",
      "                         annoncé  VB      lemma=annoncer\n",
      "                          depuis  IN      lemma=depuis\n",
      "                    Alger-Centre  NNP     lemma=Alger-Centre\n",
      "                             une  DT      lemma=un\n",
      "                        nouvelle  JJ      lemma=nouveau\n",
      "                           phase  NN      lemma=phase\n",
      "                              de  IN      lemma=de\n",
      "                   modernisation  NN      lemma=modernisation\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                           ligne  NN      lemma=ligne\n",
      "                              de  IN      lemma=de\n",
      "                        banlieue  NN      lemma=banlieue\n",
      "                         reliant  JJ      lemma=relier\n",
      "                             Bab  NNP     lemma=Bab\n",
      "                         Ezzouar  NNP     lemma=Ezzouar\n",
      "                               à  IN      lemma=à\n",
      "                           Blida  NNP     lemma=Blida\n",
      "                             via  IN      lemma=via\n",
      "                              El  NNP     lemma=El\n",
      "                         Harrach  NNP     lemma=Harrach\n",
      "                              et  CC      lemma=et\n",
      "                        Birtouta  NNP     lemma=Birtouta\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              en  IN      lemma=en\n",
      "                              s’  PRP     lemma=se\n",
      "                        appuyant  JJ      lemma=appuyer\n",
      "                             sur  IN      lemma=sur\n",
      "                             une  DT      lemma=un\n",
      "                            note  NN      lemma=note\n",
      "                         interne  NN      lemma=interne\n",
      "                          signée  NN      lemma=signé\n",
      "                             par  IN      lemma=par\n",
      "                           Samir  NNP     lemma=Samir\n",
      "                       Ould-Kaci  NNP     lemma=Ould-Kaci\n",
      "                               (  PUNCT   lemma=∅\n",
      "                           Chief  NNP     lemma=Chief\n",
      "                      Operations  NNP     lemma=Operations\n",
      "                         Officer  NNP     lemma=Officer\n",
      "                               )  PUNCT   lemma=∅\n",
      "                       indiquant  JJ      lemma=indiquer\n",
      "                              un  DT      lemma=un\n",
      "                       démarrage  NN      lemma=démarrage\n",
      "                              le  DT      lemma=le\n",
      "                               3  CD      lemma=3\n",
      "                            mars  NNP     lemma=mars\n",
      "                            2026  CD      lemma=2026\n",
      "                              et  CC      lemma=et\n",
      "                             une  DT      lemma=un\n",
      "                           durée  NN      lemma=durée\n",
      "                         estimée  NN      lemma=estimer\n",
      "                              de  IN      lemma=de\n",
      "                              18  CD      lemma=18\n",
      "                               à  IN      lemma=à\n",
      "                              24  CD      lemma=24\n",
      "                            mois  NN      lemma=mois\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            pour  IN      lemma=pour\n",
      "                              un  DT      lemma=un\n",
      "                          budget  NN      lemma=budget\n",
      "                              de  IN      lemma=de\n",
      "                             1,8  CD      lemma=1,8\n",
      "                        milliard  NN      lemma=milliard\n",
      "                             DZD  NNP     lemma=DZD\n",
      "                               (  PUNCT   lemma=∅\n",
      "                               ≈  NN      lemma=≈\n",
      "                            13,4  CD      lemma=13,4\n",
      "                               M  NN      lemma=mètre\n",
      "                               $  NN      lemma=$\n",
      "                               )  PUNCT   lemma=∅\n",
      "                            dont  PRP     lemma=dont\n",
      "                              35  CD      lemma=35\n",
      "                               %  PUNCT   lemma=∅\n",
      "                        financés  NN      lemma=financer\n",
      "                             par  IN      lemma=par\n",
      "                              un  DT      lemma=un\n",
      "                      consortium  NN      lemma=consortium\n",
      "                           privé  NN      lemma=privé\n",
      "                            mené  NN      lemma=mener\n",
      "                             par  IN      lemma=par\n",
      "                       NorthGate  NNP     lemma=NorthGate\n",
      "                  Infrastructure  NNP     lemma=infrastructure\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            avec  IN      lemma=avec\n",
      "                              l’  DT      lemma=le\n",
      "                           appui  NN      lemma=appui\n",
      "                              d’  IN      lemma=de\n",
      "                      ingénieurs  NN      lemma=ingénieur\n",
      "                           basés  NN      lemma=basé\n",
      "                               à  IN      lemma=à\n",
      "                            Lyon  NNP     lemma=Lyon\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           Milan  NNP     lemma=milan\n",
      "                              et  CC      lemma=et\n",
      "                        Hambourg  NNP     lemma=Hambourg\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              un  DT      lemma=un\n",
      "                     partenariat  NN      lemma=partenariat\n",
      "                              de  IN      lemma=de\n",
      "                       recherche  NN      lemma=recherche\n",
      "                            avec  IN      lemma=avec\n",
      "                              l’  DT      lemma=le\n",
      "                           USTHB  NNP     lemma=USTHB\n",
      "                               (  PUNCT   lemma=∅\n",
      "                      Université  NNP     lemma=université\n",
      "                             des  DT      lemma=un\n",
      "                        Sciences  NNP     lemma=science\n",
      "                              et  CC      lemma=et\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                     Technologie  NNP     lemma=technologie\n",
      "               Houari-Boumédiène  NNP     lemma=Houari-Boumédiène\n",
      "                               )  PUNCT   lemma=∅\n",
      "                            afin  NN      lemma=afin\n",
      "                              de  IN      lemma=de\n",
      "                        déployer  VB      lemma=déployer\n",
      "                              un  DT      lemma=un\n",
      "                         tableau  NN      lemma=tableau\n",
      "                              de  IN      lemma=de\n",
      "                            bord  NN      lemma=bord\n",
      "                        sécurité  NN      lemma=sécurité\n",
      "                         suivant  JJ      lemma=suivant\n",
      "                             des  DT      lemma=un\n",
      "                             KPI  NNP     lemma=KPI\n",
      "                           comme  NN      lemma=comme\n",
      "                              le  DT      lemma=le\n",
      "                            taux  JJ      lemma=taux\n",
      "                              de  IN      lemma=de\n",
      "                     ponctualité  NN      lemma=ponctualité\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             les  DT      lemma=le\n",
      "                       incidents  JJ      lemma=incident\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              la  DT      lemma=le\n",
      "                          charge  NN      lemma=charge\n",
      "                              en  IN      lemma=en\n",
      "                          heures  NN      lemma=heure\n",
      "                              de  IN      lemma=de\n",
      "                          pointe  NN      lemma=pointe\n",
      "                         pendant  IN      lemma=pendant\n",
      "                              le  DT      lemma=le\n",
      "                         Ramadan  NNP     lemma=ramadan\n",
      "                              et  CC      lemma=et\n",
      "                             les  DT      lemma=le\n",
      "                           jours  NN      lemma=jour\n",
      "                              de  IN      lemma=de\n",
      "                           match  NN      lemma=match\n",
      "                              au  DT      lemma=au\n",
      "                           stade  NN      lemma=stade\n",
      "                              de  IN      lemma=de\n",
      "                          Baraki  NNP     lemma=Baraki\n",
      "                               ;  PUNCT   lemma=∅\n",
      "                             sur  IN      lemma=sur\n",
      "                             les  DT      lemma=le\n",
      "                         réseaux  JJ      lemma=réseau\n",
      "                         sociaux  JJ      lemma=social\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             les  DT      lemma=le\n",
      "                       réactions  NN      lemma=réaction\n",
      "                         étaient  VB      lemma=être\n",
      "                     contrastées  VB      lemma=contrasté\n",
      "                               —  PUNCT   lemma=∅\n",
      "                               «  PUNCT   lemma=∅\n",
      "                              On  PRP     lemma=on\n",
      "                            veut  VB      lemma=vouloir\n",
      "                           moins  RB      lemma=moins\n",
      "                              de  IN      lemma=de\n",
      "                         retards  NN      lemma=retard\n",
      "                              et  CC      lemma=et\n",
      "                            plus  RB      lemma=plus\n",
      "                              de  IN      lemma=de\n",
      "                        sécurité  NN      lemma=sécurité\n",
      "                               »  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           écrit  VB      lemma=écrit\n",
      "                           Amina  NNP     lemma=Amina\n",
      "                               B  NN      lemma=B\n",
      "                               .  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          tandis  NN      lemma=tandis\n",
      "                             que  CC      lemma=que\n",
      "                          Mourad  NNP     lemma=Mourad\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                      commerçant  JJ      lemma=commerçant\n",
      "                            près  NN      lemma=près\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                            gare  VB      lemma=gare\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          craint  NN      lemma=craindre\n",
      "                             que  CC      lemma=que\n",
      "                             les  DT      lemma=le\n",
      "                         travaux  JJ      lemma=travail\n",
      "                      perturbent  NN      lemma=perturber\n",
      "                             les  DT      lemma=le\n",
      "                      livraisons  NN      lemma=livraison\n",
      "                               —  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            pour  IN      lemma=pour\n",
      "                         pousser  VB      lemma=pousser\n",
      "                             ton  DT      lemma=ton\n",
      "                        pipeline  NN      lemma=pipeline\n",
      "           POS/lemmatisation/NER  NNP     lemma=POS/lemmatisation/NER\n",
      "                            dans  IN      lemma=dans\n",
      "                             ses  DT      lemma=son\n",
      "                  retranchements  JJ      lemma=retranchement\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              le  DT      lemma=le\n",
      "                            même  NN      lemma=même\n",
      "                           texte  NN      lemma=texte\n",
      "                         mélange  NN      lemma=mélange\n",
      "                             des  DT      lemma=un\n",
      "                          tokens  NN      lemma=tokens\n",
      "                               “  NN      lemma=“\n",
      "                      difficiles  NN      lemma=difficile\n",
      "                               ”  NN      lemma=”\n",
      "                           comme  NN      lemma=comme\n",
      "                              un  DT      lemma=un\n",
      "                            UUID  NNP     lemma=UUID\n",
      "                               5  CD      lemma=5\n",
      "                               5  CD      lemma=5\n",
      "                               0  CD      lemma=0\n",
      "                               e  NN      lemma=e\n",
      "                               8  CD      lemma=8\n",
      "                               4  CD      lemma=4\n",
      "                               0  CD      lemma=0\n",
      "                               0  CD      lemma=0\n",
      "                               -  PUNCT   lemma=∅\n",
      "     e29b-41d4-a716-446655440000  NNP     lemma=e29b-41d4-a716-446655440000\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              un  DT      lemma=un\n",
      "                          chemin  NN      lemma=chemin\n",
      "                            Unix  NNP     lemma=Unix\n",
      "                               /  PUNCT   lemma=∅\n",
      "            var/log/nginx/access  NNP     lemma=var/log/nginx/access\n",
      "                               .  PUNCT   lemma=∅\n",
      "                             log  NN      lemma=log\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                       notations  NN      lemma=notation\n",
      "                   scientifiques  NN      lemma=scientifique\n",
      "                               3  CD      lemma=3\n",
      "                               .  PUNCT   lemma=∅\n",
      "                               2  CD      lemma=2\n",
      "                               e  NN      lemma=e\n",
      "                               -  PUNCT   lemma=∅\n",
      "                               4  CD      lemma=4\n",
      "                              et  CC      lemma=et\n",
      "                               2  CD      lemma=2\n",
      "                               ×  NN      lemma=×\n",
      "                              10  CD      lemma=10\n",
      "                               −  NN      lemma=−\n",
      "                               3  CD      lemma=3\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                        hashtags  NN      lemma=hashtags\n",
      "                               #  NN      lemma=#\n",
      "                          DevOps  NNP     lemma=DevOps\n",
      "                              et  CC      lemma=et\n",
      "                               #  NN      lemma=#\n",
      "                              IA  NNP     lemma=IA\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             des  DT      lemma=un\n",
      "                         handles  NN      lemma=handles\n",
      "                               @  PUNCT   lemma=∅\n",
      "                         support  NN      lemma=support\n",
      "                              et  CC      lemma=et\n",
      "                               @  PUNCT   lemma=∅\n",
      "                       team-lead  NNP     lemma=team-lead\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             une  DT      lemma=un\n",
      "                             URL  NNP     lemma=URL\n",
      "https://example.org/api/v1/users?id=42  NNP     lemma=https://example.org/api/v1/users?id=42\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                              un  DT      lemma=un\n",
      "                          e-mail  NNP     lemma=e-mail\n",
      "      prenom.nom+test@domaine.dz  NNP     lemma=prenom.nom+test@domaine.dz\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                             une  DT      lemma=un\n",
      "                       référence  NN      lemma=référence\n",
      "                              de  IN      lemma=de\n",
      "                          ticket  NN      lemma=ticket\n",
      "                               #  NN      lemma=#\n",
      "                          A-9981  NNP     lemma=A-9981\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                           ainsi  RB      lemma=ainsi\n",
      "                             que  CC      lemma=que\n",
      "                             des  DT      lemma=un\n",
      "                          termes  NN      lemma=terme\n",
      "                 semi-techniques  NNP     lemma=semi-techniques\n",
      "                               (  PUNCT   lemma=∅\n",
      "                             SLA  NNP     lemma=SLA\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            logs  NN      lemma=logs\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                          export  NN      lemma=export\n",
      "                             CSV  NNP     lemma=CSV\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                            RGPD  NNP     lemma=RGPD\n",
      "                               )  PUNCT   lemma=∅\n",
      "                              et  CC      lemma=et\n",
      "                              de  IN      lemma=de\n",
      "                              la  DT      lemma=le\n",
      "                     ponctuation  NN      lemma=ponctuation\n",
      "                        atypique  JJ      lemma=atypique\n",
      "                               (  PUNCT   lemma=∅\n",
      "                               «  PUNCT   lemma=∅\n",
      "                               »  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               —  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               %  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               :  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               /  PUNCT   lemma=∅\n",
      "                               ,  PUNCT   lemma=∅\n",
      "                               +  PUNCT   lemma=∅\n",
      "                               )  PUNCT   lemma=∅\n",
      "                             qui  PRP     lemma=qui\n",
      "                         peuvent  VB      lemma=pouvoir\n",
      "                           faire  VB      lemma=faire\n",
      "                       dérailler  VB      lemma=dérailler\n",
      "                              la  DT      lemma=le\n",
      "                   normalisation  NN      lemma=normalisation\n",
      "                              si  NN      lemma=si\n",
      "                            elle  PRP     lemma=elle\n",
      "                             est  VB      lemma=être\n",
      "                            trop  RB      lemma=trop\n",
      "                       agressive  JJ      lemma=agressive\n",
      "                               .  PUNCT   lemma=∅\n",
      "\n",
      "NER (IA (transformers): Davlan/bert-base-multilingual-cased-ner-hrl) (token, label):\n",
      "[('Le', 'O'), ('14', 'B-DATE'), ('février', 'I-DATE'), ('2026', 'I-DATE'), (',', 'O'), ('à', 'O'), ('07', 'O'), (':', 'O'), ('35', 'O'), (',', 'O'), ('MaghrebRail', 'B-ORG'), ('a', 'O'), ('annoncé', 'O'), ('depuis', 'O'), ('Alger-Centre', 'B-LOC'), ('une', 'O'), ('nouvelle', 'O'), ('phase', 'O'), ('de', 'O'), ('modernisation', 'O'), ('de', 'O'), ('la', 'O'), ('ligne', 'O'), ('de', 'O'), ('banlieue', 'O'), ('reliant', 'O'), ('Bab', 'B-LOC'), ('Ezzouar', 'I-LOC'), ('à', 'O'), ('Blida', 'B-LOC'), ('via', 'O'), ('El', 'B-LOC'), ('Harrach', 'I-LOC'), ('et', 'O'), ('Birtouta', 'B-LOC'), (',', 'O'), ('en', 'O'), ('s’', 'O'), ('appuyant', 'O'), ('sur', 'O'), ('une', 'O'), ('note', 'O'), ('interne', 'O'), ('signée', 'O'), ('par', 'O'), ('Samir', 'B-PERS'), ('Ould-Kaci', 'I-PERS'), ('(', 'O'), ('Chief', 'O'), ('Operations', 'O'), ('Officer', 'O'), (')', 'O'), ('indiquant', 'O'), ('un', 'O'), ('démarrage', 'O'), ('le', 'O'), ('3', 'B-DATE'), ('mars', 'I-DATE'), ('2026', 'I-DATE'), ('et', 'O'), ('une', 'O'), ('durée', 'O'), ('estimée', 'O'), ('de', 'O'), ('18', 'O'), ('à', 'O'), ('24', 'O'), ('mois', 'O'), (',', 'O'), ('pour', 'O'), ('un', 'O'), ('budget', 'O'), ('de', 'O'), ('1,8', 'O'), ('milliard', 'O'), ('DZD', 'O'), ('(', 'O'), ('≈', 'O'), ('13,4', 'O'), ('M', 'O'), ('$', 'O'), (')', 'O'), ('dont', 'O'), ('35', 'O'), ('%', 'O'), ('financés', 'O'), ('par', 'O'), ('un', 'O'), ('consortium', 'O'), ('privé', 'O'), ('mené', 'O'), ('par', 'O'), ('NorthGate', 'B-ORG'), ('Infrastructure', 'I-ORG'), (',', 'O'), ('avec', 'O'), ('l’', 'O'), ('appui', 'O'), ('d’', 'O'), ('ingénieurs', 'O'), ('basés', 'O'), ('à', 'O'), ('Lyon', 'B-LOC'), (',', 'O'), ('Milan', 'B-LOC'), ('et', 'O'), ('Hambourg', 'B-LOC'), (',', 'O'), ('et', 'O'), ('un', 'O'), ('partenariat', 'O'), ('de', 'O'), ('recherche', 'O'), ('avec', 'O'), ('l’', 'O'), ('USTHB', 'B-ORG'), ('(', 'O'), ('Université', 'B-ORG'), ('des', 'I-ORG'), ('Sciences', 'I-ORG'), ('et', 'I-ORG'), ('de', 'I-ORG'), ('la', 'I-ORG'), ('Technologie', 'I-ORG'), ('Houari-Boumédiène', 'I-ORG'), (')', 'O'), ('afin', 'O'), ('de', 'O'), ('déployer', 'O'), ('un', 'O'), ('tableau', 'O'), ('de', 'O'), ('bord', 'O'), ('sécurité', 'O'), ('suivant', 'O'), ('des', 'O'), ('KPI', 'O'), ('comme', 'O'), ('le', 'O'), ('taux', 'O'), ('de', 'O'), ('ponctualité', 'O'), (',', 'O'), ('les', 'O'), ('incidents', 'O'), (',', 'O'), ('et', 'O'), ('la', 'O'), ('charge', 'O'), ('en', 'O'), ('heures', 'O'), ('de', 'O'), ('pointe', 'O'), ('pendant', 'O'), ('le', 'O'), ('Ramadan', 'O'), ('et', 'O'), ('les', 'O'), ('jours', 'O'), ('de', 'O'), ('match', 'O'), ('au', 'O'), ('stade', 'O'), ('de', 'O'), ('Baraki', 'B-LOC'), (';', 'O'), ('sur', 'O'), ('les', 'O'), ('réseaux', 'O'), ('sociaux', 'O'), (',', 'O'), ('les', 'O'), ('réactions', 'O'), ('étaient', 'O'), ('contrastées', 'O'), ('—', 'O'), ('«', 'O'), ('On', 'O'), ('veut', 'O'), ('moins', 'O'), ('de', 'O'), ('retards', 'O'), ('et', 'O'), ('plus', 'O'), ('de', 'O'), ('sécurité', 'O'), ('»', 'O'), (',', 'O'), ('écrit', 'O'), ('Amina', 'B-PERS'), ('B', 'I-PERS'), ('.', 'I-PERS'), (',', 'O'), ('tandis', 'O'), ('que', 'O'), ('Mourad', 'B-PERS'), (',', 'O'), ('commerçant', 'O'), ('près', 'O'), ('de', 'O'), ('la', 'O'), ('gare', 'O'), (',', 'O'), ('craint', 'O'), ('que', 'O'), ('les', 'O'), ('travaux', 'O'), ('perturbent', 'O'), ('les', 'O'), ('livraisons', 'O'), ('—', 'O'), ('et', 'O'), (',', 'O'), ('pour', 'O'), ('pousser', 'O'), ('ton', 'O'), ('pipeline', 'O'), ('POS/lemmatisation/NER', 'O'), ('dans', 'O'), ('ses', 'O'), ('retranchements', 'O'), (',', 'O'), ('le', 'O'), ('même', 'O'), ('texte', 'O'), ('mélange', 'O'), ('des', 'O'), ('tokens', 'O'), ('“', 'O'), ('difficiles', 'O'), ('”', 'O'), ('comme', 'O'), ('un', 'O'), ('UUID', 'O'), ('5', 'O'), ('5', 'O'), ('0', 'O'), ('e', 'O'), ('8', 'O'), ('4', 'O'), ('0', 'O'), ('0', 'O'), ('-', 'O'), ('e29b-41d4-a716-446655440000', 'O'), (',', 'O'), ('un', 'O'), ('chemin', 'O'), ('Unix', 'O'), ('/', 'O'), ('var/log/nginx/access', 'O'), ('.', 'O'), ('log', 'O'), (',', 'O'), ('des', 'O'), ('notations', 'O'), ('scientifiques', 'O'), ('3', 'O'), ('.', 'O'), ('2', 'O'), ('e', 'O'), ('-', 'O'), ('4', 'O'), ('et', 'O'), ('2', 'O'), ('×', 'O'), ('10', 'O'), ('−', 'O'), ('3', 'O'), (',', 'O'), ('des', 'O'), ('hashtags', 'O'), ('#', 'O'), ('DevOps', 'O'), ('et', 'O'), ('#', 'O'), ('IA', 'O'), (',', 'O'), ('des', 'O'), ('handles', 'O'), ('@', 'O'), ('support', 'O'), ('et', 'O'), ('@', 'O'), ('team-lead', 'O'), (',', 'O'), ('une', 'O'), ('URL', 'O'), ('https://example.org/api/v1/users?id=42', 'O'), (',', 'O'), ('un', 'O'), ('e-mail', 'O'), ('prenom.nom+test@domaine.dz', 'O'), (',', 'O'), ('une', 'O'), ('référence', 'O'), ('de', 'O'), ('ticket', 'O'), ('#', 'O'), ('A-9981', 'O'), (',', 'O'), ('ainsi', 'O'), ('que', 'O'), ('des', 'O'), ('termes', 'O'), ('semi-techniques', 'O'), ('(', 'O'), ('SLA', 'O'), (',', 'O'), ('logs', 'O'), (',', 'O'), ('export', 'O'), ('CSV', 'O'), (',', 'O'), ('RGPD', 'O'), (')', 'O'), ('et', 'O'), ('de', 'O'), ('la', 'O'), ('ponctuation', 'O'), ('atypique', 'O'), ('(', 'O'), ('«', 'O'), ('»', 'O'), (',', 'O'), ('—', 'O'), (',', 'O'), ('%', 'O'), (',', 'O'), (':', 'O'), (',', 'O'), ('/', 'O'), (',', 'O'), ('+', 'O'), (')', 'O'), ('qui', 'O'), ('peuvent', 'O'), ('faire', 'O'), ('dérailler', 'O'), ('la', 'O'), ('normalisation', 'O'), ('si', 'O'), ('elle', 'O'), ('est', 'O'), ('trop', 'O'), ('agressive', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: 14 février 2026\n",
      "  DATE: 3 mars 2026\n",
      "  LOC: Alger-Centre\n",
      "  LOC: Bab Ezzouar\n",
      "  LOC: Blida\n",
      "  LOC: El Harrach\n",
      "  LOC: Birtouta\n",
      "  LOC: Lyon\n",
      "  LOC: Milan\n",
      "  LOC: Hambourg\n",
      "  LOC: Baraki\n",
      "  ORG: MaghrebRail\n",
      "  ORG: NorthGate Infrastructure\n",
      "  ORG: USTHB\n",
      "  ORG: Université des Sciences et de la Technologie Houari-Boumédiène\n",
      "  PERS: Samir Ould-Kaci\n",
      "  PERS: Amina B.\n",
      "  PERS: Mourad\n",
      "\n",
      "========================================================================================================================\n",
      "RUN AR (arabcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=0 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: في 14 فبراير 2026 عند الساعة 07:35 صباحًا، أعلنت شركة مغرب ريل من مقرها في الجزائر-الوسط عن برنامج لتحديث الخط الذي يربط باب الزوار بالبليدة مرورًا بالحراش وبئر توتة. وصرّح المدير التنفيذي سمير ولد القاضي بأن الأشغال ستنطلق في 3 مارس 2026 وقد تمتد بين 18 و24 شهرًا، بميزانية تقديرية تبلغ 1.8 مليار دينار جزائري، منها 35% ممولة عبر تحالف خاص تقوده NorthGate Infrastructure وبمشاركة مقاولات محلية. وأشار البيان إلى تعاون تقني مع خبراء من ليون وميلانو وهامبورغ، وإلى مساهمة مخبر بحثي في جامعة هواري بومدين. وبينما رحّبت جمعيات للمسافرين بتحسين السلامة وتقليل التأخيرات، عبّر تجار قرب المحطات عن خشيتهم من اضطراب التموين خلال رمضان وأيام المباريات في ملعب براقي، خصوصًا مع ارتفاع الضغط في ساعات الذروة.\n",
      "            في  IN      lemma=في\n",
      "            14  CD      lemma=14\n",
      "        فبراير  NNP     lemma=فِبْرايِر\n",
      "          2026  CD      lemma=2026\n",
      "           عند  IN      lemma=عند\n",
      "        الساعة  NN      lemma=ساعَة\n",
      "            07  CD      lemma=07\n",
      "             :  PUNCT   lemma=∅\n",
      "            35  CD      lemma=35\n",
      "         صباحا  NN      lemma=صُباح\n",
      "             ،  PUNCT   lemma=∅\n",
      "         أعلنت  VB      lemma=أَعْلَن\n",
      "          شركة  NN      lemma=شِرْك\n",
      "          مغرب  NNP     lemma=مَغْرِب\n",
      "           ريل  VB      lemma=رَيَّل\n",
      "            من  IN      lemma=من\n",
      "           مقر  NN      lemma=مَقَرّ\n",
      "            ها  PRP     lemma=ها\n",
      "            في  IN      lemma=في\n",
      "       الجزائر  NNP     lemma=جَزائِر\n",
      "             -  PUNCT   lemma=∅\n",
      "         الوسط  NN      lemma=وَسْط\n",
      "            عن  IN      lemma=عن\n",
      "             ب  IN      lemma=ب\n",
      "         رنامج  NNP     lemma=رنامج\n",
      "             ل  IN      lemma=ل\n",
      "         تحديث  NN      lemma=تَحْدِيث\n",
      "          الخط  NN      lemma=خَطّ\n",
      "          الذي  WDT     lemma=الذي\n",
      "          يربط  VB      lemma=رَبَط\n",
      "             ب  IN      lemma=ب\n",
      "            اب  NNP     lemma=آب\n",
      "        الزوار  NN      lemma=زائِر\n",
      "             ب  IN      lemma=ب\n",
      "       البليدة  NN      lemma=بَلِيد\n",
      "         مرورا  NN      lemma=مُرُور\n",
      "             ب  IN      lemma=ب\n",
      "        الحراش  NNP     lemma=الحراش\n",
      "          وبئر  NN      lemma=بِئْر\n",
      "          توتة  NNP     lemma=تُوت\n",
      "             .  PUNCT   lemma=∅\n",
      "          وصرح  VB      lemma=صَرَح\n",
      "        المدير  NN      lemma=مُدِير\n",
      "       التنفيذ  NN      lemma=تَنْفِيذ\n",
      "             ي  PRP     lemma=ي\n",
      "          سمير  NNP     lemma=سَمِير\n",
      "           ولد  VB      lemma=وَلَد\n",
      "         القاض  NNP     lemma=القاض\n",
      "             ي  PRP     lemma=ي\n",
      "             ب  IN      lemma=ب\n",
      "            أن  NN      lemma=أَنْ\n",
      "       الأشغال  NN      lemma=شُغْل\n",
      "             س  RP      lemma=س\n",
      "         تنطلق  VB      lemma=ٱِنْطَلَق\n",
      "            في  IN      lemma=في\n",
      "             3  CD      lemma=3\n",
      "          مارس  NNP     lemma=مارِس\n",
      "          2026  CD      lemma=2026\n",
      "           وقد  VB      lemma=وَقَد\n",
      "          تمتد  VB      lemma=ٱِمْتَدّ\n",
      "           بين  IN      lemma=بين\n",
      "            18  CD      lemma=18\n",
      "             و  CC      lemma=و\n",
      "            24  CD      lemma=24\n",
      "          شهرا  NN      lemma=شَهْر\n",
      "             ،  PUNCT   lemma=∅\n",
      "             ب  IN      lemma=ب\n",
      "       ميزانية  NN      lemma=مِيزان\n",
      "       تقديرية  NN      lemma=تَقْدِير\n",
      "          تبلغ  VB      lemma=بَلُغ\n",
      "             1  CD      lemma=1\n",
      "             .  PUNCT   lemma=∅\n",
      "             8  CD      lemma=8\n",
      "         مليار  NN      lemma=مِلْيار\n",
      "         دينار  NN      lemma=دِينار\n",
      "         جزائر  NNP     lemma=جَزائِر\n",
      "             ي  PRP     lemma=ي\n",
      "             ،  PUNCT   lemma=∅\n",
      "            من  IN      lemma=مِن\n",
      "            ها  PRP     lemma=ها\n",
      "            35  CD      lemma=35\n",
      "             %  PUNCT   lemma=∅\n",
      "         ممولة  NN      lemma=مُمَوِّل\n",
      "           عبر  IN      lemma=عبر\n",
      "         تحالف  VB      lemma=تَحالَف\n",
      "           خاص  JJ      lemma=خاصّ\n",
      "          تقود  VB      lemma=قاد\n",
      "             ه  PRP     lemma=ه\n",
      "     NorthGate  NN      lemma=NorthGate\n",
      "Infrastructure  NN      lemma=Infrastructure\n",
      "      وبمشاركة  NN      lemma=مُشارَكَة\n",
      "       مقاولات  NN      lemma=مُقاوِل\n",
      "         محلية  NN      lemma=مَحَلّ\n",
      "             .  PUNCT   lemma=∅\n",
      "             و  CC      lemma=و\n",
      "          أشار  VB      lemma=أَشار\n",
      "        البيان  NN      lemma=بَيان\n",
      "           إلى  IN      lemma=إلى\n",
      "         تعاون  VB      lemma=تَعاوَن\n",
      "            تق  VB      lemma=وَقَى\n",
      "            ني  PRP     lemma=ني\n",
      "            مع  IN      lemma=مع\n",
      "         خبراء  NN      lemma=خَبِير\n",
      "            من  IN      lemma=من\n",
      "          ليون  NNP     lemma=لِيُون\n",
      "       وميلانو  NNP     lemma=مِيلانُو\n",
      "      وهامبورغ  NNP     lemma=هامْبُورْغ\n",
      "             ،  PUNCT   lemma=∅\n",
      "             و  CC      lemma=و\n",
      "           إلى  IN      lemma=إِلَى\n",
      "        مساهمة  NN      lemma=مُساهَمَة\n",
      "          مخبر  NN      lemma=مَخْبَر\n",
      "             ب  IN      lemma=ب\n",
      "            حث  VB      lemma=حَثّ\n",
      "             ي  PRP     lemma=ي\n",
      "            في  IN      lemma=في\n",
      "         جامعة  NN      lemma=جامِع\n",
      "          هوار  NNP     lemma=هوار\n",
      "             ي  PRP     lemma=ي\n",
      "             ب  IN      lemma=ب\n",
      "         ومدين  NN      lemma=وَمِد\n",
      "             .  PUNCT   lemma=∅\n",
      "        وبينما  NN      lemma=بَيْنَما\n",
      "          رحبت  VB      lemma=رَحِب\n",
      "        جمعيات  JJ      lemma=جَمْعِيّ\n",
      "             ل  IN      lemma=ل\n",
      "      لمسافرين  NN      lemma=مُسافِر\n",
      "             ب  IN      lemma=ب\n",
      "         تحسين  VB      lemma=حَسِس\n",
      "       السلامة  NN      lemma=سَلامَة\n",
      "             و  CC      lemma=و\n",
      "         تقليل  NN      lemma=تَقْلِيل\n",
      "     التأخيرات  NN      lemma=تَأْخِير\n",
      "             ،  PUNCT   lemma=∅\n",
      "          عبّر  IN      lemma=عبر\n",
      "          تجار  VB      lemma=أَجار\n",
      "           قرب  VB      lemma=قَرِب\n",
      "       المحطات  NN      lemma=مَحَطَّة\n",
      "            عن  IN      lemma=عن\n",
      "          خشيت  VB      lemma=خَشِي\n",
      "            هم  PRP     lemma=هم\n",
      "            من  IN      lemma=من\n",
      "        اضطراب  NN      lemma=ٱِضْطِراب\n",
      "       التموين  NN      lemma=تَمْوِين\n",
      "          خلال  IN      lemma=خلال\n",
      "         رمضان  NNP     lemma=رَمَضان\n",
      "             و  CC      lemma=و\n",
      "          أيام  NN      lemma=يَوْم\n",
      "     المباريات  NN      lemma=مُباراة\n",
      "            في  IN      lemma=في\n",
      "          ملعب  NN      lemma=مَلْعَب\n",
      "             ب  IN      lemma=ب\n",
      "           راق  VB      lemma=راق\n",
      "             ي  PRP     lemma=ي\n",
      "             ،  PUNCT   lemma=∅\n",
      "         خصوصا  RB      lemma=خُصُوص\n",
      "            مع  IN      lemma=مع\n",
      "        ارتفاع  NN      lemma=ٱِرْتِفاع\n",
      "         الضغط  NN      lemma=أُضْغُط\n",
      "            في  IN      lemma=في\n",
      "         ساعات  NN      lemma=ساعَة\n",
      "        الذروة  NN      lemma=ذُرْوَة\n",
      "             .  PUNCT   lemma=∅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\moura\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('في', 'O'), ('14', 'O'), ('فبراير', 'O'), ('2026', 'B-DATE'), ('عند', 'O'), ('الساعة', 'O'), ('07', 'O'), (':', 'O'), ('35', 'O'), ('صباحًا', 'O'), ('،', 'O'), ('أعلنت', 'O'), ('شركة', 'O'), ('مغرب', 'B-ORG'), ('ريل', 'I-ORG'), ('من', 'O'), ('مقرها', 'O'), ('في', 'O'), ('الجزائر', 'B-LOC'), ('-', 'O'), ('الوسط', 'B-LOC'), ('عن', 'O'), ('برنامج', 'O'), ('لتحديث', 'O'), ('الخط', 'O'), ('الذي', 'O'), ('يربط', 'O'), ('باب', 'B-LOC'), ('الزوار', 'I-LOC'), ('بالبليدة', 'B-LOC'), ('مرورًا', 'O'), ('بالحراش', 'B-LOC'), ('وبئر', 'B-LOC'), ('توتة', 'I-LOC'), ('.', 'O'), ('وصرّح', 'O'), ('المدير', 'O'), ('التنفيذي', 'O'), ('سمير', 'B-PERS'), ('ولد', 'I-PERS'), ('القاضي', 'I-PERS'), ('بأن', 'O'), ('الأشغال', 'O'), ('ستنطلق', 'O'), ('في', 'O'), ('3', 'O'), ('مارس', 'O'), ('2026', 'B-DATE'), ('وقد', 'O'), ('تمتد', 'O'), ('بين', 'O'), ('18', 'O'), ('و', 'O'), ('24', 'O'), ('شهرًا', 'O'), ('،', 'O'), ('بميزانية', 'O'), ('تقديرية', 'O'), ('تبلغ', 'O'), ('1', 'O'), ('.', 'O'), ('8', 'O'), ('مليار', 'O'), ('دينار', 'B-MISC'), ('جزائري', 'O'), ('،', 'O'), ('منها', 'O'), ('35', 'O'), ('%', 'O'), ('ممولة', 'O'), ('عبر', 'O'), ('تحالف', 'O'), ('خاص', 'O'), ('تقوده', 'O'), ('NorthGate', 'B-ORG'), ('Infrastructure', 'I-ORG'), ('وبمشاركة', 'O'), ('مقاولات', 'O'), ('محلية', 'O'), ('.', 'O'), ('وأشار', 'O'), ('البيان', 'O'), ('إلى', 'O'), ('تعاون', 'O'), ('تقني', 'O'), ('مع', 'O'), ('خبراء', 'O'), ('من', 'O'), ('ليون', 'B-LOC'), ('وميلانو', 'B-LOC'), ('وهامبورغ', 'B-LOC'), ('،', 'O'), ('وإلى', 'O'), ('مساهمة', 'O'), ('مخبر', 'O'), ('بحثي', 'O'), ('في', 'O'), ('جامعة', 'B-ORG'), ('هواري', 'B-ORG'), ('بومدين', 'I-ORG'), ('.', 'O'), ('وبينما', 'O'), ('رحّبت', 'O'), ('جمعيات', 'O'), ('للمسافرين', 'O'), ('بتحسين', 'O'), ('السلامة', 'O'), ('وتقليل', 'O'), ('التأخيرات', 'O'), ('،', 'O'), ('عبّر', 'O'), ('تجار', 'O'), ('قرب', 'O'), ('المحطات', 'O'), ('عن', 'O'), ('خشيتهم', 'O'), ('من', 'O'), ('اضطراب', 'O'), ('التموين', 'O'), ('خلال', 'O'), ('رمضان', 'O'), ('وأيام', 'O'), ('المباريات', 'O'), ('في', 'O'), ('ملعب', 'O'), ('براقي', 'B-MISC'), ('،', 'O'), ('خصوصًا', 'O'), ('مع', 'O'), ('ارتفاع', 'O'), ('الضغط', 'O'), ('في', 'O'), ('ساعات', 'O'), ('الذروة', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: 2026\n",
      "  ORG: مغرب ريل\n",
      "  LOC: الجزائر\n",
      "  LOC: الوسط\n",
      "  LOC: باب الزوار\n",
      "  LOC: بالبليدة\n",
      "  LOC: الحراش\n",
      "  LOC: وبئر توتة\n",
      "  PERS: سمير ولد القاضي\n",
      "  DATE: 2026\n",
      "  MISC: دينار\n",
      "  ORG: NorthGate Infrastructure\n",
      "  LOC: ليون\n",
      "  LOC: وميلانو\n",
      "  LOC: وهامبورغ\n",
      "  ORG: جامعة\n",
      "  ORG: هواري بومدين\n",
      "  MISC: براقي\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=89 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 10/89 = 11.24 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=1 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: استنادًا إلى المادة 7.2 من النظام الداخلي المُحدّث بتاريخ 01/09/2025، يُشترط أن يكون أي وصول إلى البيانات ذات الطابع الشخصي مبرّرًا ومُوثّقًا ومحدودًا بالحد الأدنى الضروري، بما في ذلك العمليات التي تُجرى لأغراض الصيانة التصحيحية أو الاستجابة للحوادث. ويُذكَّر بأن سجلات التطبيقات، ونسخ الحفظ التزايدية، وملفات التصدير بصيغة CSV المُعدّة للتدقيق قد تتضمن معرّفات حساسة مثل رقم المستخدم، عنوان البريد الإلكتروني، أو رقم الهاتف، ولذلك تخضع لالتزامات السرية. وفي حال ورود طلب من طرف ثالث (عميل، شريك، أو متعاقد من الباطن)، فإن تسليم أي مقتطف من البيانات يتطلب موافقة مديرية نظم المعلومات والتحقق من خلية الامتثال، ما لم تكن هناك حالة طارئة موثّقة. كما يجب الإبلاغ عن أي سلوك غير اعتيادي خلال 24 ساعة كحد أقصى، مع تقرير يحدد التاريخ والوقت والأنظمة المعنية والسبب المحتمل والإجراءات التصحيحية.\n",
      "     استنادا  RB      lemma=ٱِسْتِناداً\n",
      "         إلى  IN      lemma=إلى\n",
      "      المادة  NN      lemma=مادَّة\n",
      "           7  CD      lemma=7\n",
      "           .  PUNCT   lemma=∅\n",
      "           2  CD      lemma=2\n",
      "          من  IN      lemma=من\n",
      "      النظام  NN      lemma=نِظام\n",
      "      الداخل  NN      lemma=داخِل\n",
      "           ي  PRP     lemma=ي\n",
      "      المحدث  JJ      lemma=مُحْدَث\n",
      "           ب  IN      lemma=ب\n",
      "       تاريخ  NN      lemma=تَأْرِيخ\n",
      "          01  CD      lemma=01\n",
      "           /  PUNCT   lemma=∅\n",
      "          09  CD      lemma=09\n",
      "           /  PUNCT   lemma=∅\n",
      "        2025  CD      lemma=2025\n",
      "           ،  PUNCT   lemma=∅\n",
      "       يشترط  VB      lemma=ٱِشْتَرَط\n",
      "          أن  RP      lemma=أن\n",
      "        يكون  VB      lemma=كان\n",
      "          أي  CC      lemma=أَيْ\n",
      "        وصول  NN      lemma=وُصُول\n",
      "         إلى  IN      lemma=إلى\n",
      "    البيانات  NN      lemma=بَيان\n",
      "         ذات  NN      lemma=ذات\n",
      "      الطابع  NN      lemma=طابِع\n",
      "       الشخص  NN      lemma=شَخْص\n",
      "           ي  PRP     lemma=ي\n",
      "       مبررا  NN      lemma=مُبَرِّر\n",
      "      وموثقا  NN      lemma=مَوْثِق\n",
      "     ومحدودا  JJ      lemma=مَحْدُود\n",
      "           ب  IN      lemma=ب\n",
      "        الحد  VB      lemma=أَلْحَد\n",
      "      الأدنى  NN      lemma=أَدْنَى\n",
      "      الضرور  NNP     lemma=الضرور\n",
      "           ي  PRP     lemma=ي\n",
      "           ،  PUNCT   lemma=∅\n",
      "         بما  NN      lemma=ما\n",
      "          في  IN      lemma=في\n",
      "         ذلك  PRP     lemma=ذلك\n",
      "    العمليات  NN      lemma=عَمَلِيَّة\n",
      "        التي  WDT     lemma=التي\n",
      "        تجرى  VB      lemma=جَرَى\n",
      "           ل  IN      lemma=ل\n",
      "       أغراض  NN      lemma=غَرَض\n",
      "     الصيانة  NN      lemma=صِيانَة\n",
      "   التصحيحية  JJ      lemma=تَصْحِيحِيّ\n",
      "          أو  CC      lemma=أو\n",
      "   الاستجابة  NN      lemma=ٱِسْتِجابَة\n",
      "           ل  IN      lemma=ل\n",
      "      لحوادث  NN      lemma=حادِث\n",
      "           .  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "        يذكر  VB      lemma=ذَكَر\n",
      "           ب  IN      lemma=ب\n",
      "          أن  NN      lemma=أَنْ\n",
      "       سجلات  NN      lemma=سِجِلّ\n",
      "   التطبيقات  NN      lemma=تَطْبِيق\n",
      "           ،  PUNCT   lemma=∅\n",
      "        ونسخ  VB      lemma=نَسَخ\n",
      "       الحفظ  NN      lemma=حِفْظ\n",
      "   التزايدية  NNP     lemma=التزايديه\n",
      "           ،  PUNCT   lemma=∅\n",
      "      وملفات  NN      lemma=مِلَفَّة\n",
      "     التصدير  NN      lemma=تَصْدِير\n",
      "           ب  IN      lemma=ب\n",
      "        صيغة  NN      lemma=صِيغَة\n",
      "         CSV  NN      lemma=CSV\n",
      "      المعدة  NN      lemma=مَعِدَة\n",
      "           ل  IN      lemma=ل\n",
      "      لتدقيق  NN      lemma=تَدْقِيق\n",
      "          قد  RP      lemma=قد\n",
      "       تتضمن  VB      lemma=تَضَمَّن\n",
      "      معرفات  NNP     lemma=معرفات\n",
      "       حساسة  NN      lemma=حَسّاس\n",
      "         مثل  IN      lemma=مثل\n",
      "         رقم  VB      lemma=رَقَم\n",
      "    المستخدم  NN      lemma=مُسْتَخْدَم\n",
      "           ،  PUNCT   lemma=∅\n",
      "       عنوان  NN      lemma=عُنْوان\n",
      "      البريد  NN      lemma=بَرِيد\n",
      "    الإلكترو  NNP     lemma=الالكترو\n",
      "          ني  PRP     lemma=ني\n",
      "           ،  PUNCT   lemma=∅\n",
      "          أو  CC      lemma=أو\n",
      "         رقم  VB      lemma=رَقَم\n",
      "      الهاتف  NN      lemma=هاتِف\n",
      "           ،  PUNCT   lemma=∅\n",
      "        ولذل  NN      lemma=ذُلّ\n",
      "           ك  PRP     lemma=ك\n",
      "        تخضع  VB      lemma=خَضَع\n",
      "           ل  IN      lemma=ل\n",
      "    التزامات  NN      lemma=ٱِلْتِزام\n",
      "      السرية  JJ      lemma=سِرِّيّ\n",
      "           .  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "          في  IN      lemma=فِي\n",
      "         حال  VB      lemma=حال\n",
      "        ورود  NN      lemma=وُرُود\n",
      "         طلب  VB      lemma=طَلَب\n",
      "          من  IN      lemma=من\n",
      "         طرف  VB      lemma=طَرُف\n",
      "        ثالث  NNP     lemma=ثالِث\n",
      "           (  PUNCT   lemma=∅\n",
      "        عميل  NN      lemma=عَمِيل\n",
      "           ،  PUNCT   lemma=∅\n",
      "          شر  VB      lemma=شَرّ\n",
      "           ي  PRP     lemma=ي\n",
      "           ك  PRP     lemma=ك\n",
      "           ،  PUNCT   lemma=∅\n",
      "          أو  CC      lemma=أو\n",
      "      متعاقد  NN      lemma=مُتَعاقِد\n",
      "          من  IN      lemma=من\n",
      "      الباطن  NN      lemma=باطِن\n",
      "           )  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "         فإن  NNP     lemma=فان\n",
      "       تسليم  NN      lemma=تَسْلِيم\n",
      "          أي  CC      lemma=أَيْ\n",
      "       مقتطف  NN      lemma=مُقْتَطَف\n",
      "          من  IN      lemma=من\n",
      "    البيانات  NN      lemma=بَيان\n",
      "       يتطلب  VB      lemma=تَطَلَّب\n",
      "      موافقة  JJ      lemma=مُوافِق\n",
      "      مديرية  NN      lemma=مُدِير\n",
      "         نظم  VB      lemma=نَظَم\n",
      "   المعلومات  JJ      lemma=مَعْلُوم\n",
      "           و  CC      lemma=و\n",
      "      التحقق  NN      lemma=تَحَقُّق\n",
      "          من  IN      lemma=من\n",
      "        خلية  NN      lemma=خَلِيَّة\n",
      "    الامتثال  NN      lemma=ٱِمْتِثال\n",
      "           ،  PUNCT   lemma=∅\n",
      "          ما  RP      lemma=ما\n",
      "          لم  RP      lemma=لم\n",
      "         تكن  VB      lemma=كان\n",
      "         هنا  VB      lemma=هَنَأ\n",
      "           ك  PRP     lemma=ك\n",
      "        حالة  NN      lemma=حالَة\n",
      "       طارئة  JJ      lemma=طارِئ\n",
      "       موثقة  NN      lemma=مَوْثِق\n",
      "           .  PUNCT   lemma=∅\n",
      "         كما  CC      lemma=كَما\n",
      "         يجب  VB      lemma=أَجاب\n",
      "     الإبلاغ  NN      lemma=إِبْلاغ\n",
      "          عن  IN      lemma=عن\n",
      "          أي  CC      lemma=أَيْ\n",
      "         سلو  NNP     lemma=سلو\n",
      "           ك  PRP     lemma=ك\n",
      "         غير  JJ      lemma=غير\n",
      "      اعتياد  NN      lemma=ٱِعْتِياد\n",
      "           ي  PRP     lemma=ي\n",
      "        خلال  IN      lemma=خلال\n",
      "          24  CD      lemma=24\n",
      "        ساعة  NN      lemma=ساعَة\n",
      "           ك  IN      lemma=ك\n",
      "          حد  NN      lemma=حَدّ\n",
      "        أقصى  VB      lemma=أَقْصَى\n",
      "           ،  PUNCT   lemma=∅\n",
      "          مع  IN      lemma=مع\n",
      "       تقرير  NN      lemma=تَقْرِير\n",
      "        يحدد  VB      lemma=حَدَّد\n",
      "     التاريخ  NN      lemma=تَأْرِيخ\n",
      "           و  CC      lemma=و\n",
      "       الوقت  NN      lemma=وَقْت\n",
      "           و  CC      lemma=و\n",
      "     الأنظمة  NN      lemma=نِظام\n",
      "     المعنية  JJ      lemma=مَعْنِيّ\n",
      "           و  CC      lemma=و\n",
      "       السبب  NN      lemma=سَبَب\n",
      "     المحتمل  JJ      lemma=مُحْتَمَل\n",
      "           و  CC      lemma=و\n",
      "   الإجراءات  NN      lemma=إِجْراء\n",
      "   التصحيحية  JJ      lemma=تَصْحِيحِيّ\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('استنادًا', 'O'), ('إلى', 'O'), ('المادة', 'O'), ('7', 'O'), ('.', 'O'), ('2', 'O'), ('من', 'O'), ('النظام', 'O'), ('الداخلي', 'O'), ('المُحدّث', 'O'), ('بتاريخ', 'O'), ('01', 'O'), ('/', 'O'), ('09', 'O'), ('/', 'O'), ('2025', 'B-DATE'), ('،', 'O'), ('يُشترط', 'O'), ('أن', 'O'), ('يكون', 'O'), ('أي', 'O'), ('وصول', 'O'), ('إلى', 'O'), ('البيانات', 'O'), ('ذات', 'O'), ('الطابع', 'O'), ('الشخصي', 'O'), ('مبرّرًا', 'O'), ('ومُوثّقًا', 'O'), ('ومحدودًا', 'O'), ('بالحد', 'O'), ('الأدنى', 'O'), ('الضروري', 'O'), ('،', 'O'), ('بما', 'O'), ('في', 'O'), ('ذلك', 'O'), ('العمليات', 'O'), ('التي', 'O'), ('تُجرى', 'O'), ('لأغراض', 'O'), ('الصيانة', 'O'), ('التصحيحية', 'O'), ('أو', 'O'), ('الاستجابة', 'O'), ('للحوادث', 'O'), ('.', 'O'), ('ويُذكَّر', 'O'), ('بأن', 'O'), ('سجلات', 'O'), ('التطبيقات', 'O'), ('،', 'O'), ('ونسخ', 'O'), ('الحفظ', 'O'), ('التزايدية', 'O'), ('،', 'O'), ('وملفات', 'O'), ('التصدير', 'O'), ('بصيغة', 'O'), ('CSV', 'O'), ('المُعدّة', 'O'), ('للتدقيق', 'O'), ('قد', 'O'), ('تتضمن', 'O'), ('معرّفات', 'O'), ('حساسة', 'O'), ('مثل', 'O'), ('رقم', 'O'), ('المستخدم', 'O'), ('،', 'O'), ('عنوان', 'O'), ('البريد', 'O'), ('الإلكتروني', 'O'), ('،', 'O'), ('أو', 'O'), ('رقم', 'O'), ('الهاتف', 'O'), ('،', 'O'), ('ولذلك', 'O'), ('تخضع', 'O'), ('لالتزامات', 'O'), ('السرية', 'O'), ('.', 'O'), ('وفي', 'O'), ('حال', 'O'), ('ورود', 'O'), ('طلب', 'O'), ('من', 'O'), ('طرف', 'O'), ('ثالث', 'O'), ('(', 'O'), ('عميل', 'O'), ('،', 'O'), ('شريك', 'O'), ('،', 'O'), ('أو', 'O'), ('متعاقد', 'O'), ('من', 'O'), ('الباطن', 'O'), (')', 'O'), ('،', 'O'), ('فإن', 'O'), ('تسليم', 'O'), ('أي', 'O'), ('مقتطف', 'O'), ('من', 'O'), ('البيانات', 'O'), ('يتطلب', 'O'), ('موافقة', 'O'), ('مديرية', 'O'), ('نظم', 'O'), ('المعلومات', 'O'), ('والتحقق', 'O'), ('من', 'O'), ('خلية', 'O'), ('الامتثال', 'O'), ('،', 'O'), ('ما', 'O'), ('لم', 'O'), ('تكن', 'O'), ('هناك', 'O'), ('حالة', 'O'), ('طارئة', 'O'), ('موثّقة', 'O'), ('.', 'O'), ('كما', 'O'), ('يجب', 'O'), ('الإبلاغ', 'O'), ('عن', 'O'), ('أي', 'O'), ('سلوك', 'O'), ('غير', 'O'), ('اعتيادي', 'O'), ('خلال', 'O'), ('24', 'O'), ('ساعة', 'O'), ('كحد', 'O'), ('أقصى', 'O'), ('،', 'O'), ('مع', 'O'), ('تقرير', 'O'), ('يحدد', 'O'), ('التاريخ', 'O'), ('والوقت', 'O'), ('والأنظمة', 'O'), ('المعنية', 'O'), ('والسبب', 'O'), ('المحتمل', 'O'), ('والإجراءات', 'O'), ('التصحيحية', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: 2025\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=102 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 17/102 = 16.67 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=2 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: في مستودع atlas-core أُضيفت تغييرات إلى الفرع feature/ner-fastpath لتقليل زمن المعالجة عندما تتضمن السلسلة مُعرّفات من نمط UUID v4 مثل 550e8400-e29b-41d4-a716-446655440000 أو مسارات يونكس مثل /var/log/nginx/access.log. يقوم نظام التطبيع باستبدال سلاسل الأرقام الطويلة لكنه يحافظ على الوحدات (ms، MB/s، kWh) وعلى الصيغ العلمية (3.2e-4، 10^6، 2×10−3)، ويترك علامات الترقيم كما هي في أسماء الكيانات: OpenAI, Inc. و A.B.C. Consulting و مستشفى مصطفى باشا و CNRS/INRIA. للتجربة أضفنا أيضًا وسومًا مثل #DevOps و#الذكاء_الاصطناعي، وحسابات مثل @support و@team-lead، وروابط مثل https://example.org/api/v1/users?id=42 وبريدًا مثل prenom.nom+test@domaine.dz، ولاحظنا أن بعض الجُمل التي تحتوي على شرطات متتابعة أو أقواس كثيرة قد تُربك وسم الأجزاء الكلامية بين الاسم والصفة، خصوصًا في تراكيب مثل خدمات-مصغّرة، شبه-وسم، وما-بعد-المعالجة.\n",
      "          في  IN      lemma=في\n",
      "      مستودع  NN      lemma=مُسْتَوْدَع\n",
      "  atlas-core  NNP     lemma=atlas-core\n",
      "       أضيفت  VB      lemma=أَضاف\n",
      "     تغييرات  NN      lemma=تَغْيِير\n",
      "         إلى  IN      lemma=إلى\n",
      "       الفرع  NN      lemma=فَرْع\n",
      "     feature  NN      lemma=feature\n",
      "           /  PUNCT   lemma=∅\n",
      "ner-fastpath  NNP     lemma=ner-fastpath\n",
      "           ل  IN      lemma=ل\n",
      "       تقليل  NN      lemma=تَقْلِيل\n",
      "         زمن  VB      lemma=زَمِن\n",
      "    المعالجة  NN      lemma=مُعالَجَة\n",
      "       عندما  NN      lemma=عَنْدَم\n",
      "       تتضمن  VB      lemma=تَضَمَّن\n",
      "     السلسلة  NN      lemma=سِلْسِلَة\n",
      "      معرفات  NNP     lemma=معرفات\n",
      "          من  IN      lemma=من\n",
      "         نمط  NN      lemma=نَمَط\n",
      "        UUID  NN      lemma=UUID\n",
      "           v  NN      lemma=v\n",
      "           4  CD      lemma=4\n",
      "         مثل  IN      lemma=مثل\n",
      "         550  CD      lemma=550\n",
      "           e  NN      lemma=e\n",
      "        8400  CD      lemma=8400\n",
      "           -  PUNCT   lemma=∅\n",
      "           e  NN      lemma=e\n",
      "          29  CD      lemma=29\n",
      "           b  NN      lemma=b\n",
      "           -  PUNCT   lemma=∅\n",
      "          41  CD      lemma=41\n",
      "           d  NN      lemma=d\n",
      "           4  CD      lemma=4\n",
      "           -  PUNCT   lemma=∅\n",
      "           a  NN      lemma=a\n",
      "         716  CD      lemma=716\n",
      "           -  PUNCT   lemma=∅\n",
      "446655440000  CD      lemma=446655440000\n",
      "          أو  CC      lemma=أو\n",
      "      مسارات  NN      lemma=مَسار\n",
      "       يونكس  NNP     lemma=يونكس\n",
      "         مثل  IN      lemma=مثل\n",
      "           /  PUNCT   lemma=∅\n",
      "         var  NN      lemma=var\n",
      "           /  PUNCT   lemma=∅\n",
      "         log  NN      lemma=log\n",
      "           /  PUNCT   lemma=∅\n",
      "       nginx  NN      lemma=nginx\n",
      "           /  PUNCT   lemma=∅\n",
      "      access  NN      lemma=access\n",
      "           .  PUNCT   lemma=∅\n",
      "         log  NN      lemma=log\n",
      "           .  PUNCT   lemma=∅\n",
      "        يقوم  VB      lemma=قام\n",
      "        نظام  NN      lemma=نِظام\n",
      "     التطبيع  NN      lemma=تَطْبِيع\n",
      "           ب  IN      lemma=ب\n",
      "     استبدال  NN      lemma=ٱِسْتِبْدال\n",
      "       سلاسل  NN      lemma=سِلْسِلَة\n",
      "     الأرقام  NN      lemma=رَقْم\n",
      "     الطويلة  JJ      lemma=طَوِيل\n",
      "        لكنه  CC      lemma=لكن\n",
      "       يحافظ  VB      lemma=حافَظ\n",
      "         على  IN      lemma=على\n",
      "     الوحدات  NN      lemma=وَحْدَة\n",
      "           (  PUNCT   lemma=∅\n",
      "          ms  NN      lemma=ms\n",
      "           ،  PUNCT   lemma=∅\n",
      "          MB  NN      lemma=MB\n",
      "           /  PUNCT   lemma=∅\n",
      "           s  NN      lemma=s\n",
      "           ،  PUNCT   lemma=∅\n",
      "         kWh  NN      lemma=kWh\n",
      "           )  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "         على  IN      lemma=عَلَى\n",
      "       الصيغ  NN      lemma=صِيغَة\n",
      "     العلمية  JJ      lemma=عِلْمِيّ\n",
      "           (  PUNCT   lemma=∅\n",
      "           3  CD      lemma=3\n",
      "           .  PUNCT   lemma=∅\n",
      "           2  CD      lemma=2\n",
      "           e  NN      lemma=e\n",
      "           -  PUNCT   lemma=∅\n",
      "           4  CD      lemma=4\n",
      "           ،  PUNCT   lemma=∅\n",
      "          10  CD      lemma=10\n",
      "           ^  PUNCT   lemma=∅\n",
      "           6  CD      lemma=6\n",
      "           ،  PUNCT   lemma=∅\n",
      "           2  CD      lemma=2\n",
      "           ×  NN      lemma=×\n",
      "          10  CD      lemma=10\n",
      "           −  NN      lemma=−\n",
      "           3  CD      lemma=3\n",
      "           )  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "         يتر  VB      lemma=وَتَر\n",
      "           ك  PRP     lemma=ك\n",
      "      علامات  NN      lemma=عَلامَة\n",
      "     الترقيم  NN      lemma=تَرْقِيم\n",
      "         كما  CC      lemma=كَما\n",
      "          هي  PRP     lemma=هِيَ\n",
      "          في  IN      lemma=في\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "    الكيانات  NN      lemma=كِيان\n",
      "           :  PUNCT   lemma=∅\n",
      "      OpenAI  NN      lemma=OpenAI\n",
      "           ,  PUNCT   lemma=∅\n",
      "         Inc  NN      lemma=Inc\n",
      "           .  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "           A  NN      lemma=A\n",
      "           .  PUNCT   lemma=∅\n",
      "           B  NN      lemma=B\n",
      "           .  PUNCT   lemma=∅\n",
      "           C  NN      lemma=C\n",
      "           .  PUNCT   lemma=∅\n",
      "  Consulting  NN      lemma=Consulting\n",
      "           و  CC      lemma=و\n",
      "      مستشفى  NN      lemma=مُسْتَشْفَى\n",
      "       مصطفى  NNP     lemma=مُصْطَفَى\n",
      "        باشا  VB      lemma=باش\n",
      "           و  CC      lemma=و\n",
      "        CNRS  NN      lemma=CNRS\n",
      "           /  PUNCT   lemma=∅\n",
      "       INRIA  NN      lemma=INRIA\n",
      "           .  PUNCT   lemma=∅\n",
      "           ل  IN      lemma=ل\n",
      "      لتجربة  NN      lemma=تَجْرِبَة\n",
      "         أضف  VB      lemma=ضَفا\n",
      "          نا  PRP     lemma=نا\n",
      "        أيضا  VB      lemma=أَيَّض\n",
      "       وسوما  NN      lemma=وَسْم\n",
      "         مثل  IN      lemma=مثل\n",
      "           #  NN      lemma=#\n",
      "      DevOps  NN      lemma=DevOps\n",
      "           و  CC      lemma=و\n",
      "           #  NN      lemma=#\n",
      "      الذكاء  NN      lemma=ذَكاء\n",
      "           _  NN      lemma=_\n",
      "    الاصطناع  NN      lemma=ٱِصْطِناع\n",
      "           ي  PRP     lemma=ي\n",
      "           ،  PUNCT   lemma=∅\n",
      "     وحسابات  NN      lemma=حِساب\n",
      "         مثل  IN      lemma=مثل\n",
      "           @  PUNCT   lemma=∅\n",
      "     support  NN      lemma=support\n",
      "           و  CC      lemma=و\n",
      "           @  PUNCT   lemma=∅\n",
      "   team-lead  NNP     lemma=team-lead\n",
      "           ،  PUNCT   lemma=∅\n",
      "      وروابط  NN      lemma=رابِطَة\n",
      "         مثل  IN      lemma=مثل\n",
      "       https  NN      lemma=https\n",
      "           :  PUNCT   lemma=∅\n",
      "           /  PUNCT   lemma=∅\n",
      "           /  PUNCT   lemma=∅\n",
      "     example  NN      lemma=example\n",
      "           .  PUNCT   lemma=∅\n",
      "         org  NN      lemma=org\n",
      "           /  PUNCT   lemma=∅\n",
      "         api  NN      lemma=api\n",
      "           /  PUNCT   lemma=∅\n",
      "           v  NN      lemma=v\n",
      "           1  CD      lemma=1\n",
      "           /  PUNCT   lemma=∅\n",
      "       users  NN      lemma=users\n",
      "           ?  PUNCT   lemma=∅\n",
      "          id  NN      lemma=id\n",
      "           =  PUNCT   lemma=∅\n",
      "          42  CD      lemma=42\n",
      "      وبريدا  NN      lemma=بَرِيد\n",
      "         مثل  IN      lemma=مثل\n",
      "      prenom  NN      lemma=prenom\n",
      "           .  PUNCT   lemma=∅\n",
      "         nom  NN      lemma=nom\n",
      "           +  PUNCT   lemma=∅\n",
      "        test  NN      lemma=test\n",
      "           @  PUNCT   lemma=∅\n",
      "     domaine  NN      lemma=domaine\n",
      "           .  PUNCT   lemma=∅\n",
      "          dz  NN      lemma=dz\n",
      "           ،  PUNCT   lemma=∅\n",
      "       ولاحظ  VB      lemma=لاحَظ\n",
      "          نا  PRP     lemma=نا\n",
      "          أن  RP      lemma=أن\n",
      "         بعض  NN      lemma=بعض\n",
      "       الجمل  NN      lemma=جَمَل\n",
      "        التي  WDT     lemma=التي\n",
      "        تحتو  VB      lemma=ٱِحْتَوَى\n",
      "           ي  PRP     lemma=ي\n",
      "         على  IN      lemma=على\n",
      "       شرطات  NNP     lemma=شرطات\n",
      "     متتابعة  NN      lemma=مُتَتابِع\n",
      "          أو  CC      lemma=أو\n",
      "       أقواس  NN      lemma=قَوْس\n",
      "           ك  IN      lemma=ك\n",
      "        ثيرة  NNP     lemma=ثيره\n",
      "          قد  RP      lemma=قد\n",
      "         ترب  VB      lemma=تَرِب\n",
      "           ك  PRP     lemma=ك\n",
      "         وسم  VB      lemma=وَسَم\n",
      "     الأجزاء  NN      lemma=جُزْء\n",
      "    الكلامية  JJ      lemma=كَلامِيّ\n",
      "         بين  IN      lemma=بين\n",
      "       الاسم  NN      lemma=ٱِسْم\n",
      "           و  CC      lemma=و\n",
      "       الصفة  NN      lemma=صِفَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "       خصوصا  RB      lemma=خُصُوص\n",
      "          في  IN      lemma=في\n",
      "      تراكيب  NN      lemma=تَرْكِيب\n",
      "         مثل  IN      lemma=مثل\n",
      "       خدمات  NN      lemma=خِدْمَة\n",
      "           -  PUNCT   lemma=∅\n",
      "       مصغرة  JJ      lemma=مُصَغَّر\n",
      "           ،  PUNCT   lemma=∅\n",
      "          شب  NN      lemma=شَبّ\n",
      "           ه  PRP     lemma=ه\n",
      "           -  PUNCT   lemma=∅\n",
      "         وسم  VB      lemma=وَسَم\n",
      "           ،  PUNCT   lemma=∅\n",
      "         وما  NN      lemma=ما\n",
      "           -  PUNCT   lemma=∅\n",
      "         بعد  IN      lemma=بعد\n",
      "           -  PUNCT   lemma=∅\n",
      "    المعالجة  NN      lemma=مُعالَجَة\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('في', 'O'), ('مستودع', 'O'), ('atlas-core', 'O'), ('أُضيفت', 'O'), ('تغييرات', 'O'), ('إلى', 'O'), ('الفرع', 'O'), ('feature', 'O'), ('/', 'O'), ('ner-fastpath', 'O'), ('لتقليل', 'O'), ('زمن', 'O'), ('المعالجة', 'O'), ('عندما', 'O'), ('تتضمن', 'O'), ('السلسلة', 'O'), ('مُعرّفات', 'O'), ('من', 'O'), ('نمط', 'O'), ('UUID', 'O'), ('v', 'O'), ('4', 'O'), ('مثل', 'O'), ('550', 'O'), ('e', 'O'), ('8400', 'O'), ('-', 'O'), ('e', 'O'), ('29', 'O'), ('b', 'O'), ('-', 'O'), ('41', 'O'), ('d', 'O'), ('4', 'O'), ('-', 'O'), ('a', 'O'), ('716', 'O'), ('-', 'O'), ('446655440000', 'O'), ('أو', 'O'), ('مسارات', 'O'), ('يونكس', 'B-MISC'), ('مثل', 'O'), ('/', 'O'), ('var', 'O'), ('/', 'O'), ('log', 'O'), ('/', 'O'), ('nginx', 'O'), ('/', 'O'), ('access', 'O'), ('.', 'O'), ('log', 'O'), ('.', 'O'), ('يقوم', 'O'), ('نظام', 'O'), ('التطبيع', 'O'), ('باستبدال', 'O'), ('سلاسل', 'O'), ('الأرقام', 'O'), ('الطويلة', 'O'), ('لكنه', 'O'), ('يحافظ', 'O'), ('على', 'O'), ('الوحدات', 'O'), ('(', 'O'), ('ms', 'O'), ('،', 'O'), ('MB', 'O'), ('/', 'O'), ('s', 'O'), ('،', 'O'), ('kWh', 'O'), (')', 'O'), ('وعلى', 'O'), ('الصيغ', 'O'), ('العلمية', 'O'), ('(', 'O'), ('3', 'O'), ('.', 'O'), ('2', 'O'), ('e', 'O'), ('-', 'O'), ('4', 'O'), ('،', 'O'), ('10', 'O'), ('^', 'O'), ('6', 'O'), ('،', 'O'), ('2', 'O'), ('×', 'O'), ('10', 'O'), ('−', 'O'), ('3', 'O'), (')', 'O'), ('،', 'O'), ('ويترك', 'O'), ('علامات', 'O'), ('الترقيم', 'O'), ('كما', 'O'), ('هي', 'O'), ('في', 'O'), ('أسماء', 'O'), ('الكيانات', 'O'), (':', 'O'), ('OpenAI', 'O'), (',', 'O'), ('Inc', 'O'), ('.', 'O'), ('و', 'O'), ('A', 'O'), ('.', 'O'), ('B', 'O'), ('.', 'O'), ('C', 'O'), ('.', 'O'), ('Consulting', 'O'), ('و', 'O'), ('مستشفى', 'O'), ('مصطفى', 'B-PERS'), ('باشا', 'I-PERS'), ('و', 'O'), ('CNRS', 'O'), ('/', 'O'), ('INRIA', 'O'), ('.', 'O'), ('للتجربة', 'O'), ('أضفنا', 'O'), ('أيضًا', 'O'), ('وسومًا', 'O'), ('مثل', 'O'), ('#', 'O'), ('DevOps', 'O'), ('و', 'O'), ('#', 'O'), ('الذكاء', 'O'), ('_', 'O'), ('الاصطناعي', 'O'), ('،', 'O'), ('وحسابات', 'O'), ('مثل', 'O'), ('@', 'O'), ('support', 'O'), ('و', 'O'), ('@', 'O'), ('team-lead', 'O'), ('،', 'O'), ('وروابط', 'O'), ('مثل', 'O'), ('https', 'O'), (':', 'O'), ('/', 'O'), ('/', 'O'), ('example', 'O'), ('.', 'O'), ('org', 'O'), ('/', 'O'), ('api', 'O'), ('/', 'O'), ('v', 'O'), ('1', 'O'), ('/', 'O'), ('users', 'O'), ('?', 'O'), ('id', 'O'), ('=', 'O'), ('42', 'O'), ('وبريدًا', 'O'), ('مثل', 'O'), ('prenom', 'O'), ('.', 'O'), ('nom', 'O'), ('+', 'O'), ('test', 'O'), ('@', 'O'), ('domaine', 'O'), ('.', 'O'), ('dz', 'O'), ('،', 'O'), ('ولاحظنا', 'O'), ('أن', 'O'), ('بعض', 'O'), ('الجُمل', 'O'), ('التي', 'O'), ('تحتوي', 'O'), ('على', 'O'), ('شرطات', 'O'), ('متتابعة', 'O'), ('أو', 'O'), ('أقواس', 'O'), ('كثيرة', 'O'), ('قد', 'O'), ('تُربك', 'O'), ('وسم', 'O'), ('الأجزاء', 'O'), ('الكلامية', 'O'), ('بين', 'O'), ('الاسم', 'O'), ('والصفة', 'O'), ('،', 'O'), ('خصوصًا', 'O'), ('في', 'O'), ('تراكيب', 'O'), ('مثل', 'O'), ('خدمات', 'O'), ('-', 'O'), ('مصغّرة', 'O'), ('،', 'O'), ('شبه', 'O'), ('-', 'O'), ('وسم', 'O'), ('،', 'O'), ('وما', 'O'), ('-', 'O'), ('بعد', 'O'), ('-', 'O'), ('المعالجة', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  MISC: يونكس\n",
      "  PERS: مصطفى باشا\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=114 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 5/114 = 4.39 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=3 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: قال ياسين: \"رأيته أمس\" ثم سكت قليلًا وكأنه يختبر معنى \"أمس\" عند السامعين، هل يقصد الجمعة 13 أم السبت 14. خرج من شارع ديدوش مراد، ثم أخذ سيارة أجرة إلى حيدرة، وفي الساعة 19:05 اتصلت به ليلى وقالت: \"نلتقي أمام فندق الأوراسي، أنا مع كريم.\" عندما وصل كان بهو الفندق صاخبًا؛ بعضهم يتحدث بالفصحى، وآخرون بالدارجة، وآخرون يخلطون الفرنسية بالإنجليزية: someone said \"the meeting is postponed\" فرد آخر \"ماشي مشكل، نتكيفو\". في لحظة لاحقة طلب منه حارس الأمن بطاقة الهوية، فتردد لأنه يحمل نسخة مصوّرة فقط، فبدأ نقاش طويل عن القاعدة والاستثناء و\"العقل\"، وكل ذلك يجعل الإحالات بالضمائر (هو، هي، هم، أحدهم) اختبارًا صعبًا على الأنظمة التي لا تُحسن الاستفادة من السياق الممتد.\n",
      "         قال  VB      lemma=قال\n",
      "       ياسين  VB      lemma=أَسِي\n",
      "           :  PUNCT   lemma=∅\n",
      "           \"  PUNCT   lemma=∅\n",
      "        رأيت  NNP     lemma=رايْت\n",
      "           ه  PRP     lemma=ه\n",
      "         أمس  RB      lemma=أَمْس\n",
      "           \"  PUNCT   lemma=∅\n",
      "          ثم  CC      lemma=ثم\n",
      "         سكت  VB      lemma=سَكُت\n",
      "       قليلا  JJ      lemma=قَلِيل\n",
      "        وكأن  NN      lemma=آن\n",
      "           ه  PRP     lemma=ه\n",
      "       يختبر  VB      lemma=ٱِخْتَبَر\n",
      "        معنى  NN      lemma=مَعْنَى\n",
      "           \"  PUNCT   lemma=∅\n",
      "         أمس  RB      lemma=أَمْس\n",
      "           \"  PUNCT   lemma=∅\n",
      "         عند  IN      lemma=عند\n",
      "    السامعين  JJ      lemma=سامِع\n",
      "           ،  PUNCT   lemma=∅\n",
      "          هل  RP      lemma=هل\n",
      "        يقصد  VB      lemma=قَصَد\n",
      "      الجمعة  NNP     lemma=جُمْعَة\n",
      "          13  CD      lemma=13\n",
      "          أم  CC      lemma=أم\n",
      "       السبت  NN      lemma=سَبَت\n",
      "          14  CD      lemma=14\n",
      "           .  PUNCT   lemma=∅\n",
      "         خرج  VB      lemma=خَرَج\n",
      "          من  IN      lemma=من\n",
      "        شارع  NN      lemma=شارِع\n",
      "       ديدوش  NNP     lemma=ديدوش\n",
      "        مراد  NNP     lemma=مُراد\n",
      "           ،  PUNCT   lemma=∅\n",
      "          ثم  CC      lemma=ثم\n",
      "         أخذ  VB      lemma=أَخَذ\n",
      "           س  RP      lemma=س\n",
      "        يارة  NNP     lemma=ياره\n",
      "        أجرة  VB      lemma=أَجَر\n",
      "         إلى  IN      lemma=إلى\n",
      "       حيدرة  NNP     lemma=حيدره\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "          في  IN      lemma=فِي\n",
      "      الساعة  NN      lemma=ساعَة\n",
      "          19  CD      lemma=19\n",
      "           :  PUNCT   lemma=∅\n",
      "          05  CD      lemma=05\n",
      "       اتصلت  VB      lemma=ٱِتَّصَل\n",
      "          به  IN      lemma=بِ\n",
      "        ليلى  NNP     lemma=لَيْلَى\n",
      "       وقالت  VB      lemma=قال\n",
      "           :  PUNCT   lemma=∅\n",
      "           \"  PUNCT   lemma=∅\n",
      "        نلتق  VB      lemma=ٱِلْتَقَى\n",
      "           ي  PRP     lemma=ي\n",
      "        أمام  NNP     lemma=إِمام\n",
      "        فندق  NN      lemma=فُنْدُق\n",
      "     الأوراس  NNP     lemma=أَوْراس\n",
      "           ي  PRP     lemma=ي\n",
      "           ،  PUNCT   lemma=∅\n",
      "         أنا  NNP     lemma=آنا\n",
      "          مع  IN      lemma=مع\n",
      "        كريم  NNP     lemma=كَرِيم\n",
      "           .  PUNCT   lemma=∅\n",
      "           \"  PUNCT   lemma=∅\n",
      "       عندما  NN      lemma=عَنْدَم\n",
      "         وصل  VB      lemma=وَصَل\n",
      "           ك  IN      lemma=ك\n",
      "          ان  NN      lemma=أَنْ\n",
      "           ب  IN      lemma=ب\n",
      "          هو  NNP     lemma=هُو\n",
      "      الفندق  NN      lemma=فُنْدُق\n",
      "       صاخبا  JJ      lemma=صاخِب\n",
      "           ؛  PUNCT   lemma=∅\n",
      "           ب  IN      lemma=ب\n",
      "          عض  NN      lemma=عَضّ\n",
      "          هم  PRP     lemma=هم\n",
      "       يتحدث  VB      lemma=تَحَدَّث\n",
      "           ب  IN      lemma=ب\n",
      "      الفصحى  NN      lemma=فُصْحَى\n",
      "           ،  PUNCT   lemma=∅\n",
      "      وآخرون  JJ      lemma=آخَر\n",
      "           ب  IN      lemma=ب\n",
      "     الدارجة  JJ      lemma=دارِج\n",
      "           ،  PUNCT   lemma=∅\n",
      "      وآخرون  JJ      lemma=آخَر\n",
      "      يخلطون  VB      lemma=خَلَط\n",
      "    الفرنسية  NN      lemma=فَرَنْسِيّ\n",
      "           ب  IN      lemma=ب\n",
      "  الإنجليزية  NN      lemma=إِنْجلِيزِيَّة\n",
      "           :  PUNCT   lemma=∅\n",
      "     someone  NN      lemma=someone\n",
      "        said  NN      lemma=said\n",
      "           \"  PUNCT   lemma=∅\n",
      "         the  NN      lemma=the\n",
      "     meeting  NN      lemma=meeting\n",
      "          is  NN      lemma=is\n",
      "   postponed  NN      lemma=postponed\n",
      "           \"  PUNCT   lemma=∅\n",
      "         فرد  NN      lemma=فَرْد\n",
      "         آخر  JJ      lemma=آخَر\n",
      "           \"  PUNCT   lemma=∅\n",
      "         ماش  NN      lemma=ماشِي\n",
      "           ي  PRP     lemma=ي\n",
      "        مشكل  NN      lemma=مُشْكِل\n",
      "           ،  PUNCT   lemma=∅\n",
      "      نتكيفو  NNP     lemma=نتكيفو\n",
      "           \"  PUNCT   lemma=∅\n",
      "           .  PUNCT   lemma=∅\n",
      "          في  IN      lemma=في\n",
      "           ل  IN      lemma=ل\n",
      "         حظة  NN      lemma=حَظّ\n",
      "           ل  IN      lemma=ل\n",
      "        احقة  NN      lemma=أَحَقّ\n",
      "         طلب  VB      lemma=طَلَب\n",
      "          من  IN      lemma=مِن\n",
      "           ه  PRP     lemma=ه\n",
      "        حارس  NN      lemma=حارِس\n",
      "       الأمن  NN      lemma=آمِن\n",
      "           ب  IN      lemma=ب\n",
      "        طاقة  NN      lemma=طاق\n",
      "      الهوية  NN      lemma=هُوِيَّة\n",
      "           ،  PUNCT   lemma=∅\n",
      "           ف  CC      lemma=ف\n",
      "        تردد  VB      lemma=تَرَدَّد\n",
      "        لأنه  RP      lemma=لأن\n",
      "        يحمل  VB      lemma=حَمَل\n",
      "        نسخة  VB      lemma=نَسَخ\n",
      "       مصورة  NN      lemma=مُصَوِّرَة\n",
      "         فقط  VB      lemma=فَقَط\n",
      "           ،  PUNCT   lemma=∅\n",
      "        فبدأ  VB      lemma=بَدَأ\n",
      "        نقاش  NNP     lemma=نَقّاش\n",
      "        طويل  NNP     lemma=طَوِيل\n",
      "          عن  IN      lemma=عن\n",
      "     القاعدة  NN      lemma=قاعِدَة\n",
      "           و  CC      lemma=و\n",
      "   الاستثناء  NN      lemma=ٱِسْتِثْناء\n",
      "           و  CC      lemma=و\n",
      "           \"  PUNCT   lemma=∅\n",
      "       العقل  NN      lemma=عِقال\n",
      "           \"  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "         وكل  VB      lemma=وَكَل\n",
      "         ذلك  PRP     lemma=ذلك\n",
      "        يجعل  VB      lemma=جَعَل\n",
      "    الإحالات  NN      lemma=إِحالَة\n",
      "           ب  IN      lemma=ب\n",
      "     الضمائر  NN      lemma=ضَمِير\n",
      "           (  PUNCT   lemma=∅\n",
      "          هو  NNP     lemma=هُو\n",
      "           ،  PUNCT   lemma=∅\n",
      "          هي  PRP     lemma=هِيَ\n",
      "           ،  PUNCT   lemma=∅\n",
      "          هم  PRP     lemma=هُم\n",
      "           ،  PUNCT   lemma=∅\n",
      "         أحد  NNP     lemma=أَحَد\n",
      "          هم  PRP     lemma=هم\n",
      "           )  PUNCT   lemma=∅\n",
      "     اختبارا  NN      lemma=ٱِخْتِبار\n",
      "        صعبا  JJ      lemma=صَعْب\n",
      "         على  IN      lemma=على\n",
      "     الأنظمة  NN      lemma=نِظام\n",
      "        التي  WDT     lemma=التي\n",
      "          لا  RP      lemma=لا\n",
      "        تحسن  VB      lemma=حَسُن\n",
      "   الاستفادة  NN      lemma=ٱِسْتِفادَة\n",
      "          من  IN      lemma=من\n",
      "      السياق  NN      lemma=سِياق\n",
      "      الممتد  JJ      lemma=مُمْتَدّ\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('قال', 'O'), ('ياسين', 'B-PERS'), (':', 'O'), ('\"', 'O'), ('رأيته', 'O'), ('أمس', 'O'), ('\"', 'O'), ('ثم', 'O'), ('سكت', 'O'), ('قليلًا', 'O'), ('وكأنه', 'O'), ('يختبر', 'O'), ('معنى', 'O'), ('\"', 'O'), ('أمس', 'O'), ('\"', 'O'), ('عند', 'O'), ('السامعين', 'O'), ('،', 'O'), ('هل', 'O'), ('يقصد', 'O'), ('الجمعة', 'O'), ('13', 'O'), ('أم', 'O'), ('السبت', 'O'), ('14', 'O'), ('.', 'O'), ('خرج', 'O'), ('من', 'O'), ('شارع', 'O'), ('ديدوش', 'B-LOC'), ('مراد', 'I-LOC'), ('،', 'O'), ('ثم', 'O'), ('أخذ', 'O'), ('سيارة', 'O'), ('أجرة', 'O'), ('إلى', 'O'), ('حيدرة', 'B-LOC'), ('،', 'O'), ('وفي', 'O'), ('الساعة', 'O'), ('19', 'O'), (':', 'O'), ('05', 'O'), ('اتصلت', 'O'), ('به', 'O'), ('ليلى', 'B-PERS'), ('وقالت', 'O'), (':', 'O'), ('\"', 'O'), ('نلتقي', 'O'), ('أمام', 'O'), ('فندق', 'O'), ('الأوراسي', 'B-ORG'), ('،', 'O'), ('أنا', 'O'), ('مع', 'O'), ('كريم', 'B-PERS'), ('.', 'O'), ('\"', 'O'), ('عندما', 'O'), ('وصل', 'O'), ('كان', 'O'), ('بهو', 'O'), ('الفندق', 'O'), ('صاخبًا', 'O'), ('؛', 'O'), ('بعضهم', 'O'), ('يتحدث', 'O'), ('بالفصحى', 'O'), ('،', 'O'), ('وآخرون', 'O'), ('بالدارجة', 'O'), ('،', 'O'), ('وآخرون', 'O'), ('يخلطون', 'O'), ('الفرنسية', 'O'), ('بالإنجليزية', 'O'), (':', 'O'), ('someone', 'O'), ('said', 'O'), ('\"', 'O'), ('the', 'O'), ('meeting', 'O'), ('is', 'O'), ('postponed', 'O'), ('\"', 'O'), ('فرد', 'O'), ('آخر', 'O'), ('\"', 'O'), ('ماشي', 'O'), ('مشكل', 'O'), ('،', 'O'), ('نتكيفو', 'O'), ('\"', 'O'), ('.', 'O'), ('في', 'O'), ('لحظة', 'O'), ('لاحقة', 'O'), ('طلب', 'O'), ('منه', 'O'), ('حارس', 'O'), ('الأمن', 'O'), ('بطاقة', 'O'), ('الهوية', 'O'), ('،', 'O'), ('فتردد', 'O'), ('لأنه', 'O'), ('يحمل', 'O'), ('نسخة', 'O'), ('مصوّرة', 'O'), ('فقط', 'O'), ('،', 'O'), ('فبدأ', 'O'), ('نقاش', 'O'), ('طويل', 'O'), ('عن', 'O'), ('القاعدة', 'O'), ('والاستثناء', 'O'), ('و', 'O'), ('\"', 'O'), ('العقل', 'O'), ('\"', 'O'), ('،', 'O'), ('وكل', 'O'), ('ذلك', 'O'), ('يجعل', 'O'), ('الإحالات', 'O'), ('بالضمائر', 'O'), ('(', 'O'), ('هو', 'O'), ('،', 'O'), ('هي', 'O'), ('،', 'O'), ('هم', 'O'), ('،', 'O'), ('أحدهم', 'O'), (')', 'O'), ('اختبارًا', 'O'), ('صعبًا', 'O'), ('على', 'O'), ('الأنظمة', 'O'), ('التي', 'O'), ('لا', 'O'), ('تُحسن', 'O'), ('الاستفادة', 'O'), ('من', 'O'), ('السياق', 'O'), ('الممتد', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  PERS: ياسين\n",
      "  LOC: ديدوش مراد\n",
      "  LOC: حيدرة\n",
      "  PERS: ليلى\n",
      "  ORG: الأوراسي\n",
      "  PERS: كريم\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=94 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 11/94 = 11.70 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=4 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: وفق مذكرة تداولتها عدة منصات إعلامية، عالجت شركة SaharaPay نحو 2.4 مليون معاملة في الربع الرابع من 2025 مقابل 1.7 مليون في الربع السابق، بمتوسط سلة يبلغ 3,250 دينار جزائري وبنسبة فشل انخفضت من 2.9% إلى 1.8%. وأشارت المذكرة إلى مستثمرين مثل Atlas Ventures وCrescent Capital وصندوق مقره دبي، كما تحدثت عن شراكة \"قيد النقاش\" مع بنك عمومي وحل للدفع عبر الهاتف. يرى محلل مالي أن هدف الوصول إلى الربحية في 2027 \"طموح لكنه ممكن\" إذا استمرت وتيرة النمو فوق 25% سنويًا ولم ترتفع تكاليف الامتثال بشكل مفاجئ. في التعليقات، خلط بعض المستخدمين بين الشركة وعلامة أخرى باسم قريب، وظهرت إشاعات تتكرر بصيغ مختلفة، وهو ما يضغط على NER للتفريق بين المنظمات والأماكن والفترات الزمنية والأموال رغم الضجيج.\n",
      "         وفق  VB      lemma=وَفَق\n",
      "       مذكرة  NN      lemma=مُذَكَّر\n",
      "      تداولت  VB      lemma=تَداوَل\n",
      "          ها  PRP     lemma=ها\n",
      "         عدة  NN      lemma=عِدَة\n",
      "       منصات  NN      lemma=مِنَصَّة\n",
      "     إعلامية  JJ      lemma=إِعْلامِيّ\n",
      "           ،  PUNCT   lemma=∅\n",
      "       عالجت  VB      lemma=عالَج\n",
      "        شركة  NN      lemma=شِرْك\n",
      "   SaharaPay  NN      lemma=SaharaPay\n",
      "         نحو  IN      lemma=نَحْوَ\n",
      "           2  CD      lemma=2\n",
      "           .  PUNCT   lemma=∅\n",
      "           4  CD      lemma=4\n",
      "       مليون  NN      lemma=مِلْيُون\n",
      "      معاملة  NN      lemma=مَعْمَل\n",
      "          في  IN      lemma=في\n",
      "       الربع  NN      lemma=رَبْع\n",
      "      الرابع  JJ      lemma=رابِع\n",
      "          من  IN      lemma=من\n",
      "        2025  CD      lemma=2025\n",
      "       مقابل  NN      lemma=مُقابِل\n",
      "           1  CD      lemma=1\n",
      "           .  PUNCT   lemma=∅\n",
      "           7  CD      lemma=7\n",
      "       مليون  NN      lemma=مِلْيُون\n",
      "          في  IN      lemma=في\n",
      "       الربع  NN      lemma=رَبْع\n",
      "      السابق  JJ      lemma=سابِق\n",
      "           ،  PUNCT   lemma=∅\n",
      "           ب  IN      lemma=ب\n",
      "       متوسط  JJ      lemma=مُتَوَسِّط\n",
      "         سلة  NN      lemma=سَلَّة\n",
      "        يبلغ  VB      lemma=بَلُغ\n",
      "           3  CD      lemma=3\n",
      "           ,  PUNCT   lemma=∅\n",
      "         250  CD      lemma=250\n",
      "       دينار  NN      lemma=دِينار\n",
      "       جزائر  NNP     lemma=جَزائِر\n",
      "           ي  PRP     lemma=ي\n",
      "      وبنسبة  NN      lemma=نَسَب\n",
      "         فشل  VB      lemma=فَشِل\n",
      "      انخفضت  VB      lemma=ٱِنْخَفَض\n",
      "          من  IN      lemma=من\n",
      "           2  CD      lemma=2\n",
      "           .  PUNCT   lemma=∅\n",
      "           9  CD      lemma=9\n",
      "           %  PUNCT   lemma=∅\n",
      "         إلى  IN      lemma=إلى\n",
      "           1  CD      lemma=1\n",
      "           .  PUNCT   lemma=∅\n",
      "           8  CD      lemma=8\n",
      "           %  PUNCT   lemma=∅\n",
      "           .  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "       أشارت  VB      lemma=أَشار\n",
      "     المذكرة  NN      lemma=مُذَكِّرَة\n",
      "         إلى  IN      lemma=إلى\n",
      "    مستثمرين  NN      lemma=مُسْتَثْمِر\n",
      "         مثل  IN      lemma=مثل\n",
      "       Atlas  NN      lemma=Atlas\n",
      "    Ventures  NN      lemma=Ventures\n",
      "           و  CC      lemma=و\n",
      "    Crescent  NN      lemma=Crescent\n",
      "     Capital  NN      lemma=Capital\n",
      "      وصندوق  NN      lemma=صُنْدُوق\n",
      "         مقر  NN      lemma=مَقَرّ\n",
      "           ه  PRP     lemma=ه\n",
      "          دب  NNP     lemma=دُبّ\n",
      "           ي  PRP     lemma=ي\n",
      "           ،  PUNCT   lemma=∅\n",
      "         كما  CC      lemma=كَما\n",
      "       تحدثت  VB      lemma=تَحَدَّث\n",
      "          عن  IN      lemma=عن\n",
      "       شراكة  NN      lemma=شِراكَة\n",
      "           \"  PUNCT   lemma=∅\n",
      "         قيد  IN      lemma=قَيْدَ\n",
      "      النقاش  NN      lemma=نِقاش\n",
      "           \"  PUNCT   lemma=∅\n",
      "          مع  IN      lemma=مع\n",
      "           ب  IN      lemma=ب\n",
      "          نك  NNP     lemma=نك\n",
      "        عموم  NN      lemma=عُمُوم\n",
      "           ي  PRP     lemma=ي\n",
      "         وحل  VB      lemma=وَحِل\n",
      "           ل  IN      lemma=ل\n",
      "        لدفع  VB      lemma=دَفَع\n",
      "         عبر  IN      lemma=عبر\n",
      "      الهاتف  NN      lemma=هاتِف\n",
      "           .  PUNCT   lemma=∅\n",
      "         يرى  VB      lemma=وَرَى\n",
      "        محلل  JJ      lemma=مُحَلَّل\n",
      "         مال  VB      lemma=مال\n",
      "           ي  PRP     lemma=ي\n",
      "          أن  RP      lemma=أن\n",
      "         هدف  VB      lemma=هَدَف\n",
      "      الوصول  NN      lemma=وُصُول\n",
      "         إلى  IN      lemma=إلى\n",
      "     الربحية  JJ      lemma=رِبْحِيّ\n",
      "          في  IN      lemma=في\n",
      "        2027  CD      lemma=2027\n",
      "           \"  PUNCT   lemma=∅\n",
      "        طموح  JJ      lemma=طَمُوح\n",
      "        لكنه  CC      lemma=لكن\n",
      "          مم  IN      lemma=مِن\n",
      "          كن  PRP     lemma=كن\n",
      "           \"  PUNCT   lemma=∅\n",
      "         إذا  CC      lemma=إِذا\n",
      "      استمرت  VB      lemma=ٱِسْتَمَرّ\n",
      "           و  CC      lemma=و\n",
      "        تيرة  NNP     lemma=تيره\n",
      "       النمو  NN      lemma=نُمُوّ\n",
      "         فوق  NN      lemma=فَوْقَ\n",
      "          25  CD      lemma=25\n",
      "           %  PUNCT   lemma=∅\n",
      "           س  RP      lemma=س\n",
      "        نويا  VB      lemma=نَوَى\n",
      "         ولم  NN      lemma=لِمَ\n",
      "       ترتفع  VB      lemma=ٱِرْتَفَع\n",
      "      تكاليف  NN      lemma=تَكْلِيف\n",
      "    الامتثال  NN      lemma=ٱِمْتِثال\n",
      "           ب  IN      lemma=ب\n",
      "         شكل  VB      lemma=شَكِل\n",
      "       مفاجئ  JJ      lemma=مُفاجِئ\n",
      "           .  PUNCT   lemma=∅\n",
      "          في  IN      lemma=في\n",
      "   التعليقات  NN      lemma=تَعْلِيق\n",
      "           ،  PUNCT   lemma=∅\n",
      "         خلط  VB      lemma=خَلَط\n",
      "         بعض  NN      lemma=بعض\n",
      "  المستخدمين  NN      lemma=مُسْتَخْدَم\n",
      "         بين  IN      lemma=بين\n",
      "      الشركة  NN      lemma=شَرِكَة\n",
      "      وعلامة  NN      lemma=عَلامَة\n",
      "        أخرى  JJ      lemma=آخَر\n",
      "        باسم  NNP     lemma=باسِم\n",
      "        قريب  JJ      lemma=قَرِيب\n",
      "           ،  PUNCT   lemma=∅\n",
      "       وظهرت  VB      lemma=ظَهَر\n",
      "      إشاعات  NN      lemma=إِشاعَة\n",
      "       تتكرر  VB      lemma=تَكَرَّر\n",
      "           ب  IN      lemma=ب\n",
      "         صيغ  NN      lemma=صِيغَة\n",
      "      مختلفة  JJ      lemma=مُخْتَلَف\n",
      "           ،  PUNCT   lemma=∅\n",
      "         وهو  PRP     lemma=هُوَ\n",
      "          ما  RP      lemma=ما\n",
      "        يضغط  VB      lemma=ضَغَط\n",
      "         على  IN      lemma=على\n",
      "         NER  NN      lemma=NER\n",
      "           ل  IN      lemma=ل\n",
      "      لتفريق  NN      lemma=تَفْرِيق\n",
      "         بين  IN      lemma=بين\n",
      "    المنظمات  NN      lemma=مُنَظِّم\n",
      "           و  CC      lemma=و\n",
      "       الأما  NN      lemma=أَلَم\n",
      "          كن  PRP     lemma=كن\n",
      "           و  CC      lemma=و\n",
      "     الفترات  NN      lemma=فَتْرَة\n",
      "     الزمنية  JJ      lemma=زَمَنِيّ\n",
      "           و  CC      lemma=و\n",
      "     الأموال  NN      lemma=مال\n",
      "         رغم  IN      lemma=رَغْمَ\n",
      "      الضجيج  NN      lemma=ضَجِيج\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('وفق', 'O'), ('مذكرة', 'O'), ('تداولتها', 'O'), ('عدة', 'O'), ('منصات', 'O'), ('إعلامية', 'O'), ('،', 'O'), ('عالجت', 'O'), ('شركة', 'O'), ('SaharaPay', 'B-ORG'), ('نحو', 'O'), ('2', 'O'), ('.', 'O'), ('4', 'O'), ('مليون', 'O'), ('معاملة', 'O'), ('في', 'O'), ('الربع', 'O'), ('الرابع', 'O'), ('من', 'O'), ('2025', 'B-DATE'), ('مقابل', 'O'), ('1', 'O'), ('.', 'O'), ('7', 'O'), ('مليون', 'O'), ('في', 'O'), ('الربع', 'O'), ('السابق', 'O'), ('،', 'O'), ('بمتوسط', 'O'), ('سلة', 'O'), ('يبلغ', 'O'), ('3', 'O'), (',', 'O'), ('250', 'O'), ('دينار', 'B-MISC'), ('جزائري', 'O'), ('وبنسبة', 'O'), ('فشل', 'O'), ('انخفضت', 'O'), ('من', 'O'), ('2', 'O'), ('.', 'O'), ('9', 'O'), ('%', 'O'), ('إلى', 'O'), ('1', 'O'), ('.', 'O'), ('8', 'O'), ('%', 'O'), ('.', 'O'), ('وأشارت', 'O'), ('المذكرة', 'O'), ('إلى', 'O'), ('مستثمرين', 'O'), ('مثل', 'O'), ('Atlas', 'B-ORG'), ('Ventures', 'I-ORG'), ('و', 'O'), ('Crescent', 'B-ORG'), ('Capital', 'I-ORG'), ('وصندوق', 'O'), ('مقره', 'O'), ('دبي', 'B-LOC'), ('،', 'O'), ('كما', 'O'), ('تحدثت', 'O'), ('عن', 'O'), ('شراكة', 'O'), ('\"', 'O'), ('قيد', 'O'), ('النقاش', 'O'), ('\"', 'O'), ('مع', 'O'), ('بنك', 'O'), ('عمومي', 'O'), ('وحل', 'O'), ('للدفع', 'O'), ('عبر', 'O'), ('الهاتف', 'O'), ('.', 'O'), ('يرى', 'O'), ('محلل', 'O'), ('مالي', 'O'), ('أن', 'O'), ('هدف', 'O'), ('الوصول', 'O'), ('إلى', 'O'), ('الربحية', 'O'), ('في', 'O'), ('2027', 'B-DATE'), ('\"', 'O'), ('طموح', 'O'), ('لكنه', 'O'), ('ممكن', 'O'), ('\"', 'O'), ('إذا', 'O'), ('استمرت', 'O'), ('وتيرة', 'O'), ('النمو', 'O'), ('فوق', 'O'), ('25', 'O'), ('%', 'O'), ('سنويًا', 'O'), ('ولم', 'O'), ('ترتفع', 'O'), ('تكاليف', 'O'), ('الامتثال', 'O'), ('بشكل', 'O'), ('مفاجئ', 'O'), ('.', 'O'), ('في', 'O'), ('التعليقات', 'O'), ('،', 'O'), ('خلط', 'O'), ('بعض', 'O'), ('المستخدمين', 'O'), ('بين', 'O'), ('الشركة', 'O'), ('وعلامة', 'O'), ('أخرى', 'O'), ('باسم', 'O'), ('قريب', 'O'), ('،', 'O'), ('وظهرت', 'O'), ('إشاعات', 'O'), ('تتكرر', 'O'), ('بصيغ', 'O'), ('مختلفة', 'O'), ('،', 'O'), ('وهو', 'O'), ('ما', 'O'), ('يضغط', 'O'), ('على', 'O'), ('NER', 'O'), ('للتفريق', 'O'), ('بين', 'O'), ('المنظمات', 'O'), ('والأماكن', 'O'), ('والفترات', 'O'), ('الزمنية', 'O'), ('والأموال', 'O'), ('رغم', 'O'), ('الضجيج', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  ORG: SaharaPay\n",
      "  DATE: 2025\n",
      "  MISC: دينار\n",
      "  ORG: Atlas Ventures\n",
      "  ORG: Crescent Capital\n",
      "  LOC: دبي\n",
      "  DATE: 2027\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=90 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 16/90 = 17.78 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=5 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: بصراحة ما فهمتش علاش خدمة FastLivr راهي تتعطل بزاف من 02-2026… البارح طلبت على 18:00 وكان مكتوب \"يوصل 19:30\"، وبعدها بدقيقة ولات \"Annulé\" على 19:29! الدعم جاوبني: \"Bonjour، dossier #A-9981 en cours.\" بصح \"en cours\" هذي واش معناها، خمس دقايق ولا خمس أيام؟ بعثت إيميل لـ support@fastlivr.dz وبعثت ميساج فالإنستغرام @fastlivr_officiel وماكان حتى رد. العنوان كان واضح: 12، شارع الإخوة بوعدو، القبة، الجزائر، وزاد اتصل برقم آخر وقال \"السيستام\"… إذا نظامك يتعامل مع لهجة/اختصارات (راه، بزاف، ماكانش، بصح) ومع الفرنسية داخل النص، ويقدر يلتقط المؤسسة والتاريخ التقريبي ومعرّف التذكرة والمكان، فهذا اختبار قوي.\n",
      "           ب  IN      lemma=ب\n",
      "       صراحة  NN      lemma=صُراح\n",
      "          ما  RP      lemma=ما\n",
      "       فهمتش  NNP     lemma=فهمتش\n",
      "        علاش  NNP     lemma=علاش\n",
      "        خدمة  VB      lemma=خَدَم\n",
      "    FastLivr  NN      lemma=FastLivr\n",
      "          را  NNP     lemma=را\n",
      "           ه  PRP     lemma=ه\n",
      "           ي  PRP     lemma=ي\n",
      "       تتعطل  VB      lemma=تَعَطَّل\n",
      "        بزاف  NNP     lemma=بزاف\n",
      "          من  IN      lemma=من\n",
      "          02  CD      lemma=02\n",
      "           -  PUNCT   lemma=∅\n",
      "        2026  CD      lemma=2026\n",
      "           …  PUNCT   lemma=∅\n",
      "      البارح  NN      lemma=بارِح\n",
      "        طلبت  VB      lemma=طَلَب\n",
      "         على  IN      lemma=على\n",
      "          18  CD      lemma=18\n",
      "           :  PUNCT   lemma=∅\n",
      "          00  CD      lemma=00\n",
      "        وكان  VB      lemma=كان\n",
      "       مكتوب  JJ      lemma=مَكْتُوب\n",
      "           \"  PUNCT   lemma=∅\n",
      "        يوصل  VB      lemma=وَصَل\n",
      "          19  CD      lemma=19\n",
      "           :  PUNCT   lemma=∅\n",
      "          30  CD      lemma=30\n",
      "           \"  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "        وبعد  NN      lemma=بُعْد\n",
      "          ها  PRP     lemma=ها\n",
      "           ب  IN      lemma=ب\n",
      "       دقيقة  NN      lemma=دَقِيق\n",
      "        ولات  NN      lemma=آتِي\n",
      "           \"  PUNCT   lemma=∅\n",
      "       Annul  NN      lemma=Annul\n",
      "           é  NN      lemma=é\n",
      "           \"  PUNCT   lemma=∅\n",
      "         على  IN      lemma=على\n",
      "          19  CD      lemma=19\n",
      "           :  PUNCT   lemma=∅\n",
      "          29  CD      lemma=29\n",
      "           !  PUNCT   lemma=∅\n",
      "       الدعم  NN      lemma=دَعْم\n",
      "        جاوب  VB      lemma=جاوَب\n",
      "          ني  PRP     lemma=ني\n",
      "           :  PUNCT   lemma=∅\n",
      "           \"  PUNCT   lemma=∅\n",
      "     Bonjour  NN      lemma=Bonjour\n",
      "           ،  PUNCT   lemma=∅\n",
      "     dossier  NN      lemma=dossier\n",
      "           #  NN      lemma=#\n",
      "           A  NN      lemma=A\n",
      "           -  PUNCT   lemma=∅\n",
      "        9981  CD      lemma=9981\n",
      "          en  NN      lemma=en\n",
      "       cours  NN      lemma=cours\n",
      "           .  PUNCT   lemma=∅\n",
      "           \"  PUNCT   lemma=∅\n",
      "         بصح  NNP     lemma=بصح\n",
      "           \"  PUNCT   lemma=∅\n",
      "          en  NN      lemma=en\n",
      "       cours  NN      lemma=cours\n",
      "           \"  PUNCT   lemma=∅\n",
      "          هذ  NNP     lemma=هذ\n",
      "           ي  PRP     lemma=ي\n",
      "         واش  NN      lemma=واشِي\n",
      "          مع  IN      lemma=مَع\n",
      "          نا  PRP     lemma=نا\n",
      "          ها  PRP     lemma=ها\n",
      "           ،  PUNCT   lemma=∅\n",
      "         خمس  NN      lemma=خُمْس\n",
      "       دقايق  NNP     lemma=دقايق\n",
      "         ولا  NN      lemma=لا\n",
      "         خمس  NN      lemma=خُمْس\n",
      "        أيام  NN      lemma=يَوْم\n",
      "           ؟  PUNCT   lemma=∅\n",
      "        بعثت  VB      lemma=بَعَث\n",
      "       إيميل  NNP     lemma=ايميل\n",
      "           ل  IN      lemma=لِ\n",
      "     support  NN      lemma=support\n",
      "           @  PUNCT   lemma=∅\n",
      "    fastlivr  NN      lemma=fastlivr\n",
      "           .  PUNCT   lemma=∅\n",
      "          dz  NN      lemma=dz\n",
      "       وبعثت  VB      lemma=بَعَث\n",
      "       ميساج  NNP     lemma=ميساج\n",
      "           ف  CC      lemma=ف\n",
      "  الإنستغرام  NNP     lemma=الانستغرام\n",
      "           @  PUNCT   lemma=∅\n",
      "    fastlivr  NN      lemma=fastlivr\n",
      "           _  NN      lemma=_\n",
      "    officiel  NN      lemma=officiel\n",
      "      وماكان  NNP     lemma=وماكان\n",
      "         حتى  IN      lemma=حتى\n",
      "          رد  NN      lemma=رَدّ\n",
      "           .  PUNCT   lemma=∅\n",
      "     العنوان  NN      lemma=عُنْوان\n",
      "           ك  IN      lemma=ك\n",
      "          ان  NN      lemma=أَنْ\n",
      "        واضح  JJ      lemma=واضِح\n",
      "           :  PUNCT   lemma=∅\n",
      "          12  CD      lemma=12\n",
      "           ،  PUNCT   lemma=∅\n",
      "        شارع  NN      lemma=شارِع\n",
      "      الإخوة  NN      lemma=إِخْوَة\n",
      "       بوعدو  NNP     lemma=بوعدو\n",
      "           ،  PUNCT   lemma=∅\n",
      "       القبة  NN      lemma=قُبَّة\n",
      "           ،  PUNCT   lemma=∅\n",
      "     الجزائر  NNP     lemma=جَزائِر\n",
      "           ،  PUNCT   lemma=∅\n",
      "        وزاد  NN      lemma=زاد\n",
      "        اتصل  VB      lemma=ٱِتَّصَل\n",
      "           ب  IN      lemma=ب\n",
      "         رقم  VB      lemma=رَقَم\n",
      "         آخر  JJ      lemma=آخَر\n",
      "        وقال  VB      lemma=قال\n",
      "           \"  PUNCT   lemma=∅\n",
      "    السيستام  NNP     lemma=السيستام\n",
      "           \"  PUNCT   lemma=∅\n",
      "           …  PUNCT   lemma=∅\n",
      "         إذا  CC      lemma=إِذا\n",
      "        نظام  NN      lemma=نِظام\n",
      "           ك  PRP     lemma=ك\n",
      "      يتعامل  VB      lemma=تَعامَل\n",
      "          مع  IN      lemma=مع\n",
      "        لهجة  NN      lemma=لُهْجَة\n",
      "           /  PUNCT   lemma=∅\n",
      "    اختصارات  NN      lemma=ٱِخْتِصار\n",
      "           (  PUNCT   lemma=∅\n",
      "          را  NNP     lemma=را\n",
      "           ه  PRP     lemma=ه\n",
      "           ،  PUNCT   lemma=∅\n",
      "        بزاف  NNP     lemma=بزاف\n",
      "           ،  PUNCT   lemma=∅\n",
      "      ماكانش  NNP     lemma=ماكانش\n",
      "           ،  PUNCT   lemma=∅\n",
      "         بصح  NNP     lemma=بصح\n",
      "           )  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "          مع  IN      lemma=مَع\n",
      "    الفرنسية  NN      lemma=فَرَنْسِيّ\n",
      "        داخل  VB      lemma=داخَل\n",
      "        النص  NN      lemma=نَصّ\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "        يقدر  VB      lemma=قَدِر\n",
      "       يلتقط  VB      lemma=ٱِلْتَقَط\n",
      "     المؤسسة  JJ      lemma=مُؤَسِّس\n",
      "           و  CC      lemma=و\n",
      "     التاريخ  NN      lemma=تَأْرِيخ\n",
      "     التقريب  NN      lemma=تَقْرِيب\n",
      "           ي  PRP     lemma=ي\n",
      "       ومعرف  NNP     lemma=ومعرف\n",
      "     التذكرة  NN      lemma=تَذْكَرَة\n",
      "           و  CC      lemma=و\n",
      "      المكان  NN      lemma=مَكان\n",
      "           ،  PUNCT   lemma=∅\n",
      "        فهذا  PRP     lemma=هٰذا\n",
      "      اختبار  NN      lemma=ٱِخْتِبار\n",
      "          قو  NNP     lemma=قو\n",
      "           ي  PRP     lemma=ي\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('بصراحة', 'O'), ('ما', 'O'), ('فهمتش', 'O'), ('علاش', 'O'), ('خدمة', 'O'), ('FastLivr', 'B-MISC'), ('راهي', 'I-MISC'), ('تتعطل', 'O'), ('بزاف', 'O'), ('من', 'O'), ('02', 'O'), ('-', 'O'), ('2026', 'B-DATE'), ('…', 'O'), ('البارح', 'O'), ('طلبت', 'O'), ('على', 'O'), ('18', 'O'), (':', 'O'), ('00', 'O'), ('وكان', 'O'), ('مكتوب', 'O'), ('\"', 'O'), ('يوصل', 'O'), ('19', 'O'), (':', 'O'), ('30', 'O'), ('\"', 'O'), ('،', 'O'), ('وبعدها', 'O'), ('بدقيقة', 'O'), ('ولات', 'O'), ('\"', 'O'), ('Annul', 'O'), ('é', 'O'), ('\"', 'O'), ('على', 'O'), ('19', 'O'), (':', 'O'), ('29', 'O'), ('!', 'O'), ('الدعم', 'O'), ('جاوبني', 'O'), (':', 'O'), ('\"', 'O'), ('Bonjour', 'B-PERS'), ('،', 'O'), ('dossier', 'O'), ('#', 'O'), ('A', 'O'), ('-', 'O'), ('9981', 'O'), ('en', 'O'), ('cours', 'O'), ('.', 'O'), ('\"', 'O'), ('بصح', 'O'), ('\"', 'O'), ('en', 'O'), ('cours', 'O'), ('\"', 'O'), ('هذي', 'O'), ('واش', 'O'), ('معناها', 'O'), ('،', 'O'), ('خمس', 'O'), ('دقايق', 'O'), ('ولا', 'O'), ('خمس', 'O'), ('أيام', 'O'), ('؟', 'O'), ('بعثت', 'O'), ('إيميل', 'O'), ('لـ', 'O'), ('support', 'O'), ('@', 'O'), ('fastlivr', 'O'), ('.', 'O'), ('dz', 'O'), ('وبعثت', 'O'), ('ميساج', 'B-PERS'), ('فالإنستغرام', 'B-ORG'), ('@', 'O'), ('fastlivr', 'O'), ('_', 'O'), ('officiel', 'B-MISC'), ('وماكان', 'O'), ('حتى', 'O'), ('رد', 'O'), ('.', 'O'), ('العنوان', 'O'), ('كان', 'O'), ('واضح', 'O'), (':', 'O'), ('12', 'O'), ('،', 'O'), ('شارع', 'O'), ('الإخوة', 'O'), ('بوعدو', 'B-PERS'), ('،', 'O'), ('القبة', 'B-LOC'), ('،', 'O'), ('الجزائر', 'B-LOC'), ('،', 'O'), ('وزاد', 'O'), ('اتصل', 'O'), ('برقم', 'O'), ('آخر', 'O'), ('وقال', 'O'), ('\"', 'O'), ('السيستام', 'B-MISC'), ('\"', 'O'), ('…', 'O'), ('إذا', 'O'), ('نظامك', 'O'), ('يتعامل', 'O'), ('مع', 'O'), ('لهجة', 'O'), ('/', 'O'), ('اختصارات', 'O'), ('(', 'O'), ('راه', 'O'), ('،', 'O'), ('بزاف', 'O'), ('،', 'O'), ('ماكانش', 'O'), ('،', 'O'), ('بصح', 'O'), (')', 'O'), ('ومع', 'O'), ('الفرنسية', 'O'), ('داخل', 'O'), ('النص', 'O'), ('،', 'O'), ('ويقدر', 'O'), ('يلتقط', 'O'), ('المؤسسة', 'O'), ('والتاريخ', 'O'), ('التقريبي', 'O'), ('ومعرّف', 'O'), ('التذكرة', 'O'), ('والمكان', 'O'), ('،', 'O'), ('فهذا', 'O'), ('اختبار', 'O'), ('قوي', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  MISC: FastLivr راهي\n",
      "  DATE: 2026\n",
      "  PERS: Bonjour\n",
      "  PERS: ميساج\n",
      "  ORG: الإنستغرام\n",
      "  MISC: officiel\n",
      "  PERS: بوعدو\n",
      "  LOC: القبة\n",
      "  LOC: الجزائر\n",
      "  MISC: السيستام\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=89 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 7/89 = 7.87 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=6 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: في صيف 2024 زار فريق طلابي من قسنطينة وتلمسان وغرداية متحف الباردو ثم انتقل إلى تيبازة لحضور مهرجان محلي استمر ثلاثة أيام، قبل أن يسافر أحدهم إلى تونس العاصمة ثم إلى صفاقس لحضور ورشة تدريب. كتبوا في تقريرهم أن التنقل بين المدن كشف اختلاف اللهجات والعادات، وأن لافتات الشوارع تحمل أحيانًا أسماء تاريخية وأحيانًا أسماء أشخاص، وأن أسماء الأماكن قد تتشابه مع أسماء العائلات. لاحقًا، في ديسمبر 2025، أعادوا قراءة التقرير وأضافوا ملحقًا عن تكاليف الرحلة: 45,000 دج للنقل والإقامة، و12,500 دج للزيارات، مع مقارنة بين 2024 و2025 في عدد المشاركين. هذا النوع من النصوص يختبر قدرة NER على التقاط المدن والبلدان والأحداث والمبالغ، وقدرة اللمّتة على التعامل مع صيغ الجمع والتثنية والتأنيث.\n",
      "          في  IN      lemma=في\n",
      "         صيف  NN      lemma=صَيْف\n",
      "        2024  CD      lemma=2024\n",
      "         زار  VB      lemma=زَأَر\n",
      "        فريق  NNP     lemma=فَرِيق\n",
      "        طلاب  NN      lemma=طالِب\n",
      "           ي  PRP     lemma=ي\n",
      "          من  IN      lemma=من\n",
      "     قسنطينة  NNP     lemma=قُسَنْطِينَة\n",
      "           و  CC      lemma=و\n",
      "      تلمسان  VB      lemma=لَمَس\n",
      "     وغرداية  NNP     lemma=وغردايه\n",
      "        متحف  NN      lemma=مَتْحَف\n",
      "     الباردو  NNP     lemma=الباردو\n",
      "          ثم  CC      lemma=ثم\n",
      "       انتقل  VB      lemma=ٱِنْتَقَل\n",
      "         إلى  IN      lemma=إلى\n",
      "      تيبازة  NNP     lemma=تِيبازَه\n",
      "           ل  IN      lemma=ل\n",
      "        حضور  NN      lemma=حُضُور\n",
      "      مهرجان  NN      lemma=مَهْرَجان\n",
      "         محل  VB      lemma=مَحَل\n",
      "           ي  PRP     lemma=ي\n",
      "       استمر  VB      lemma=ٱِسْتَمَرّ\n",
      "       ثلاثة  JJ      lemma=ثَلاث\n",
      "        أيام  NN      lemma=يَوْم\n",
      "           ،  PUNCT   lemma=∅\n",
      "         قبل  IN      lemma=قبل\n",
      "          أن  RP      lemma=أن\n",
      "       يسافر  VB      lemma=سافَر\n",
      "         أحد  NNP     lemma=أَحَد\n",
      "          هم  PRP     lemma=هم\n",
      "         إلى  IN      lemma=إلى\n",
      "        تونس  VB      lemma=تَوْنَس\n",
      "     العاصمة  NN      lemma=عاصِمَة\n",
      "          ثم  CC      lemma=ثم\n",
      "         إلى  IN      lemma=إلى\n",
      "       صفاقس  NNP     lemma=صَفاقِس\n",
      "           ل  IN      lemma=ل\n",
      "        حضور  NN      lemma=حُضُور\n",
      "        ورشة  NN      lemma=وَرْشَة\n",
      "       تدريب  NN      lemma=تَدْرِيب\n",
      "           .  PUNCT   lemma=∅\n",
      "       كتبوا  VB      lemma=كَتَب\n",
      "          في  IN      lemma=في\n",
      "       تقرير  NN      lemma=تَقْرِير\n",
      "          هم  PRP     lemma=هم\n",
      "          أن  RP      lemma=أن\n",
      "      التنقل  NN      lemma=تَنَقُّل\n",
      "         بين  IN      lemma=بين\n",
      "       المدن  NN      lemma=مَدِينَة\n",
      "           ك  IN      lemma=ك\n",
      "          شف  NN      lemma=شِفّ\n",
      "      اختلاف  NN      lemma=ٱِخْتِلاف\n",
      "     اللهجات  NN      lemma=لَهْجَة\n",
      "           و  CC      lemma=و\n",
      "     العادات  NN      lemma=عادَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "         وأن  NN      lemma=وَإِن\n",
      "      لافتات  VB      lemma=ٱِفْتات\n",
      "     الشوارع  NN      lemma=شارِع\n",
      "        تحمل  VB      lemma=حَمَل\n",
      "        أحيا  VB      lemma=حَيّ\n",
      "          نا  PRP     lemma=نا\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "     تاريخية  NN      lemma=تارِيخ\n",
      "           و  CC      lemma=و\n",
      "        أحيا  VB      lemma=حَيّ\n",
      "          نا  PRP     lemma=نا\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "       أشخاص  NN      lemma=شَخْص\n",
      "           ،  PUNCT   lemma=∅\n",
      "         وأن  NN      lemma=وَإِن\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "       الأما  NN      lemma=أَلَم\n",
      "          كن  PRP     lemma=كن\n",
      "          قد  RP      lemma=قد\n",
      "       تتشاب  NNP     lemma=تتشاب\n",
      "           ه  PRP     lemma=ه\n",
      "          مع  IN      lemma=مع\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "    العائلات  NN      lemma=عائِلَة\n",
      "           .  PUNCT   lemma=∅\n",
      "           ل  IN      lemma=ل\n",
      "        احقا  VB      lemma=أَحَقّ\n",
      "           ،  PUNCT   lemma=∅\n",
      "          في  IN      lemma=في\n",
      "      ديسمبر  NNP     lemma=دِيسَمْبِر\n",
      "        2025  CD      lemma=2025\n",
      "           ،  PUNCT   lemma=∅\n",
      "      أعادوا  VB      lemma=أَعاد\n",
      "       قراءة  NN      lemma=قِراءَة\n",
      "     التقرير  NN      lemma=تَقْرِير\n",
      "           و  CC      lemma=و\n",
      "      أضافوا  VB      lemma=أَضاف\n",
      "       ملحقا  JJ      lemma=مُلْحَق\n",
      "          عن  IN      lemma=عن\n",
      "      تكاليف  NN      lemma=تَكْلِيف\n",
      "      الرحلة  NN      lemma=رِحْلَة\n",
      "           :  PUNCT   lemma=∅\n",
      "          45  CD      lemma=45\n",
      "           ,  PUNCT   lemma=∅\n",
      "         000  CD      lemma=000\n",
      "          دج  VB      lemma=دَجّ\n",
      "           ل  IN      lemma=ل\n",
      "        لنقل  VB      lemma=نَقَل\n",
      "           و  CC      lemma=و\n",
      "     الإقامة  NN      lemma=إِقامَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "          12  CD      lemma=12\n",
      "           ,  PUNCT   lemma=∅\n",
      "         500  CD      lemma=500\n",
      "          دج  VB      lemma=دَجّ\n",
      "           ل  IN      lemma=ل\n",
      "     لزيارات  NN      lemma=زِيارَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "          مع  IN      lemma=مع\n",
      "      مقارنة  JJ      lemma=مُقارَن\n",
      "         بين  IN      lemma=بين\n",
      "        2024  CD      lemma=2024\n",
      "           و  CC      lemma=و\n",
      "        2025  CD      lemma=2025\n",
      "          في  IN      lemma=في\n",
      "         عدد  NN      lemma=عَدَد\n",
      "   المشاركين  NN      lemma=مُشارِك\n",
      "           .  PUNCT   lemma=∅\n",
      "         هذا  PRP     lemma=هذا\n",
      "       النوع  NN      lemma=نَوْع\n",
      "          من  IN      lemma=من\n",
      "      النصوص  NN      lemma=نَصّ\n",
      "       يختبر  VB      lemma=ٱِخْتَبَر\n",
      "        قدرة  NN      lemma=قِدْر\n",
      "         NER  NN      lemma=NER\n",
      "         على  IN      lemma=على\n",
      "      التقاط  NN      lemma=ٱِلْتِقاط\n",
      "       المدن  NN      lemma=مَدِينَة\n",
      "           و  CC      lemma=و\n",
      "     البلدان  NN      lemma=بَلَد\n",
      "           و  CC      lemma=و\n",
      "     الأحداث  NN      lemma=حَدَث\n",
      "           و  CC      lemma=و\n",
      "     المبالغ  NN      lemma=مَبْلَغ\n",
      "           ،  PUNCT   lemma=∅\n",
      "       وقدرة  NN      lemma=قِدْر\n",
      "      اللمتة  NNP     lemma=اللمته\n",
      "         على  IN      lemma=على\n",
      "     التعامل  NN      lemma=تَعامُل\n",
      "          مع  IN      lemma=مع\n",
      "         صيغ  NN      lemma=صِيغَة\n",
      "       الجمع  NN      lemma=جَمْع\n",
      "           و  CC      lemma=و\n",
      "     التثنية  NN      lemma=تَثْنِيَة\n",
      "           و  CC      lemma=و\n",
      "     التأنيث  NN      lemma=تَأْنِيث\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('في', 'O'), ('صيف', 'O'), ('2024', 'B-DATE'), ('زار', 'O'), ('فريق', 'O'), ('طلابي', 'O'), ('من', 'O'), ('قسنطينة', 'B-LOC'), ('وتلمسان', 'B-LOC'), ('وغرداية', 'B-LOC'), ('متحف', 'O'), ('الباردو', 'B-ORG'), ('ثم', 'O'), ('انتقل', 'O'), ('إلى', 'O'), ('تيبازة', 'B-LOC'), ('لحضور', 'O'), ('مهرجان', 'O'), ('محلي', 'O'), ('استمر', 'O'), ('ثلاثة', 'O'), ('أيام', 'O'), ('،', 'O'), ('قبل', 'O'), ('أن', 'O'), ('يسافر', 'O'), ('أحدهم', 'O'), ('إلى', 'O'), ('تونس', 'B-LOC'), ('العاصمة', 'O'), ('ثم', 'O'), ('إلى', 'O'), ('صفاقس', 'B-LOC'), ('لحضور', 'O'), ('ورشة', 'O'), ('تدريب', 'O'), ('.', 'O'), ('كتبوا', 'O'), ('في', 'O'), ('تقريرهم', 'O'), ('أن', 'O'), ('التنقل', 'O'), ('بين', 'O'), ('المدن', 'O'), ('كشف', 'O'), ('اختلاف', 'O'), ('اللهجات', 'O'), ('والعادات', 'O'), ('،', 'O'), ('وأن', 'O'), ('لافتات', 'O'), ('الشوارع', 'O'), ('تحمل', 'O'), ('أحيانًا', 'O'), ('أسماء', 'O'), ('تاريخية', 'O'), ('وأحيانًا', 'O'), ('أسماء', 'O'), ('أشخاص', 'O'), ('،', 'O'), ('وأن', 'O'), ('أسماء', 'O'), ('الأماكن', 'O'), ('قد', 'O'), ('تتشابه', 'O'), ('مع', 'O'), ('أسماء', 'O'), ('العائلات', 'O'), ('.', 'O'), ('لاحقًا', 'O'), ('،', 'O'), ('في', 'O'), ('ديسمبر', 'O'), ('2025', 'B-DATE'), ('،', 'O'), ('أعادوا', 'O'), ('قراءة', 'O'), ('التقرير', 'O'), ('وأضافوا', 'O'), ('ملحقًا', 'O'), ('عن', 'O'), ('تكاليف', 'O'), ('الرحلة', 'O'), (':', 'O'), ('45', 'O'), (',', 'O'), ('000', 'O'), ('دج', 'O'), ('للنقل', 'O'), ('والإقامة', 'O'), ('،', 'O'), ('و', 'O'), ('12', 'O'), (',', 'O'), ('500', 'O'), ('دج', 'O'), ('للزيارات', 'O'), ('،', 'O'), ('مع', 'O'), ('مقارنة', 'O'), ('بين', 'O'), ('2024', 'B-DATE'), ('و', 'O'), ('2025', 'B-DATE'), ('في', 'O'), ('عدد', 'O'), ('المشاركين', 'O'), ('.', 'O'), ('هذا', 'O'), ('النوع', 'O'), ('من', 'O'), ('النصوص', 'O'), ('يختبر', 'O'), ('قدرة', 'O'), ('NER', 'O'), ('على', 'O'), ('التقاط', 'O'), ('المدن', 'O'), ('والبلدان', 'O'), ('والأحداث', 'O'), ('والمبالغ', 'O'), ('،', 'O'), ('وقدرة', 'O'), ('اللمّتة', 'O'), ('على', 'O'), ('التعامل', 'O'), ('مع', 'O'), ('صيغ', 'O'), ('الجمع', 'O'), ('والتثنية', 'O'), ('والتأنيث', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  DATE: 2024\n",
      "  LOC: قسنطينة\n",
      "  LOC: وتلمسان\n",
      "  LOC: وغرداية\n",
      "  ORG: الباردو\n",
      "  LOC: تيبازة\n",
      "  LOC: تونس\n",
      "  LOC: صفاقس\n",
      "  DATE: 2025\n",
      "  DATE: 2024\n",
      "  DATE: 2025\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=83 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 9/83 = 10.84 %\n",
      "\n",
      "##########################################################################################\n",
      "DOC=arab.docx | page=1 | sent=7 | lang=ar\n",
      "##########################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "INPUT: نشرت وحدة بحث في مستشفى جامعي بالجزائر دراسة رصدية عن متابعة مرضى السكري من النوع الثاني بين 2021 و2025، وذكرت أن العينة شملت 312 مشاركًا تتراوح أعمارهم بين 35 و70 سنة، مع تقسيم فرعي حسب الالتزام بالمتابعة الشهرية. تضمن الملخص أسماء مؤسسات مثل كلية الطب بجامعة الجزائر ومنظمة صحية إقليمية، وأشار إلى تواريخ زيارات محددة (15/03/2023، 07/11/2024) وإلى قياسات رقمية تتكرر بصيغ مختلفة (HbA1c، ضغط الدم، مؤشر كتلة الجسم)، كما احتوى على اختصارات عربية وأجنبية في الجملة نفسها. مثل هذه الفقرة تختبر POS واللمّتة في مصطلحات مركبة، وتختبر NER في أسماء المؤسسات والتواريخ والأعداد دون الحاجة إلى أي استنتاجات علاجية.\n",
      "        نشرت  VB      lemma=نَشَر\n",
      "        وحدة  NNP     lemma=وَحْدَة\n",
      "         بحث  VB      lemma=بَحَث\n",
      "          في  IN      lemma=في\n",
      "      مستشفى  NN      lemma=مُسْتَشْفَى\n",
      "        جامع  VB      lemma=جامَع\n",
      "           ي  PRP     lemma=ي\n",
      "           ب  IN      lemma=ب\n",
      "     الجزائر  NNP     lemma=جَزائِر\n",
      "       دراسة  NN      lemma=دِراسَة\n",
      "       رصدية  NN      lemma=رَصَد\n",
      "          عن  IN      lemma=عن\n",
      "      متابعة  NN      lemma=مُتابَع\n",
      "        مرضى  NN      lemma=مَرَض\n",
      "       السكر  NN      lemma=سُكْر\n",
      "           ي  PRP     lemma=ي\n",
      "          من  IN      lemma=من\n",
      "       النوع  NN      lemma=نَوْع\n",
      "        الثا  NNP     lemma=الثا\n",
      "          ني  PRP     lemma=ني\n",
      "         بين  IN      lemma=بين\n",
      "        2021  CD      lemma=2021\n",
      "           و  CC      lemma=و\n",
      "        2025  CD      lemma=2025\n",
      "           ،  PUNCT   lemma=∅\n",
      "       وذكرت  VB      lemma=ذَكَر\n",
      "          أن  RP      lemma=أن\n",
      "      العينة  NN      lemma=عِينَة\n",
      "        شملت  VB      lemma=شَمِل\n",
      "         312  CD      lemma=312\n",
      "      مشاركا  NN      lemma=مُشارِك\n",
      "      تتراوح  VB      lemma=تَراوَح\n",
      "       أعمار  NN      lemma=عُمْر\n",
      "          هم  PRP     lemma=هم\n",
      "         بين  IN      lemma=بين\n",
      "          35  CD      lemma=35\n",
      "           و  CC      lemma=و\n",
      "          70  CD      lemma=70\n",
      "         سنة  NN      lemma=سَنَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "          مع  IN      lemma=مع\n",
      "       تقسيم  NN      lemma=تَقْسِيم\n",
      "         فرع  VB      lemma=فَرَّع\n",
      "           ي  PRP     lemma=ي\n",
      "         حسب  VB      lemma=حَسَب\n",
      "    الالتزام  NN      lemma=ٱِلْتِزام\n",
      "           ب  IN      lemma=ب\n",
      "    المتابعة  NN      lemma=مُتابَع\n",
      "     الشهرية  JJ      lemma=شَهْرِيّ\n",
      "           .  PUNCT   lemma=∅\n",
      "        تضمن  VB      lemma=ضَمِن\n",
      "      الملخص  JJ      lemma=مُلَخَّص\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "      مؤسسات  JJ      lemma=مُؤَسِّس\n",
      "         مثل  IN      lemma=مثل\n",
      "           ك  IN      lemma=ك\n",
      "         لية  NN      lemma=لَيّ\n",
      "        الطب  NN      lemma=طِبّ\n",
      "           ب  IN      lemma=ب\n",
      "       جامعة  VB      lemma=جامَع\n",
      "     الجزائر  NNP     lemma=جَزائِر\n",
      "           و  CC      lemma=و\n",
      "       منظمة  NN      lemma=مُنَظِّم\n",
      "        صحية  VB      lemma=صَحِي\n",
      "     إقليمية  NN      lemma=إِقْلِيم\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "        أشار  VB      lemma=أَشار\n",
      "         إلى  IN      lemma=إلى\n",
      "      تواريخ  NN      lemma=تارِيخ\n",
      "      زيارات  NN      lemma=زِيارَة\n",
      "       محددة  JJ      lemma=مُحَدَّد\n",
      "           (  PUNCT   lemma=∅\n",
      "          15  CD      lemma=15\n",
      "           /  PUNCT   lemma=∅\n",
      "          03  CD      lemma=03\n",
      "           /  PUNCT   lemma=∅\n",
      "        2023  CD      lemma=2023\n",
      "           ،  PUNCT   lemma=∅\n",
      "          07  CD      lemma=07\n",
      "           /  PUNCT   lemma=∅\n",
      "          11  CD      lemma=11\n",
      "           /  PUNCT   lemma=∅\n",
      "        2024  CD      lemma=2024\n",
      "           )  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "         إلى  IN      lemma=إِلَى\n",
      "      قياسات  NN      lemma=قِياس\n",
      "       رقمية  NN      lemma=رَقْم\n",
      "       تتكرر  VB      lemma=تَكَرَّر\n",
      "           ب  IN      lemma=ب\n",
      "         صيغ  NN      lemma=صِيغَة\n",
      "      مختلفة  JJ      lemma=مُخْتَلَف\n",
      "           (  PUNCT   lemma=∅\n",
      "         HbA  NN      lemma=HbA\n",
      "           1  CD      lemma=1\n",
      "           c  NN      lemma=c\n",
      "           ،  PUNCT   lemma=∅\n",
      "         ضغط  VB      lemma=ضَغَط\n",
      "        الدم  NN      lemma=دَم\n",
      "           ،  PUNCT   lemma=∅\n",
      "        مؤشر  NN      lemma=مُؤَشِّر\n",
      "           ك  IN      lemma=ك\n",
      "         تلة  VB      lemma=تَلِه\n",
      "       الجسم  NN      lemma=جِسْم\n",
      "           )  PUNCT   lemma=∅\n",
      "           ،  PUNCT   lemma=∅\n",
      "         كما  CC      lemma=كَما\n",
      "       احتوى  VB      lemma=ٱِحْتَوَى\n",
      "         على  IN      lemma=على\n",
      "    اختصارات  NN      lemma=ٱِخْتِصار\n",
      "       عربية  JJ      lemma=عَرَبِيّ\n",
      "           و  CC      lemma=و\n",
      "      أجنبية  JJ      lemma=أَجْنَبِيّ\n",
      "          في  IN      lemma=في\n",
      "      الجملة  NN      lemma=جُمْلَة\n",
      "         نفس  VB      lemma=نَفِس\n",
      "          ها  PRP     lemma=ها\n",
      "           .  PUNCT   lemma=∅\n",
      "         مثل  IN      lemma=مثل\n",
      "         هذه  PRP     lemma=هذه\n",
      "      الفقرة  NN      lemma=فَقْرَة\n",
      "       تختبر  VB      lemma=ٱِخْتَبَر\n",
      "         POS  NN      lemma=POS\n",
      "           و  CC      lemma=و\n",
      "      اللمتة  NNP     lemma=اللمته\n",
      "          في  IN      lemma=في\n",
      "     مصطلحات  NN      lemma=مُصْطَلَح\n",
      "       مركبة  NN      lemma=مَرْكَبَة\n",
      "           ،  PUNCT   lemma=∅\n",
      "           و  CC      lemma=و\n",
      "       تختبر  VB      lemma=ٱِخْتَبَر\n",
      "         NER  NN      lemma=NER\n",
      "          في  IN      lemma=في\n",
      "       أسماء  NNP     lemma=أَسْماء\n",
      "    المؤسسات  JJ      lemma=مُؤَسِّس\n",
      "           و  CC      lemma=و\n",
      "    التواريخ  NN      lemma=تارِيخ\n",
      "           و  CC      lemma=و\n",
      "     الأعداد  NN      lemma=إِعْداد\n",
      "         دون  IN      lemma=دون\n",
      "      الحاجة  NN      lemma=حاجَة\n",
      "         إلى  IN      lemma=إلى\n",
      "          أي  CC      lemma=أَيْ\n",
      "   استنتاجات  NN      lemma=ٱِسْتِنْتاج\n",
      "      علاجية  NN      lemma=عِلاج\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (pretrained + deterministic fixes) (token, label):\n",
      "[('نشرت', 'O'), ('وحدة', 'O'), ('بحث', 'O'), ('في', 'O'), ('مستشفى', 'O'), ('جامعي', 'O'), ('بالجزائر', 'B-LOC'), ('دراسة', 'O'), ('رصدية', 'O'), ('عن', 'O'), ('متابعة', 'O'), ('مرضى', 'O'), ('السكري', 'O'), ('من', 'O'), ('النوع', 'O'), ('الثاني', 'O'), ('بين', 'O'), ('2021', 'B-DATE'), ('و', 'O'), ('2025', 'B-DATE'), ('،', 'O'), ('وذكرت', 'O'), ('أن', 'O'), ('العينة', 'O'), ('شملت', 'O'), ('312', 'O'), ('مشاركًا', 'O'), ('تتراوح', 'O'), ('أعمارهم', 'O'), ('بين', 'O'), ('35', 'O'), ('و', 'O'), ('70', 'O'), ('سنة', 'O'), ('،', 'O'), ('مع', 'O'), ('تقسيم', 'O'), ('فرعي', 'O'), ('حسب', 'O'), ('الالتزام', 'O'), ('بالمتابعة', 'O'), ('الشهرية', 'O'), ('.', 'O'), ('تضمن', 'O'), ('الملخص', 'O'), ('أسماء', 'O'), ('مؤسسات', 'O'), ('مثل', 'O'), ('كلية', 'O'), ('الطب', 'O'), ('بجامعة', 'B-ORG'), ('الجزائر', 'B-LOC'), ('ومنظمة', 'O'), ('صحية', 'O'), ('إقليمية', 'O'), ('،', 'O'), ('وأشار', 'O'), ('إلى', 'O'), ('تواريخ', 'O'), ('زيارات', 'O'), ('محددة', 'O'), ('(', 'O'), ('15', 'O'), ('/', 'O'), ('03', 'O'), ('/', 'O'), ('2023', 'B-DATE'), ('،', 'O'), ('07', 'O'), ('/', 'O'), ('11', 'O'), ('/', 'O'), ('2024', 'B-DATE'), (')', 'O'), ('وإلى', 'O'), ('قياسات', 'O'), ('رقمية', 'O'), ('تتكرر', 'O'), ('بصيغ', 'O'), ('مختلفة', 'O'), ('(', 'O'), ('HbA', 'O'), ('1', 'O'), ('c', 'O'), ('،', 'O'), ('ضغط', 'O'), ('الدم', 'O'), ('،', 'O'), ('مؤشر', 'O'), ('كتلة', 'O'), ('الجسم', 'O'), (')', 'O'), ('،', 'O'), ('كما', 'O'), ('احتوى', 'O'), ('على', 'O'), ('اختصارات', 'O'), ('عربية', 'O'), ('وأجنبية', 'O'), ('في', 'O'), ('الجملة', 'O'), ('نفسها', 'O'), ('.', 'O'), ('مثل', 'O'), ('هذه', 'O'), ('الفقرة', 'O'), ('تختبر', 'O'), ('POS', 'O'), ('واللمّتة', 'O'), ('في', 'O'), ('مصطلحات', 'O'), ('مركبة', 'O'), ('،', 'O'), ('وتختبر', 'O'), ('NER', 'O'), ('في', 'O'), ('أسماء', 'O'), ('المؤسسات', 'O'), ('والتواريخ', 'O'), ('والأعداد', 'O'), ('دون', 'O'), ('الحاجة', 'O'), ('إلى', 'O'), ('أي', 'O'), ('استنتاجات', 'O'), ('علاجية', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "  LOC: الجزائر\n",
      "  DATE: 2021\n",
      "  DATE: 2025\n",
      "  ORG: بجامعة\n",
      "  LOC: الجزائر\n",
      "  DATE: 2023\n",
      "  DATE: 2024\n",
      "\n",
      "Audit (heuristique):\n",
      "  Segmentation: OK=77 | Err=0 | Err%=0.00 %\n",
      "  Lemma suspect: 13/77 = 16.88 %\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Chemin vers tes .py\n",
    "# =========================\n",
    "import sys, types, re, importlib\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\"  # dossier qui contient engcode.py / frcode.py / arabcode.py\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# =========================\n",
    "# 2) Petit \"nb_utils\" en mémoire (pas besoin de créer nb_utils.py)\n",
    "#    -> utilisé par run_from_previous_cell() dans tes scripts\n",
    "# =========================\n",
    "nb_utils = types.ModuleType(\"nb_utils\")\n",
    "\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "def get_previous_cell_input():\n",
    "    g = globals()\n",
    "    for k in (\"selected\", \"TOK_DOCS\", \"FINAL_DOCS\", \"DOCS\", \"TEXT_DOCS\", \"_\"):\n",
    "        if k in g and g[k] is not None:\n",
    "            return g[k]\n",
    "    return None\n",
    "\n",
    "def iter_sentences_from_input(data):\n",
    "    \"\"\"\n",
    "    Yield: (doc_name, page_idx, sent_idx, sent_text)\n",
    "    Supporte: TOK_DOCS/selected (pages->sentences_layout), FINAL_DOCS (list[{text}]), etc.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    # Cas 1: liste de docs avec pages (TOK_DOCS / selected)\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"pages\" in data[0]:\n",
    "        for d_i, doc in enumerate(data):\n",
    "            doc_name = doc.get(\"filename\") or doc.get(\"doc_id\") or f\"doc#{d_i}\"\n",
    "            pages = doc.get(\"pages\") or []\n",
    "            for p_i, pg in enumerate(pages):\n",
    "                page_idx = pg.get(\"page_index\", pg.get(\"page\", p_i+1))\n",
    "                sent_items = pg.get(\"sentences_layout\") or pg.get(\"sentences\") or pg.get(\"chunks\") or []\n",
    "                for s_i, s in enumerate(sent_items):\n",
    "                    if isinstance(s, dict):\n",
    "                        if s.get(\"is_sentence\") is False:\n",
    "                            continue\n",
    "                        sent = s.get(\"text\") or \"\"\n",
    "                    else:\n",
    "                        sent = str(s)\n",
    "                    yield doc_name, page_idx, s_i, sent\n",
    "        return\n",
    "\n",
    "    # Cas 2: FINAL_DOCS : list[{text, filename?}]\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"text\" in data[0]:\n",
    "        for i, d in enumerate(data):\n",
    "            doc_name = d.get(\"filename\") or d.get(\"doc_id\") or f\"doc#{i}\"\n",
    "            yield doc_name, None, None, d.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 3: dict {text:...}\n",
    "    if isinstance(data, dict) and \"text\" in data:\n",
    "        doc_name = data.get(\"filename\") or data.get(\"doc_id\") or \"doc\"\n",
    "        yield doc_name, None, None, data.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 4: string direct\n",
    "    if isinstance(data, str):\n",
    "        yield \"text\", None, None, data\n",
    "        return\n",
    "\n",
    "    raise TypeError(f\"Format d'entrée non supporté: {type(data)}\")\n",
    "\n",
    "nb_utils.detect_lang = detect_lang\n",
    "nb_utils.get_previous_cell_input = get_previous_cell_input\n",
    "nb_utils.iter_sentences_from_input = iter_sentences_from_input\n",
    "sys.modules[\"nb_utils\"] = nb_utils  # rend \"import nb_utils\" possible\n",
    "\n",
    "# =========================\n",
    "# 3) Import + reload tes 3 modules\n",
    "# =========================\n",
    "import engcode\n",
    "import frcode\n",
    "\n",
    "# arabcode peut échouer si camel_tools n'est pas installé => on skip proprement\n",
    "try:\n",
    "    import arabcode\n",
    "    HAVE_AR = True\n",
    "except Exception as e:\n",
    "    HAVE_AR = False\n",
    "    print(\"[warn] arabcode.py non chargé (dépendances manquantes ?). Détail:\", e)\n",
    "\n",
    "importlib.reload(engcode)\n",
    "importlib.reload(frcode)\n",
    "if HAVE_AR:\n",
    "    importlib.reload(arabcode)\n",
    "\n",
    "# =========================\n",
    "# 4) Exécution: chaque script filtre sa langue et print son output\n",
    "# =========================\n",
    "data = get_previous_cell_input()\n",
    "if data is None:\n",
    "    raise RuntimeError(\"Je ne trouve pas de données d'entrée. Assure-toi que la cellule précédente crée 'selected' (ou FINAL_DOCS / TOK_DOCS).\")\n",
    "\n",
    "MAX_SENTENCES_PER_LANG = None  # ex: 30 pour debug, ou None pour tout\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN EN (engcode.py)\")\n",
    "print(\"=\"*120)\n",
    "engcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN FR (frcode.py)\")\n",
    "print(\"=\"*120)\n",
    "frcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "if HAVE_AR:\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"RUN AR (arabcode.py)\")\n",
    "    print(\"=\"*120)\n",
    "    arabcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6493d2",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59526b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bd9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
