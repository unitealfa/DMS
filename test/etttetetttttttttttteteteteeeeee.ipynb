{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9e27ff",
   "metadata": {},
   "source": [
    "## lire le document de quelle type il est et si cest une image ou contien du text dans sont code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e2dc739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'documents/testword.docx',\n",
       "  'ext': '.docx',\n",
       "  'mime': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
       "  'label': 'Word document (DOCX)',\n",
       "  'content': 'text'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List, Dict, Any\n",
    "\n",
    "\n",
    "# Saisie possible:\n",
    "# INPUT_FILE = \"a.pdf, b.docx, c.png\"\n",
    "# INPUT_FILE = [\"a.pdf\", \"b.docx\", \"c.png\"]\n",
    "INPUT_FILE: Optional[Union[str, Sequence[str]]] = (\n",
    "    # \"epsteanpdf.pdf, epsteain22.pdf, testexcel.xlsx, testword.docx, image2tab.webp, contras-14page.pdf, signettab.png\"\n",
    "    # \"contras-14page.pdf, testword.docx, testexcel.xlsx, signettab.png, image2tab.webp\"\n",
    "    \"documents/testword.docx\"\n",
    ")\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1     # 1 caractère => \"text\"\n",
    "MIN_CHARS_PDF = 30       # seuil de texte extrait\n",
    "PDF_MAX_PAGES = 3        # on teste les N premières pages\n",
    "\n",
    "# Dossiers de recherche si un nom est donné sans chemin (utile en notebook)\n",
    "SEARCH_DIRS = [\n",
    "    os.getcwd(),\n",
    "    \"/mnt/data\",  # utile dans l'environnement ChatGPT\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "# ----------------- input parsing -----------------\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    \"\"\"Retourne toujours une liste. Supporte une string avec virgules (CSV).\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s:\n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Résout un chemin:\n",
    "    - si p existe tel quel -> retourne p\n",
    "    - sinon essaie SEARCH_DIRS + basename(p)\n",
    "    - sinon retourne None (introuvable)\n",
    "    \"\"\"\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ----------------- format detection -----------------\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    # ZIP containers (DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB/ZIP)\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                # EPUB\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                # Office OpenXML\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                # OpenDocument\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    # Ancien Office (OLE2)\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "# ----------------- text vs image_only -----------------\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    \"\"\"Compte du texte dans du XML (éléments + fallback simple).\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    \"\"\"\n",
    "    DOCX/XLSX/PPTX/ODT/ODS/ODP/EPUB\n",
    "    True si on trouve au moins MIN_CHARS_OFFICE caractères.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                # corps\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                # headers/footers (souvent du texte “isolé”)\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                if \"content.xml\" in names:\n",
    "                    return _xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE\n",
    "                return False\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    \"\"\"Retourne PdfReader depuis pypdf ou PyPDF2, ou None si indisponible.\"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    PDF:\n",
    "    - True si extract_text() produit assez de caractères, OU si fonts / opérateurs texte présents.\n",
    "    - Si aucune lib PDF n'est dispo: fallback binaire (cherche /Font ou opérateurs BT/Tj).\n",
    "    \"\"\"\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        # fallback binaire: moins fiable, mais évite de renvoyer faux systématique\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)  # 2MB max\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "\n",
    "        extracted_score = 0\n",
    "        saw_font = False\n",
    "        saw_text_ops = False\n",
    "\n",
    "        for page in pages:\n",
    "            # 1) extraction texte\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "\n",
    "            # 2) fonts dans resources\n",
    "            try:\n",
    "                res = page.get(\"/Resources\") or {}\n",
    "                font = res.get(\"/Font\")\n",
    "                if font:\n",
    "                    saw_font = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # 3) opérateurs texte dans stream\n",
    "            try:\n",
    "                contents = page.get_contents()\n",
    "                if contents is None:\n",
    "                    continue\n",
    "                if hasattr(contents, \"get_data\"):\n",
    "                    data = contents.get_data()\n",
    "                else:\n",
    "                    data = b\"\".join(c.get_data() for c in contents)  # type: ignore\n",
    "                if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                    saw_text_ops = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return saw_font or saw_text_ops\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    \"\"\"Retourne seulement: 'text' ou 'image_only'.\"\"\"\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    # Images => image_only\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    # Formats texte compressés (Office/ODF/EPUB)\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    # Tout le reste => image_only (car tu veux 2 états)\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "def analyze_many_two_states(input_file: Optional[Union[str, Sequence[str]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sortie:\n",
    "      [{\"path\": ..., \"ext\": ..., \"mime\": ..., \"label\": ..., \"content\": \"text|image_only\"}, ...]\n",
    "    Ignore les fichiers introuvables.\n",
    "    \"\"\"\n",
    "    raw_paths = normalize_input_files(input_file)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for raw in raw_paths:\n",
    "        p = resolve_path(raw)\n",
    "        if p is None:\n",
    "            continue\n",
    "\n",
    "        ft = detect_path_type(p)\n",
    "        out.append({\n",
    "            \"path\": p,\n",
    "            \"ext\": ft.ext,\n",
    "            \"mime\": ft.mime,\n",
    "            \"label\": ft.label,\n",
    "            \"content\": content_kind_two_states(p, ft),\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test\n",
    "analyze_many_two_states(INPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d63c5",
   "metadata": {},
   "source": [
    "### si image faire passer sur un pretraitemetn lamelirer sinon un document avce text dans sont code source aallors pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c99d5904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] content='text' -> c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\testword.docx\n",
      "[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Union, List\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "\n",
    "\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    np = None\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In notebooks __file__ is undefined; fall back to current working directory.\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_LANG = \"fra\"\n",
    "DEFAULT_CONTRAST = 1.5\n",
    "DEFAULT_SHARPNESS = 1.2\n",
    "DEFAULT_BRIGHTNESS = 1.0\n",
    "DEFAULT_UPSCALE = 1.5\n",
    "DEFAULT_DPI = 300\n",
    "\n",
    "# Heuristiques\n",
    "MIN_CHARS_OFFICE = 1\n",
    "MIN_CHARS_PDF = 30\n",
    "PDF_MAX_PAGES = 3\n",
    "SEARCH_DIRS = [os.getcwd(), \"/mnt/data\"]  # utile en notebook\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FileType:\n",
    "    ext: str\n",
    "    mime: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def _read_head(path: str, n: int = 16384) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read(n)\n",
    "\n",
    "\n",
    "def normalize_input_files(x: Optional[Union[str, Sequence[str]]]) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" not in s: \n",
    "            return [s]\n",
    "        parts = next(csv.reader([s], skipinitialspace=True))\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    return [str(p).strip() for p in x if str(p).strip()]\n",
    "\n",
    "\n",
    "def resolve_path(p: str) -> Optional[str]:\n",
    "    p = os.path.expandvars(os.path.expanduser(p.strip()))\n",
    "    if os.path.exists(p):\n",
    "        return os.path.abspath(p)\n",
    "\n",
    "    base = os.path.basename(p)\n",
    "    for d in SEARCH_DIRS:\n",
    "        alt = os.path.join(d, base)\n",
    "        if os.path.exists(alt):\n",
    "            return os.path.abspath(alt)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_path_type(path: str) -> FileType:\n",
    "    head = _read_head(path)\n",
    "\n",
    "    if head.startswith(b\"%PDF-\"):\n",
    "        return FileType(\".pdf\", \"application/pdf\", \"PDF document\")\n",
    "\n",
    "    if head.startswith(b\"II*\\x00\") or head.startswith(b\"MM\\x00*\"):\n",
    "        return FileType(\".tif\", \"image/tiff\", \"TIFF image\")\n",
    "\n",
    "    if head.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return FileType(\".png\", \"image/png\", \"PNG image\")\n",
    "\n",
    "    if head.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return FileType(\".jpg\", \"image/jpeg\", \"JPEG image\")\n",
    "\n",
    "    if len(head) >= 12 and head.startswith(b\"RIFF\") and head[8:12] == b\"WEBP\":\n",
    "        return FileType(\".webp\", \"image/webp\", \"WEBP image\")\n",
    "\n",
    "    if head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = set(z.namelist())\n",
    "\n",
    "                if \"mimetype\" in names and \"META-INF/container.xml\" in names:\n",
    "                    try:\n",
    "                        mt = z.read(\"mimetype\")[:64].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/epub+zip\":\n",
    "                        return FileType(\".epub\", \"application/epub+zip\", \"EPUB eBook\")\n",
    "\n",
    "                if \"word/document.xml\" in names:\n",
    "                    return FileType(\".docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"Word document (DOCX)\")\n",
    "                if \"xl/workbook.xml\" in names:\n",
    "                    return FileType(\".xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\", \"Excel workbook (XLSX)\")\n",
    "                if \"ppt/presentation.xml\" in names:\n",
    "                    return FileType(\".pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\", \"PowerPoint presentation (PPTX)\")\n",
    "\n",
    "                if \"content.xml\" in names and \"META-INF/manifest.xml\" in names:\n",
    "                    mt = \"\"\n",
    "                    try:\n",
    "                        if \"mimetype\" in names:\n",
    "                            mt = z.read(\"mimetype\")[:128].decode(\"ascii\", errors=\"ignore\").strip()\n",
    "                    except Exception:\n",
    "                        mt = \"\"\n",
    "                    if mt == \"application/vnd.oasis.opendocument.text\":\n",
    "                        return FileType(\".odt\", mt, \"OpenDocument Text (ODT)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.spreadsheet\":\n",
    "                        return FileType(\".ods\", mt, \"OpenDocument Spreadsheet (ODS)\")\n",
    "                    if mt == \"application/vnd.oasis.opendocument.presentation\":\n",
    "                        return FileType(\".odp\", mt, \"OpenDocument Presentation (ODP)\")\n",
    "                    return FileType(\".odf\", \"application/zip\", \"OpenDocument container\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return FileType(\".zip\", \"application/zip\", \"ZIP archive/container\")\n",
    "\n",
    "    if head.startswith(b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\"):\n",
    "        return FileType(\".ole\", \"application/x-ole-storage\", \"OLE2 container (old Office)\")\n",
    "\n",
    "    return FileType(\"\", \"application/octet-stream\", \"Unknown / binary\")\n",
    "\n",
    "\n",
    "def _xml_text_len(xml_bytes: bytes) -> int:\n",
    "    try:\n",
    "        root = ET.fromstring(xml_bytes)\n",
    "        total = 0\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                total += len(elem.text.strip())\n",
    "        return total\n",
    "    except Exception:\n",
    "        s = re.sub(rb\"<[^>]+>\", b\" \", xml_bytes)\n",
    "        return len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "\n",
    "\n",
    "def _zip_has_text(path: str, ext: str) -> bool:\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "\n",
    "            if ext == \".docx\":\n",
    "                total = 0\n",
    "                if \"word/document.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"word/document.xml\"))\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                    if total >= MIN_CHARS_OFFICE:\n",
    "                        break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".xlsx\":\n",
    "                total = 0\n",
    "                if \"xl/sharedStrings.xml\" in names:\n",
    "                    total += _xml_text_len(z.read(\"xl/sharedStrings.xml\"))\n",
    "                if total < MIN_CHARS_OFFICE:\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\"):\n",
    "                            total += _xml_text_len(z.read(nm))\n",
    "                            if total >= MIN_CHARS_OFFICE:\n",
    "                                break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext == \".pptx\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\"):\n",
    "                        total += _xml_text_len(z.read(nm))\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "            if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                return (\"content.xml\" in names) and (_xml_text_len(z.read(\"content.xml\")) >= MIN_CHARS_OFFICE)\n",
    "\n",
    "            if ext == \".epub\":\n",
    "                total = 0\n",
    "                for nm in names:\n",
    "                    low = nm.lower()\n",
    "                    if low.endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        s = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "                        total += len(re.sub(rb\"\\s+\", b\" \", s).strip())\n",
    "                        if total >= MIN_CHARS_OFFICE:\n",
    "                            break\n",
    "                return total >= MIN_CHARS_OFFICE\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_pdf_reader():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _pdf_has_text(path: str) -> bool:\n",
    "    PdfReader = _get_pdf_reader()\n",
    "\n",
    "    if PdfReader is None:\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = f.read(2_000_000)\n",
    "            if b\"/Font\" in data:\n",
    "                return True\n",
    "            if b\"BT\" in data and (b\"Tj\" in data or b\"TJ\" in data):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        pages = reader.pages[: max(1, PDF_MAX_PAGES)]\n",
    "        extracted_score = 0\n",
    "\n",
    "        for page in pages:\n",
    "            txt = page.extract_text() or \"\"\n",
    "            extracted_score += len(\"\".join(txt.split()))\n",
    "            if extracted_score >= MIN_CHARS_PDF:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def content_kind_two_states(path: str, ftype: FileType) -> str:\n",
    "    ext = ftype.ext.lower()\n",
    "\n",
    "    if ext in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\", \".ico\"}:\n",
    "        return \"image_only\"\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        return \"text\" if _pdf_has_text(path) else \"image_only\"\n",
    "\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        return \"text\" if _zip_has_text(path, ext) else \"image_only\"\n",
    "\n",
    "    return \"image_only\"\n",
    "\n",
    "\n",
    "# --------- ROUTAGE ---------\n",
    "ORIGINAL_INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "_raw_items = normalize_input_files(ORIGINAL_INPUT_FILE)\n",
    "\n",
    "IMAGE_ONLY_FILES: List[str] = []\n",
    "TEXT_FILES: List[str] = []\n",
    "MISSING_FILES: List[str] = []\n",
    "\n",
    "for item in _raw_items:\n",
    "    p = resolve_path(item)\n",
    "    if p is None:\n",
    "        MISSING_FILES.append(item)\n",
    "        continue\n",
    "\n",
    "    ft = detect_path_type(p)\n",
    "    kind = content_kind_two_states(p, ft)\n",
    "\n",
    "    if kind == \"image_only\":\n",
    "        IMAGE_ONLY_FILES.append(p)\n",
    "    else:\n",
    "        TEXT_FILES.append(p)\n",
    "        print(f\"[skip] content='text' -> {p}\")\n",
    "\n",
    "# IMPORTANT: ton code OCR (cellule suivante) reste inchangé, il lira INPUT_FILE ici\n",
    "INPUT_FILE = IMAGE_ONLY_FILES\n",
    "\n",
    "if MISSING_FILES:\n",
    "    print(\"[missing] fichiers introuvables:\")\n",
    "    for m in MISSING_FILES:\n",
    "        print(\" -\", m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SHOW_PREPROCESSED = True   #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhanceOptions:\n",
    "    contrast: float = DEFAULT_CONTRAST\n",
    "    sharpness: float = DEFAULT_SHARPNESS\n",
    "    brightness: float = DEFAULT_BRIGHTNESS\n",
    "    upscale: float = DEFAULT_UPSCALE\n",
    "    gamma: Optional[float] = None  # gamma correction; <1 brightens darks, >1 darkens\n",
    "    pad: int = 0  # pixels to pad around the image\n",
    "    median: Optional[int] = None  # kernel size for median filter (odd int, e.g., 3)\n",
    "    unsharp_radius: Optional[float] = None  # e.g., 1.0\n",
    "    unsharp_percent: int = 150\n",
    "    invert: bool = False\n",
    "    autocontrast_cutoff: Optional[int] = None  # 0-100; percentage to clip for autocontrast\n",
    "    equalize: bool = False  # histogram equalization\n",
    "    auto_rotate: bool = False  # attempt orientation detection + rotate\n",
    "    otsu: bool = False  # auto-threshold with Otsu (requires numpy)\n",
    "    threshold: Optional[int] = None  # 0-255; if set, applies a binary threshold\n",
    "\n",
    "\n",
    "def build_config(\n",
    "    oem: Optional[int],\n",
    "    psm: Optional[int],\n",
    "    base_flags: Iterable[str],\n",
    "    dpi: Optional[int],\n",
    "    tessdata_dir: Optional[Path],\n",
    "    user_words: Optional[Path],\n",
    "    user_patterns: Optional[Path],\n",
    ") -> str:\n",
    "    parts: List[str] = []\n",
    "    if oem is not None:\n",
    "        parts.append(f\"--oem {oem}\")\n",
    "    if psm is not None:\n",
    "        parts.append(f\"--psm {psm}\")\n",
    "    if dpi is not None:\n",
    "        parts.append(f\"--dpi {dpi}\")\n",
    "    if tessdata_dir is not None:\n",
    "        parts.append(f'--tessdata-dir \"{tessdata_dir}\"')\n",
    "    if user_words is not None:\n",
    "        parts.append(f'--user-words \"{user_words}\"')\n",
    "    if user_patterns is not None:\n",
    "        parts.append(f'--user-patterns \"{user_patterns}\"')\n",
    "    parts.extend(base_flags)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def ensure_environment(lang: str) -> None:\n",
    "    try:\n",
    "        _ = pytesseract.get_tesseract_version()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        sys.exit(\"Tesseract binary not found on PATH. Install it and its language data.\")\n",
    "    if lang:\n",
    "        try:\n",
    "            available = set(pytesseract.get_languages(config=\"\"))\n",
    "            requested = set(lang.split(\"+\"))\n",
    "            missing = requested - available\n",
    "            if missing:\n",
    "                print(\n",
    "                    f\"Warning: missing languages: {', '.join(sorted(missing))}. \"\n",
    "                    f\"Available: {', '.join(sorted(available))}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "        except pytesseract.TesseractError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def auto_rotate_if_needed(img: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    if not enhance.auto_rotate:\n",
    "        return img\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        angle = None\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                try:\n",
    "                    angle = int(line.split(\":\")[1].strip())\n",
    "                except ValueError:\n",
    "                    angle = None\n",
    "                break\n",
    "        if angle is not None and angle % 360 != 0:\n",
    "            return img.rotate(-angle, expand=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    img = image.convert(\"L\")\n",
    "    img = auto_rotate_if_needed(img, enhance)\n",
    "\n",
    "    if enhance.invert:\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    if enhance.pad and enhance.pad > 0:\n",
    "        img = ImageOps.expand(img, border=enhance.pad, fill=255)\n",
    "\n",
    "    if enhance.autocontrast_cutoff is not None:\n",
    "        cutoff = max(0, min(100, enhance.autocontrast_cutoff))\n",
    "        img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "\n",
    "    if enhance.equalize:\n",
    "        img = ImageOps.equalize(img)\n",
    "\n",
    "    if enhance.upscale and enhance.upscale != 1.0:\n",
    "        w, h = img.size\n",
    "        img = img.resize((int(w * enhance.upscale), int(h * enhance.upscale)), Image.LANCZOS)\n",
    "\n",
    "    if enhance.gamma and enhance.gamma > 0:\n",
    "        inv_gamma = 1.0 / enhance.gamma\n",
    "        lut = [pow(x / 255.0, inv_gamma) * 255 for x in range(256)]\n",
    "        img = img.point(lut)\n",
    "\n",
    "    if enhance.brightness and enhance.brightness != 1.0:\n",
    "        img = ImageEnhance.Brightness(img).enhance(enhance.brightness)\n",
    "\n",
    "    if enhance.contrast and enhance.contrast != 1.0:\n",
    "        img = ImageEnhance.Contrast(img).enhance(enhance.contrast)\n",
    "\n",
    "    if enhance.sharpness and enhance.sharpness != 1.0:\n",
    "        img = ImageEnhance.Sharpness(img).enhance(enhance.sharpness)\n",
    "\n",
    "    if enhance.unsharp_radius:\n",
    "        img = img.filter(\n",
    "            ImageFilter.UnsharpMask(\n",
    "                radius=enhance.unsharp_radius,\n",
    "                percent=enhance.unsharp_percent,\n",
    "                threshold=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if enhance.median and enhance.median > 1 and enhance.median % 2 == 1:\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=enhance.median))\n",
    "\n",
    "    if enhance.threshold is not None:\n",
    "        thr = max(0, min(255, enhance.threshold))\n",
    "        img = img.point(lambda p, t=thr: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "    elif enhance.otsu and np is not None:\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        hist, _ = np.histogram(arr, bins=256, range=(0, 256))\n",
    "        total = arr.size\n",
    "        sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "        sum_b = 0.0\n",
    "        w_b = 0.0\n",
    "        max_var = 0.0\n",
    "        threshold = 0\n",
    "\n",
    "        for i in range(256):\n",
    "            w_b += hist[i]\n",
    "            if w_b == 0:\n",
    "                continue\n",
    "            w_f = total - w_b\n",
    "            if w_f == 0:\n",
    "                break\n",
    "            sum_b += i * hist[i]\n",
    "            m_b = sum_b / w_b\n",
    "            m_f = (sum_total - sum_b) / w_f\n",
    "            var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "            if var_between > max_var:\n",
    "                max_var = var_between\n",
    "                threshold = i\n",
    "\n",
    "        img = img.point(lambda p, t=threshold: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-l\", \"--lang\", default=DEFAULT_LANG)\n",
    "    parser.add_argument(\"--oem\", type=int, choices=range(0, 4), default=None)\n",
    "    parser.add_argument(\"--psm\", type=int, choices=range(0, 14), default=None)\n",
    "    parser.add_argument(\"--dpi\", type=int, default=DEFAULT_DPI)\n",
    "    parser.add_argument(\"--tessdata-dir\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-words\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-patterns\", type=Path, default=None)\n",
    "    parser.add_argument(\"--whitelist\", type=str, default=None)\n",
    "    parser.add_argument(\"--blacklist\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--contrast\", type=float, default=DEFAULT_CONTRAST)\n",
    "    parser.add_argument(\"--sharpness\", type=float, default=DEFAULT_SHARPNESS)\n",
    "    parser.add_argument(\"--brightness\", type=float, default=DEFAULT_BRIGHTNESS)\n",
    "    parser.add_argument(\"--upscale\", type=float, default=DEFAULT_UPSCALE)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--pad\", type=int, default=0)\n",
    "    parser.add_argument(\"--threshold\", type=int, default=None)\n",
    "    parser.add_argument(\"--median\", type=int, default=None)\n",
    "    parser.add_argument(\"--unsharp-radius\", type=float, default=None)\n",
    "    parser.add_argument(\"--unsharp-percent\", type=int, default=150)\n",
    "    parser.add_argument(\"--invert\", action=\"store_true\")\n",
    "    parser.add_argument(\"--autocontrast-cutoff\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--auto-rotate\", action=\"store_true\")\n",
    "    parser.add_argument(\"--otsu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"*\",\n",
    "        default=[],\n",
    "        metavar=\"CFG\",\n",
    "        help=\"Additional configuration flags passed verbatim to tesseract (e.g., -c foo=bar).\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args(list(argv) if argv is not None else [])\n",
    "\n",
    "\n",
    "#  Exécution Cellule 1 (jusqu’à l’affichage) \n",
    "\n",
    "args = parse_args()\n",
    "ensure_environment(args.lang)\n",
    "\n",
    "enhance = EnhanceOptions(\n",
    "    contrast=args.contrast,\n",
    "    sharpness=args.sharpness,\n",
    "    brightness=args.brightness,\n",
    "    upscale=args.upscale,\n",
    "    gamma=args.gamma,\n",
    "    pad=args.pad,\n",
    "    median=args.median,\n",
    "    unsharp_radius=args.unsharp_radius,\n",
    "    unsharp_percent=args.unsharp_percent,\n",
    "    invert=args.invert,\n",
    "    autocontrast_cutoff=args.autocontrast_cutoff,\n",
    "    equalize=args.equalize,\n",
    "    auto_rotate=args.auto_rotate,\n",
    "    otsu=args.otsu,\n",
    "    threshold=args.threshold,\n",
    ")\n",
    "\n",
    "config_flags: List[str] = list(args.config)\n",
    "\n",
    "# AJOUTE ÇA :\n",
    "config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "\n",
    "if args.whitelist:\n",
    "    config_flags.append(f\"-c tessedit_char_whitelist={args.whitelist}\")\n",
    "if args.blacklist:\n",
    "    config_flags.append(f\"-c tessedit_char_blacklist={args.blacklist}\")\n",
    "\n",
    "\n",
    "def _normalize_input_files(val):\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        items = list(val)\n",
    "    else:\n",
    "        items = [val]\n",
    "\n",
    "    out = []\n",
    "    for item in items:\n",
    "        if item is None:\n",
    "            continue\n",
    "        if isinstance(item, Path):\n",
    "            out.append(str(item))\n",
    "            continue\n",
    "        s = str(item).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if \",\" in s:\n",
    "            parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "            out.extend(parts)\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# Backwards-compatible alias (older cell name)\n",
    "_normalize_input_file = _normalize_input_files\n",
    "\n",
    "# Safeguard if INPUT_FILE cell not executed yet\n",
    "INPUT_FILE = globals().get(\"INPUT_FILE\", None)\n",
    "\n",
    "\n",
    "def _load_images_from_path(path: Path, dpi: int):\n",
    "    if path.suffix.lower() == \".pdf\":\n",
    "        try:\n",
    "            from pdf2image import convert_from_path\n",
    "        except Exception:\n",
    "            sys.exit(\n",
    "                \"pdf2image is not available. Install it and Poppler to read PDF files.\"\n",
    "            )\n",
    "        try:\n",
    "            return convert_from_path(str(path), dpi=dpi)\n",
    "        except Exception as exc:\n",
    "            sys.exit(f\"PDF conversion failed for {path}: {exc}\")\n",
    "    # default: image file (supports multi-page TIFF)\n",
    "    img = Image.open(path)\n",
    "    n_frames = getattr(img, \"n_frames\", 1)\n",
    "    if n_frames and n_frames > 1:\n",
    "        images = []\n",
    "        for i in range(n_frames):\n",
    "            try:\n",
    "                img.seek(i)\n",
    "            except Exception:\n",
    "                break\n",
    "            images.append(img.copy())\n",
    "        return images\n",
    "    return [img]\n",
    "\n",
    "\n",
    "input_items = _normalize_input_files(INPUT_FILE)\n",
    "if not input_items:\n",
    "    print(\"[info] Aucun fichier à OCR (image_only). Tout ce que tu as donné est détecté comme 'text'.\")\n",
    "    DOCS = []\n",
    "else:\n",
    "    DOCS = []\n",
    "    for item in input_items:\n",
    "        path = Path(item)\n",
    "        if not path.is_absolute():\n",
    "            path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "        print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "        dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "        images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "        if len(images) == 1:\n",
    "            original = images[0]\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "        else:\n",
    "            total = len(images)\n",
    "            for idx, original in enumerate(images, start=1):\n",
    "                prepped = preprocess_image(original, enhance)\n",
    "                DOCS.append({\n",
    "                    \"path\": path,\n",
    "                    \"original\": original,\n",
    "                    \"prepped\": prepped,\n",
    "                    \"page_index\": idx,\n",
    "                    \"page_count\": total\n",
    "                })\n",
    "\n",
    "\n",
    "DOCS = []\n",
    "for item in input_items:\n",
    "    path = Path(item)\n",
    "    if not path.is_absolute():\n",
    "        path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "    if not path.exists():\n",
    "        sys.exit(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "    print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "    dpi_val = int(getattr(args, \"dpi\", DEFAULT_DPI) or DEFAULT_DPI)\n",
    "    images = _load_images_from_path(path, dpi=dpi_val)\n",
    "\n",
    "    if len(images) == 1:\n",
    "        original = images[0]\n",
    "        prepped = preprocess_image(original, enhance)\n",
    "        DOCS.append({\"path\": path, \"original\": original, \"prepped\": prepped})\n",
    "    else:\n",
    "        total = len(images)\n",
    "        for idx, original in enumerate(images, start=1):\n",
    "            prepped = preprocess_image(original, enhance)\n",
    "            DOCS.append({\n",
    "                \"path\": path,\n",
    "                \"original\": original,\n",
    "                \"prepped\": prepped,\n",
    "                \"page_index\": idx,\n",
    "                \"page_count\": total\n",
    "            })\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "for doc in DOCS:\n",
    "    original = doc[\"original\"]\n",
    "    prepped = doc[\"prepped\"]\n",
    "    path = doc[\"path\"]\n",
    "\n",
    "    display(original.convert(\"RGB\") if original.mode not in (\"RGB\",\"L\") else original)\n",
    "\n",
    "    if \"SHOW_PREPROCESSED\" not in globals() or SHOW_PREPROCESSED:\n",
    "        display(prepped.convert(\"RGB\") if prepped.mode not in (\"RGB\",\"L\") else prepped)\n",
    "\n",
    "# Keep globals aligned with the last document for backwards compatibility.\n",
    "if DOCS:\n",
    "    path = DOCS[-1][\"path\"]\n",
    "    original = DOCS[-1][\"original\"]\n",
    "    prepped = DOCS[-1][\"prepped\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bb258",
   "metadata": {},
   "source": [
    "### si image passer sur teseract ou document extraire sont contenue apartire de sont contenue code a la fin les deux => input txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0db4b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[doc] testword.docx | content=text | extraction=native:docx:xml | pages=1\n",
      "[page 1/1]\n",
      "Équipée d'un moteur V10 de 620 chevaux, l'Audi R8 passe de 0 à 100 km/h en moins de 3,5 secondes, affirmant ainsi sa place de supercar emblématique de la marque aux anneaux. \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE:\n",
    "# Cette cellule suppose que la cellule précédente a déjà exécuté:\n",
    "# - la détection/routage (TEXT_FILES / IMAGE_ONLY_FILES / INPUT_FILE)\n",
    "# - le preprocess + affichage (DOCS avec \"prepped\")\n",
    "# Donc ici on fait:\n",
    "# 1) OCR Tesseract UNIQUEMENT sur DOCS (images -> [info])\n",
    "# 2) Extraction NATIVE (sans OCR) sur TEXT_FILES (-> [skip] content='text')\n",
    "#\n",
    "# Objectif print:\n",
    "# - 1 seule fois, à la fin\n",
    "# - affiche: fichier, nb pages, puis texte de chaque page\n",
    "\n",
    "import uuid\n",
    "import re\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# ==================== Réglage PRINT ====================\n",
    "# False => aucune sortie pendant OCR/native\n",
    "# True  => debug pendant extraction (à éviter si tu veux 1 seul print)\n",
    "PRINT_DURING_EXTRACTION = False\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (flags tesseract pour espaces/tables) --------------------\n",
    "if \"config_flags\" in globals():\n",
    "    if \"-c preserve_interword_spaces=1\" not in config_flags:\n",
    "        config_flags.append(\"-c preserve_interword_spaces=1\")\n",
    "    if \"-c textord_tabfind_find_tables=1\" not in config_flags:\n",
    "        config_flags.append(\"-c textord_tabfind_find_tables=1\")\n",
    "\n",
    "# -------------------- AJOUT MINIMAL (reconstruction layout via TSV) --------------------\n",
    "def _median(values):\n",
    "    values = sorted(values)\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    mid = n // 2\n",
    "    if n % 2 == 1:\n",
    "        return values[mid]\n",
    "    return (values[mid - 1] + values[mid]) / 2.0\n",
    "\n",
    "def _estimate_char_metrics(data: dict):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    ws = data.get(\"width\", [])\n",
    "    hs = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        w = int(ws[i]) if i < len(ws) else 0\n",
    "        h = int(hs[i]) if i < len(hs) else 0\n",
    "        if h > 0:\n",
    "            heights.append(h)\n",
    "\n",
    "        L = len(s)\n",
    "        if w > 0 and L > 0:\n",
    "            widths.append(w / float(L))\n",
    "\n",
    "    char_w = _median(widths) or 10.0\n",
    "    line_h = _median(heights) or 20.0\n",
    "\n",
    "    if char_w <= 1:\n",
    "        char_w = 10.0\n",
    "    if line_h <= 1:\n",
    "        line_h = 20.0\n",
    "\n",
    "    return float(char_w), float(line_h)\n",
    "\n",
    "def _render_layout_from_data(data: dict, img_w: int, img_h: int) -> str:\n",
    "    char_w, line_h = _estimate_char_metrics(data)\n",
    "    line_tol = max(6.0, line_h * 0.55)\n",
    "\n",
    "    items = []\n",
    "    texts = data.get(\"text\", [])\n",
    "    confs = data.get(\"conf\", [])\n",
    "    lefts = data.get(\"left\", [])\n",
    "    tops = data.get(\"top\", [])\n",
    "    widths = data.get(\"width\", [])\n",
    "    heights = data.get(\"height\", [])\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = str(t)\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        try:\n",
    "            c = float(confs[i])\n",
    "        except Exception:\n",
    "            c = 0.0\n",
    "        if c < 0:\n",
    "            continue\n",
    "\n",
    "        l = int(lefts[i]) if i < len(lefts) else 0\n",
    "        tp = int(tops[i]) if i < len(tops) else 0\n",
    "        w = int(widths[i]) if i < len(widths) else 0\n",
    "        h = int(heights[i]) if i < len(heights) else 0\n",
    "\n",
    "        items.append({\"text\": s, \"left\": l, \"top\": tp, \"right\": l + w, \"height\": h})\n",
    "\n",
    "    items.sort(key=lambda x: (x[\"top\"], x[\"left\"]))\n",
    "\n",
    "    lines = []\n",
    "    for it in items:\n",
    "        placed = False\n",
    "        if lines and abs(it[\"top\"] - lines[-1][\"top\"]) <= line_tol:\n",
    "            lines[-1][\"words\"].append(it)\n",
    "            lines[-1][\"top\"] = min(lines[-1][\"top\"], it[\"top\"])\n",
    "            placed = True\n",
    "        if not placed:\n",
    "            for ln in reversed(lines):\n",
    "                if abs(it[\"top\"] - ln[\"top\"]) <= line_tol:\n",
    "                    ln[\"words\"].append(it)\n",
    "                    ln[\"top\"] = min(ln[\"top\"], it[\"top\"])\n",
    "                    placed = True\n",
    "                    break\n",
    "        if not placed:\n",
    "            lines.append({\"top\": it[\"top\"], \"words\": [it]})\n",
    "\n",
    "    lines.sort(key=lambda ln: ln[\"top\"])\n",
    "\n",
    "    out_lines = []\n",
    "    prev_row = None\n",
    "\n",
    "    for ln in lines:\n",
    "        words = sorted(ln[\"words\"], key=lambda x: x[\"left\"])\n",
    "        row = int(round(ln[\"top\"] / line_h)) if line_h > 0 else 0\n",
    "        if prev_row is not None:\n",
    "            gap = row - prev_row\n",
    "            if gap > 1:\n",
    "                for _ in range(gap - 1):\n",
    "                    out_lines.append(\"\")\n",
    "        prev_row = row\n",
    "\n",
    "        line_str = \"\"\n",
    "        cursor = 0\n",
    "        for w in words:\n",
    "            col = int(round(w[\"left\"] / char_w)) if char_w > 0 else 0\n",
    "            if col < 0:\n",
    "                col = 0\n",
    "\n",
    "            if cursor == 0 and not line_str:\n",
    "                if col > 0:\n",
    "                    line_str += \" \" * col\n",
    "                    cursor = col\n",
    "            else:\n",
    "                needed = col - cursor\n",
    "                if needed <= 0:\n",
    "                    needed = 1\n",
    "                line_str += \" \" * needed\n",
    "                cursor += needed\n",
    "\n",
    "            line_str += w[\"text\"]\n",
    "            cursor += len(w[\"text\"])\n",
    "\n",
    "        out_lines.append(line_str)\n",
    "\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "# -------------------- OCR --------------------\n",
    "config = build_config(\n",
    "    args.oem,\n",
    "    args.psm,\n",
    "    config_flags,\n",
    "    args.dpi,\n",
    "    args.tessdata_dir,\n",
    "    args.user_words,\n",
    "    args.user_patterns,\n",
    ")\n",
    "\n",
    "if \"DOCS\" not in globals():\n",
    "    DOCS = []\n",
    "\n",
    "def _basename(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return Path(val).name\n",
    "    except Exception:\n",
    "        s = str(val)\n",
    "        return s.replace(\"\\\\\", \"/\").split(\"/\")[-1]\n",
    "\n",
    "# If DOCS is a list of pages (legacy), group into document-level objects\n",
    "if DOCS and isinstance(DOCS[0], dict) and \"pages\" not in DOCS[0]:\n",
    "    groups = {}\n",
    "    for i, page in enumerate(DOCS, start=1):\n",
    "        raw = str(page.get(\"path\") or \"batch\")\n",
    "        key = f\"{raw}::p{page.get('page_index') or i}\"\n",
    "        groups.setdefault(key, []).append(page)\n",
    "\n",
    "    packed = []\n",
    "    for key, pages in groups.items():\n",
    "        pages_sorted = sorted(pages, key=lambda p: int(p.get(\"page_index\") or 0)) if pages else []\n",
    "\n",
    "        source_files = [_basename(p.get(\"path\")) for p in pages_sorted if _basename(p.get(\"path\"))]\n",
    "        source_files = list(dict.fromkeys(source_files))\n",
    "\n",
    "        filename = source_files[0] if len(source_files) == 1 else (_basename(key) or \"batch\")\n",
    "\n",
    "        doc = {\"doc_id\": str(uuid.uuid4()), \"filename\": filename, \"source_files\": source_files, \"pages\": []}\n",
    "        page_index = 1\n",
    "        for p in pages_sorted:\n",
    "            idx = int(p.get(\"page_index\") or page_index)\n",
    "            src_path = p.get(\"path\")\n",
    "            doc[\"pages\"].append({\n",
    "                \"page_index\": idx,\n",
    "                \"image\": p.get(\"original\"),\n",
    "                \"prepped\": p.get(\"prepped\"),\n",
    "                \"source_path\": src_path,\n",
    "                \"source_file\": _basename(src_path)\n",
    "            })\n",
    "            page_index += 1\n",
    "        doc[\"page_count_total\"] = len(doc[\"pages\"])\n",
    "        packed.append(doc)\n",
    "\n",
    "    DOCS = packed\n",
    "\n",
    "# Ensure doc-level metadata consistency (even if DOCS already has pages)\n",
    "for doc in DOCS:\n",
    "    pages = doc.get(\"pages\", []) or []\n",
    "    for i, page in enumerate(pages, start=1):\n",
    "        if not page.get(\"page_index\"):\n",
    "            page[\"page_index\"] = i\n",
    "        if not page.get(\"source_file\"):\n",
    "            src_path = page.get(\"source_path\") or page.get(\"path\")\n",
    "            page[\"source_file\"] = _basename(src_path)\n",
    "\n",
    "    doc[\"page_count_total\"] = len(pages)\n",
    "\n",
    "    if not doc.get(\"source_files\"):\n",
    "        source_files = [p.get(\"source_file\") for p in pages if p.get(\"source_file\")]\n",
    "        doc[\"source_files\"] = list(dict.fromkeys(source_files))\n",
    "\n",
    "    if not doc.get(\"filename\"):\n",
    "        if len(doc.get(\"source_files\", [])) == 1:\n",
    "            doc[\"filename\"] = doc[\"source_files\"][0]\n",
    "        elif len(doc.get(\"source_files\", [])) > 1:\n",
    "            doc[\"filename\"] = \"batch\"\n",
    "\n",
    "for doc in DOCS:\n",
    "    pages_text = []\n",
    "    for page in doc.get(\"pages\", []):\n",
    "        prepped = page.get(\"prepped\")\n",
    "        if prepped is None:\n",
    "            raise RuntimeError(\"prepped image missing. Run the input/preprocess cell first.\")\n",
    "\n",
    "        data = pytesseract.image_to_data(prepped, lang=args.lang, config=config, output_type=Output.DICT)\n",
    "        w, h = prepped.size\n",
    "        OCR_TEXT = _render_layout_from_data(data, w, h)\n",
    "\n",
    "        page[\"ocr_text\"] = OCR_TEXT\n",
    "        pages_text.append(OCR_TEXT)\n",
    "\n",
    "        if PRINT_DURING_EXTRACTION:\n",
    "            src = page.get(\"source_file\") or _basename(page.get(\"source_path\")) or \"\"\n",
    "            total = doc.get(\"page_count_total\", 1)\n",
    "            print(f\"[ocr] {doc.get('filename')} | file={src} | page {page.get('page_index')}/{total}\")\n",
    "            print(OCR_TEXT)\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "    doc[\"pages_text\"] = pages_text\n",
    "    doc[\"ocr_text\"] = \"\\n\\n\".join(pages_text)\n",
    "\n",
    "# Backwards compatibility\n",
    "if DOCS:\n",
    "    OCR_TEXT = DOCS[-1].get(\"ocr_text\", \"\")\n",
    "\n",
    "# -------------------- EXTRACTION NATIVE POUR [skip] (TEXT_FILES) --------------------\n",
    "def _get_pdf_reader_with_name():\n",
    "    try:\n",
    "        from pypdf import PdfReader  # type: ignore\n",
    "        return PdfReader, \"pypdf\"\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader  # type: ignore\n",
    "            return PdfReader, \"PyPDF2\"\n",
    "        except ImportError:\n",
    "            return None, \"none\"\n",
    "\n",
    "# AJOUT MINIMAL: tenter un mode \"layout\" si dispo, sinon fallback normal\n",
    "def _pdf_extract_text_preserve_layout(page) -> str:\n",
    "    try:\n",
    "        return page.extract_text(extraction_mode=\"layout\") or \"\"\n",
    "    except TypeError:\n",
    "        return page.extract_text() or \"\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            return page.extract_text() or \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def _docx_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.findall(\".//w:p\", ns):\n",
    "        line_parts = []\n",
    "        for node in p.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                line_parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}tab\"):\n",
    "                line_parts.append(\"\\t\")\n",
    "            elif tag.endswith(\"}br\") or tag.endswith(\"}cr\"):\n",
    "                line_parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(line_parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _pptx_slide_xml_to_text(xml_bytes: bytes) -> str:\n",
    "    ns = {\n",
    "        \"a\": \"http://schemas.openxmlformats.org/drawingml/2006/main\",\n",
    "        \"p\": \"http://schemas.openxmlformats.org/presentationml/2006/main\",\n",
    "    }\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    out_lines = []\n",
    "    for para in root.findall(\".//a:p\", ns):\n",
    "        parts = []\n",
    "        for node in para.iter():\n",
    "            tag = node.tag\n",
    "            if tag.endswith(\"}t\"):\n",
    "                parts.append(node.text if node.text is not None else \"\")\n",
    "            elif tag.endswith(\"}br\"):\n",
    "                parts.append(\"\\n\")\n",
    "        out_lines.append(\"\".join(parts))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _xlsx_col_to_index(col_letters: str) -> int:\n",
    "    n = 0\n",
    "    for ch in col_letters:\n",
    "        if \"A\" <= ch <= \"Z\":\n",
    "            n = n * 26 + (ord(ch) - ord(\"A\") + 1)\n",
    "    return n\n",
    "\n",
    "def _xlsx_shared_strings(xml_bytes: bytes) -> list:\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    out = []\n",
    "    for si in root.findall(\".//s:si\", ns):\n",
    "        parts = []\n",
    "        for t in si.findall(\".//s:t\", ns):\n",
    "            parts.append(t.text if t.text is not None else \"\")\n",
    "        out.append(\"\".join(parts))\n",
    "    return out\n",
    "\n",
    "def _xlsx_sheet_to_text(sheet_xml: bytes, shared: list) -> str:\n",
    "    ns = {\"s\": \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"}\n",
    "    root = ET.fromstring(sheet_xml)\n",
    "\n",
    "    lines = []\n",
    "    for row in root.findall(\".//s:row\", ns):\n",
    "        cells = row.findall(\"./s:c\", ns)\n",
    "        row_map = {}\n",
    "        max_col = 0\n",
    "\n",
    "        for c in cells:\n",
    "            r = c.get(\"r\") or \"\"\n",
    "            col_letters = \"\".join([ch for ch in r if ch.isalpha()]).upper()\n",
    "            col_idx = _xlsx_col_to_index(col_letters) if col_letters else 0\n",
    "            if col_idx > max_col:\n",
    "                max_col = col_idx\n",
    "\n",
    "            cell_type = c.get(\"t\")\n",
    "            v = c.find(\"./s:v\", ns)\n",
    "            is_node = c.find(\"./s:is\", ns)\n",
    "\n",
    "            val = \"\"\n",
    "            if cell_type == \"s\" and v is not None and v.text is not None:\n",
    "                try:\n",
    "                    val = shared[int(v.text)]\n",
    "                except Exception:\n",
    "                    val = v.text\n",
    "            elif cell_type == \"inlineStr\" and is_node is not None:\n",
    "                parts = []\n",
    "                for t in is_node.findall(\".//s:t\", ns):\n",
    "                    parts.append(t.text if t.text is not None else \"\")\n",
    "                val = \"\".join(parts)\n",
    "            else:\n",
    "                if v is not None and v.text is not None:\n",
    "                    val = v.text\n",
    "\n",
    "            row_map[col_idx] = val\n",
    "\n",
    "        if max_col <= 0:\n",
    "            lines.append(\"\")\n",
    "        else:\n",
    "            parts = []\n",
    "            for i in range(1, max_col + 1):\n",
    "                parts.append(row_map.get(i, \"\"))\n",
    "            lines.append(\"\\t\".join(parts))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _odf_content_to_text(xml_bytes: bytes) -> str:\n",
    "    ns_text = \"urn:oasis:names:tc:opendocument:xmlns:text:1.0\"\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "\n",
    "    def walk(node):\n",
    "        pieces = []\n",
    "        if node.text is not None:\n",
    "            pieces.append(node.text)\n",
    "\n",
    "        for child in list(node):\n",
    "            tag = child.tag\n",
    "            if tag == f\"{{{ns_text}}}s\":\n",
    "                c = child.get(f\"{{{ns_text}}}c\") or child.get(\"c\") or \"1\"\n",
    "                try:\n",
    "                    pieces.append(\" \" * int(c))\n",
    "                except Exception:\n",
    "                    pieces.append(\" \")\n",
    "            else:\n",
    "                pieces.append(walk(child))\n",
    "\n",
    "            if child.tail is not None:\n",
    "                pieces.append(child.tail)\n",
    "        return \"\".join(pieces)\n",
    "\n",
    "    out_lines = []\n",
    "    for p in root.iter():\n",
    "        if p.tag == f\"{{{ns_text}}}p\":\n",
    "            out_lines.append(walk(p))\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def _html_bytes_to_text_preserve(b: bytes) -> str:\n",
    "    b = re.sub(rb\"(?i)<br\\s*/?>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"(?i)</p\\s*>\", b\"\\n\", b)\n",
    "    b = re.sub(rb\"<[^>]+>\", b\" \", b)\n",
    "    try:\n",
    "        return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return str(b)\n",
    "\n",
    "def extract_text_native(path: str) -> dict:\n",
    "    ft = detect_path_type(path)  # défini dans cellule précédente\n",
    "    ext = ft.ext.lower()\n",
    "    filename = Path(path).name\n",
    "\n",
    "    # PDF\n",
    "    if ext == \".pdf\":\n",
    "        PdfReader, backend = _get_pdf_reader_with_name()\n",
    "        if PdfReader is not None:\n",
    "            reader = PdfReader(path)\n",
    "            pages = reader.pages\n",
    "            pages_text = []\n",
    "            total = len(pages)\n",
    "\n",
    "            for i, page in enumerate(pages, start=1):\n",
    "                # MODIF MINIMALE: extraction \"layout\" si dispo => garde mieux espaces/sauts de ligne\n",
    "                txt = _pdf_extract_text_preserve_layout(page)\n",
    "                pages_text.append(txt)\n",
    "                if PRINT_DURING_EXTRACTION:\n",
    "                    print(f\"[native:{backend}] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": f\"native:pdf:{backend}\",\n",
    "                \"text\": full,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "\n",
    "        # Fallback pdfminer\n",
    "        try:\n",
    "            from pdfminer.high_level import extract_text  # type: ignore\n",
    "            full = extract_text(path) or \"\"\n",
    "            pages = full.split(\"\\f\")\n",
    "            pages_text = [p for p in pages]  # garder brut\n",
    "            total = len(pages_text)\n",
    "\n",
    "            if PRINT_DURING_EXTRACTION:\n",
    "                for i, txt in enumerate(pages_text, start=1):\n",
    "                    print(f\"[native:pdfminer] {filename} page {i}/{total}\")\n",
    "                    print(txt)\n",
    "                    print(\"-\" * 120)\n",
    "\n",
    "            full2 = \"\\n\\n\".join(pages_text)\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:pdfminer\",\n",
    "                \"text\": full2,\n",
    "                \"pages_text\": pages_text,\n",
    "                \"page_count_total\": total,\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:pdf:none\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "            }\n",
    "\n",
    "    # Office/OpenDocument/EPUB\n",
    "    if ext in {\".docx\", \".xlsx\", \".pptx\", \".odt\", \".ods\", \".odp\", \".epub\"}:\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                names = z.namelist()\n",
    "\n",
    "                if ext == \".docx\":\n",
    "                    parts = []\n",
    "                    if \"word/document.xml\" in names:\n",
    "                        parts.append(_docx_xml_to_text(z.read(\"word/document.xml\")))\n",
    "                    for nm in names:\n",
    "                        if nm.startswith(\"word/header\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                        if nm.startswith(\"word/footer\") and nm.endswith(\".xml\"):\n",
    "                            parts.append(_docx_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(parts)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:docx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],      # docx: pas de \"pages\" fiables => 1 bloc\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".xlsx\":\n",
    "                    shared = []\n",
    "                    if \"xl/sharedStrings.xml\" in names:\n",
    "                        try:\n",
    "                            shared = _xlsx_shared_strings(z.read(\"xl/sharedStrings.xml\"))\n",
    "                        except Exception:\n",
    "                            shared = []\n",
    "\n",
    "                    sheet_files = [nm for nm in names if nm.startswith(\"xl/worksheets/\") and nm.endswith(\".xml\")]\n",
    "                    sheet_files_sorted = sorted(sheet_files)\n",
    "\n",
    "                    pages_text = []\n",
    "                    total = len(sheet_files_sorted)\n",
    "                    for nm in sheet_files_sorted:\n",
    "                        sheet_text = _xlsx_sheet_to_text(z.read(nm), shared)\n",
    "                        pages_text.append(sheet_text)\n",
    "\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:xlsx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # sheets = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext == \".pptx\":\n",
    "                    slides = [nm for nm in names if nm.startswith(\"ppt/slides/\") and nm.endswith(\".xml\")]\n",
    "                    slides_sorted = sorted(slides)\n",
    "                    pages_text = []\n",
    "                    total = len(slides_sorted)\n",
    "                    for nm in slides_sorted:\n",
    "                        pages_text.append(_pptx_slide_xml_to_text(z.read(nm)))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:pptx:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,   # slides = pages\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "                if ext in {\".odt\", \".ods\", \".odp\"}:\n",
    "                    text = \"\"\n",
    "                    if \"content.xml\" in names:\n",
    "                        text = _odf_content_to_text(z.read(\"content.xml\"))\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": f\"native:{ext[1:]}:xml\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": [text],\n",
    "                        \"page_count_total\": 1,\n",
    "                    }\n",
    "\n",
    "                if ext == \".epub\":\n",
    "                    htmls = [nm for nm in names if nm.lower().endswith((\".xhtml\", \".html\", \".htm\"))]\n",
    "                    htmls_sorted = sorted(htmls)\n",
    "                    pages_text = []\n",
    "                    total = len(htmls_sorted)\n",
    "                    for nm in htmls_sorted:\n",
    "                        try:\n",
    "                            b = z.read(nm)\n",
    "                        except Exception:\n",
    "                            b = b\"\"\n",
    "                        pages_text.append(_html_bytes_to_text_preserve(b))\n",
    "                    text = \"\\n\\n\".join(pages_text)\n",
    "                    return {\n",
    "                        \"doc_id\": str(uuid.uuid4()),\n",
    "                        \"filename\": filename,\n",
    "                        \"source_path\": path,\n",
    "                        \"content\": \"text\",\n",
    "                        \"extraction\": \"native:epub:html\",\n",
    "                        \"text\": text,\n",
    "                        \"pages_text\": pages_text,\n",
    "                        \"page_count_total\": max(1, total),\n",
    "                    }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"filename\": filename,\n",
    "                \"source_path\": path,\n",
    "                \"content\": \"text\",\n",
    "                \"extraction\": \"native:zip:error\",\n",
    "                \"text\": \"\",\n",
    "                \"pages_text\": [\"\"],\n",
    "                \"page_count_total\": 1,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": str(uuid.uuid4()),\n",
    "        \"filename\": filename,\n",
    "        \"source_path\": path,\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": \"native:unsupported\",\n",
    "        \"text\": \"\",\n",
    "        \"pages_text\": [\"\"],\n",
    "        \"page_count_total\": 1,\n",
    "    }\n",
    "\n",
    "# TEXT_FILES vient de la cellule précédente (celle qui a fait les [skip])\n",
    "TEXT_DOCS: List[dict] = []\n",
    "if \"TEXT_FILES\" not in globals():\n",
    "    TEXT_FILES = []\n",
    "\n",
    "for p in TEXT_FILES:\n",
    "    try:\n",
    "        TEXT_DOCS.append(extract_text_native(p))\n",
    "    except Exception as e:\n",
    "        TEXT_DOCS.append({\n",
    "            \"doc_id\": str(uuid.uuid4()),\n",
    "            \"filename\": Path(p).name,\n",
    "            \"source_path\": p,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": \"native:error\",\n",
    "            \"text\": \"\",\n",
    "            \"pages_text\": [\"\"],\n",
    "            \"page_count_total\": 1,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "# -------------------- SORTIE FINALE (OCR + NATIVE) --------------------\n",
    "FINAL_DOCS: List[dict] = []\n",
    "\n",
    "# OCR docs (images)\n",
    "for d in DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"image_only\",\n",
    "        \"extraction\": \"ocr:tesseract\",\n",
    "        \"text\": d.get(\"ocr_text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "# Native docs (text)\n",
    "for d in TEXT_DOCS:\n",
    "    pages_text = d.get(\"pages_text\") or [d.get(\"text\") or \"\"]\n",
    "    page_count_total = d.get(\"page_count_total\") or len(pages_text) or 1\n",
    "    FINAL_DOCS.append({\n",
    "        \"doc_id\": d.get(\"doc_id\"),\n",
    "        \"filename\": d.get(\"filename\"),\n",
    "        \"content\": \"text\",\n",
    "        \"extraction\": d.get(\"extraction\"),\n",
    "        \"text\": d.get(\"text\", \"\"),\n",
    "        \"pages_text\": pages_text,\n",
    "        \"page_count_total\": page_count_total,\n",
    "    })\n",
    "\n",
    "for d in FINAL_DOCS:\n",
    "    filename = d.get(\"filename\")\n",
    "    content = d.get(\"content\")\n",
    "    extraction = d.get(\"extraction\")\n",
    "    pages_text = d.get(\"pages_text\") or []\n",
    "    total = int(d.get(\"page_count_total\") or len(pages_text) or 1)\n",
    "\n",
    "    print(f\"[doc] {filename} | content={content} | extraction={extraction} | pages={total}\")\n",
    "\n",
    "    if not pages_text:\n",
    "        print(\"\")\n",
    "        print(\"\\n\" + (\"-\" * 120) + \"\\n\")\n",
    "        continue\n",
    "\n",
    "    for i, txt in enumerate(pages_text, start=1):\n",
    "        print(f\"[page {i}/{total}]\")\n",
    "        print(txt if txt is not None else \"\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097c827",
   "metadata": {},
   "source": [
    "### Tokenisation \"layout\" (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c54db6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "[doc] testword.docx\n",
      "  doc_id       : 4bdf95b6-9ade-4d32-8dea-b58bd8c864bb\n",
      "  content      : text\n",
      "  extraction   : native:docx:xml\n",
      "  pages_total  : 1\n",
      "  chars_total  : 174\n",
      "  recompose_ok : True\n",
      "  paths:\n",
      "    - c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\testword.docx\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[page 1/1] source_path=c:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\documents\\testword.docx\n",
      "  lang         : fr\n",
      "  chars        : 174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  sentences_layout: 1 chunks total | sentences=1 | noise=0 | showing 1/1 (filter_is_sentence=True, fallback=False, min_nonspace=12)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[sent 1/1] page=1 start=0 end=174 line=1 col=0 chars=174 nonspace=142 is_noise=False is_sentence=True layout=plain\n",
      "Équipée d'un moteur V10 de 620 chevaux, l'Audi R8 passe de 0 à 100 km/h en moins de 3,5 secondes, affirmant ainsi sa place de supercar emblématique de la marque aux anneaux. \n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# ==================== Réglages ====================\n",
    "TARGET = None\n",
    "\n",
    "PRINT_SENTENCES = True\n",
    "MAX_SENTENCES_PREVIEW = 80   # None => imprime tout\n",
    "PRINT_REPR = False           # True => debug espaces invisibles via repr(chunk)\n",
    "\n",
    "MIN_SENTENCE_NONSPACE = 12\n",
    "PRINT_ONLY_SENTENCES = True\n",
    "PRINT_PAGE_TEXT = False\n",
    "\n",
    "# ==================== NLTK data ====================\n",
    "def _ensure_nltk():\n",
    "    for pkg, probe in ((\"punkt\", \"tokenizers/punkt\"), (\"punkt_tab\", \"tokenizers/punkt_tab\")):\n",
    "        try:\n",
    "            nltk.data.find(probe)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] NLTK download failed for {pkg}: {e}\")\n",
    "\n",
    "_ensure_nltk()\n",
    "\n",
    "# ==================== Détection langue (simple) ====================\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "# ==================== Sentence split \"layout\" (fallback) ====================\n",
    "_AR_END_RE = re.compile(r\"([.!?؟]+)(\\s+|$)\", flags=re.UNICODE)\n",
    "\n",
    "def split_ar_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    last = 0\n",
    "    for m in _AR_END_RE.finditer(text):\n",
    "        end = m.end()\n",
    "        chunks.append(text[last:end])\n",
    "        last = end\n",
    "    if last < len(text):\n",
    "        chunks.append(text[last:])\n",
    "    return chunks\n",
    "\n",
    "def _load_punkt_pickle(lang_pickle_name: str):\n",
    "    p = nltk.data.find(f\"tokenizers/punkt/{lang_pickle_name}.pickle\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def split_punkt_layout(text: str, lang_pickle_name: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    tok = _load_punkt_pickle(lang_pickle_name)\n",
    "    spans = list(tok.span_tokenize(text))\n",
    "    if not spans:\n",
    "        return [text]\n",
    "    starts = [0] + [spans[i][0] for i in range(1, len(spans))]\n",
    "    ends = [spans[i+1][0] for i in range(len(spans)-1)] + [len(text)]\n",
    "    return [text[starts[i]:ends[i]] for i in range(len(ends))]\n",
    "\n",
    "def sentence_chunks_layout(text: str, lang: str):\n",
    "    lang = (lang or \"\").lower()\n",
    "    if lang.startswith(\"ar\"):\n",
    "        return split_ar_layout(text)\n",
    "    if lang.startswith(\"fr\"):\n",
    "        return split_punkt_layout(text, \"french\")\n",
    "    if lang.startswith(\"en\"):\n",
    "        return split_punkt_layout(text, \"english\")\n",
    "    return split_punkt_layout(text, \"english\")\n",
    "\n",
    "# ==================== Split sections/alinéas (layout-preserving) ====================\n",
    "def _iter_line_spans(text: str):\n",
    "    if not text:\n",
    "        return\n",
    "    start = 0\n",
    "    for m in re.finditer(r\"\\n\", text):\n",
    "        end = m.end()\n",
    "        yield start, end\n",
    "        start = end\n",
    "    if start < len(text):\n",
    "        yield start, len(text)\n",
    "\n",
    "def _collapse_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def _mask_digits(s: str) -> str:\n",
    "    return re.sub(r\"\\d\", \"#\", s)\n",
    "\n",
    "_NUM_SIMPLE_RE = re.compile(r\"(?i)^[ \\t]*\\(?\\d{1,3}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ALPHA_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[a-z]\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_ROMAN_RE      = re.compile(r\"(?i)^[ \\t]*\\(?[ivxlcdm]{1,8}\\)?[ \\t]*[.)][ \\t]*(?:\\S|$)\")\n",
    "_NUM_MULTI_RE  = re.compile(r\"^[ \\t]*\\d{1,3}(?:\\.\\d{1,3})+[ \\t]*(?:[.)])?[ \\t]+(?=\\S)\")\n",
    "\n",
    "_KEYWORD_STRONG_RE = re.compile(r\"(?i)^[ \\t]*(article|section|chapitre|chapter|part)\\b\")\n",
    "_KEYWORD_WEAK_HEADING_RE = re.compile(\n",
    "    r\"\"\"(?ix)^[ \\t]*\n",
    "    (schedule|exhibit|appendix|annexe|annex)\n",
    "    [ \\t]+\n",
    "    ([A-Z0-9]{1,8}|[ivxlcdm]{1,8}|\\d{1,3})\n",
    "    [ \\t]*\n",
    "    (?:[:\\-–—][ \\t]*\\S.*)?\n",
    "    [ \\t]*$\n",
    "    \"\"\"\n",
    ")\n",
    "_SEP_RE = re.compile(r\"^[ \\t]*[-_]{4,}[ \\t]*$\")\n",
    "\n",
    "_LABEL_ONLY_RE = re.compile(\n",
    "    r\"(?is)^[ \\t]*\"\n",
    "    r\"(?:\\(?\\d{1,3}\\)?|\\(?[a-z]\\)?|\\(?[ivxlcdm]{1,8}\\)?)\"\n",
    "    r\"[ \\t]*[.)][ \\t]*$\"\n",
    ")\n",
    "\n",
    "def _is_section_start_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _KEYWORD_STRONG_RE.match(s):\n",
    "        return True\n",
    "    if _KEYWORD_WEAK_HEADING_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_SIMPLE_RE.match(s):\n",
    "        return True\n",
    "    if _NUM_MULTI_RE.match(s):\n",
    "        label = _collapse_ws(s).split(\" \", 1)[0]\n",
    "        parts = label.split(\".\")\n",
    "        if len(parts) >= 2 and parts[-1] in (\"00\", \"000\"):\n",
    "            return False\n",
    "        return True\n",
    "    if _ALPHA_RE.match(s) or _ROMAN_RE.match(s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _merge_label_only(chunks):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        if i + 1 < len(chunks) and _LABEL_ONLY_RE.match(chunks[i]):\n",
    "            out.append(chunks[i] + chunks[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(chunks[i])\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def split_sections_layout(text: str, allow_alpha_roman: bool = True):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = {0}\n",
    "    for ls, le in _iter_line_spans(text):\n",
    "        line = text[ls:le]\n",
    "        if _is_section_start_line(line):\n",
    "            if not allow_alpha_roman:\n",
    "                s = line.rstrip(\"\\n\")\n",
    "                if (\n",
    "                    _KEYWORD_STRONG_RE.match(s)\n",
    "                    or _KEYWORD_WEAK_HEADING_RE.match(s)\n",
    "                    or _NUM_SIMPLE_RE.match(s)\n",
    "                    or _NUM_MULTI_RE.match(s)\n",
    "                ):\n",
    "                    starts.add(ls)\n",
    "            else:\n",
    "                starts.add(ls)\n",
    "\n",
    "    starts = sorted(starts)\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        a, b = starts[i], starts[i+1]\n",
    "        if a != b:\n",
    "            chunks.append(text[a:b])\n",
    "    chunks.append(text[starts[-1]:])\n",
    "\n",
    "    return _merge_label_only(chunks)\n",
    "\n",
    "_PARA_BREAK_RE = re.compile(r\"(?:\\n[ \\t]*){2,}\")\n",
    "\n",
    "def split_paragraphs_layout(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    starts = [0]\n",
    "    for m in _PARA_BREAK_RE.finditer(text):\n",
    "        starts.append(m.end())\n",
    "    starts = sorted(set(starts))\n",
    "    if len(starts) == 1:\n",
    "        return [text]\n",
    "    out = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        out.append(text[starts[i]:starts[i+1]])\n",
    "    out.append(text[starts[-1]:])\n",
    "    return out\n",
    "\n",
    "def chunk_layout_universal(text: str, lang: str):\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    lines = [text[ls:le].rstrip(\"\\n\") for ls, le in _iter_line_spans(text)]\n",
    "    num_kw_hits = 0\n",
    "    alpha_roman_hits = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        if (\n",
    "            _KEYWORD_STRONG_RE.match(ln)\n",
    "            or _KEYWORD_WEAK_HEADING_RE.match(ln)\n",
    "            or _NUM_SIMPLE_RE.match(ln)\n",
    "            or _NUM_MULTI_RE.match(ln)\n",
    "        ):\n",
    "            num_kw_hits += 1\n",
    "        elif _ALPHA_RE.match(ln) or _ROMAN_RE.match(ln):\n",
    "            alpha_roman_hits += 1\n",
    "\n",
    "    is_structured = (num_kw_hits >= 2) or (alpha_roman_hits >= 3)\n",
    "\n",
    "    if is_structured:\n",
    "        chunks = split_sections_layout(text, allow_alpha_roman=True)\n",
    "        if len(chunks) > 1:\n",
    "            return chunks\n",
    "\n",
    "    paras = split_paragraphs_layout(text)\n",
    "    if len(paras) > 1:\n",
    "        return paras\n",
    "\n",
    "    return sentence_chunks_layout(text, lang)\n",
    "\n",
    "# ======================================================================\n",
    "#  MULTI-COLONNES (général, robuste) + TABLE (inchangé)\n",
    "#  + micro-table: interpréter les headers multi-col comme un \"table chunk\"\n",
    "# ======================================================================\n",
    "\n",
    "GAP_MIN_OCR = 10\n",
    "GAP_MIN_NATIVE = 6\n",
    "\n",
    "MERGE_COL_DIST_OCR = 22\n",
    "MERGE_COL_DIST_NATIVE = 16\n",
    "\n",
    "MICROTABLE_MAX_ROWS = 30\n",
    "MICROTABLE_MIN_DENS = 0.25\n",
    "MICROTABLE_MIN_MULTIROW = 2\n",
    "\n",
    "TABLE_HINT_RE = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    \\b(\n",
    "        qt[ée]|\n",
    "        désignation|designation|\n",
    "        prix|\n",
    "        montan?t|\n",
    "        r[ée]f[ée]rence|reference|\n",
    "        description|\n",
    "        quantit[ée]|\n",
    "        p\\.?\\s*unitaire|\n",
    "        valeur|\n",
    "        total\\s*ht|total|\n",
    "        tva|vat\n",
    "    )\\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d+(?:[.,]\\d+)?\")\n",
    "DEC_RE = re.compile(r\"\\d+[.,]\\d+\")\n",
    "\n",
    "def _space_runs_ge(s: str, n: int):\n",
    "    return [(m.start(), m.end()) for m in re.finditer(r\"[ ]{%d,}\" % n, s or \"\")]\n",
    "\n",
    "def _has_big_gap(s: str, gap_min: int, min_count: int = 1) -> bool:\n",
    "    return len(_space_runs_ge(s, gap_min)) >= min_count\n",
    "\n",
    "def _num_tokens(s: str) -> int:\n",
    "    return len(NUM_RE.findall(s or \"\"))\n",
    "\n",
    "def _dec_tokens(s: str) -> int:\n",
    "    return len(DEC_RE.findall(s or \"\"))\n",
    "\n",
    "def _is_table_line(line: str, gap_min: int) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return False\n",
    "    if s.count(\"\\t\") >= 2:\n",
    "        return True\n",
    "    if TABLE_HINT_RE.search(s):\n",
    "        return True\n",
    "    if _has_big_gap(s, gap_min, min_count=2):\n",
    "        if _num_tokens(s) >= 3:\n",
    "            return True\n",
    "        if _dec_tokens(s) >= 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _cluster_centers(values, tol=2, min_hits=1):\n",
    "    if not values:\n",
    "        return []\n",
    "    xs = sorted(values)\n",
    "    clusters = []\n",
    "    cur = [xs[0]]\n",
    "    for v in xs[1:]:\n",
    "        if abs(v - cur[-1]) <= tol:\n",
    "            cur.append(v)\n",
    "        else:\n",
    "            clusters.append(cur)\n",
    "            cur = [v]\n",
    "    clusters.append(cur)\n",
    "\n",
    "    centers = []\n",
    "    for c in clusters:\n",
    "        if len(c) >= min_hits:\n",
    "            c2 = sorted(c)\n",
    "            centers.append(c2[len(c2)//2])\n",
    "    return sorted(set(centers))\n",
    "\n",
    "def _upper_ratio(s: str) -> float:\n",
    "    letters = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s or \"\")\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    upp = sum(1 for ch in letters if ch.isupper())\n",
    "    return upp / max(1, len(letters))\n",
    "\n",
    "def _sep_spans(line: str, gap_min: int):\n",
    "    s = line or \"\"\n",
    "    spans = []\n",
    "    for m in re.finditer(r\"\\t+\", s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    for m in re.finditer(r\"[ ]{%d,}\" % gap_min, s):\n",
    "        spans.append((m.start(), m.end()))\n",
    "    if not spans:\n",
    "        return []\n",
    "    spans.sort()\n",
    "    merged = [spans[0]]\n",
    "    for a, b in spans[1:]:\n",
    "        la, lb = merged[-1]\n",
    "        if a <= lb:\n",
    "            merged[-1] = (la, max(lb, b))\n",
    "        else:\n",
    "            merged.append((a, b))\n",
    "    return merged\n",
    "\n",
    "def _line_segments_by_gaps(line: str, gap_min: int):\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    if not s.strip():\n",
    "        return []\n",
    "    seps = _sep_spans(s, gap_min)\n",
    "    segs = []\n",
    "    prev = 0\n",
    "    cuts = seps + [(len(s), len(s))]\n",
    "    for a, b in cuts:\n",
    "        if a < prev:\n",
    "            continue\n",
    "        chunk = s[prev:a]\n",
    "        m1 = re.search(r\"\\S\", chunk)\n",
    "        if m1:\n",
    "            l = m1.start()\n",
    "            r = len(chunk.rstrip(\" \\t\"))\n",
    "            text = chunk[l:r]\n",
    "            segs.append({\"x\": prev + l, \"a\": prev + l, \"b\": prev + r, \"text\": text})\n",
    "        prev = b\n",
    "    return segs\n",
    "\n",
    "def _looks_like_title_line(line: str) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\").strip()\n",
    "    if not s or len(s) > 50:\n",
    "        return False\n",
    "    if _SEP_RE.match(s):\n",
    "        return False\n",
    "    if _is_section_start_line(line):\n",
    "        return True\n",
    "    if _upper_ratio(s) >= 0.85 and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s) and not re.search(r\"\\d\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_multicol_candidate_line(line: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return False\n",
    "    if _SEP_RE.match(st):\n",
    "        return False\n",
    "    if _is_table_line(line, gap_min):\n",
    "        return False\n",
    "\n",
    "    segs = _line_segments_by_gaps(s, gap_min)\n",
    "    if len(segs) >= 3:\n",
    "        return True\n",
    "    if len(segs) == 2:\n",
    "        if is_ocr:\n",
    "            return True\n",
    "        if _has_big_gap(s, gap_min, 1):\n",
    "            return True\n",
    "        if re.search(r\"[:#№°/\\\\\\-–—]\", s) or re.search(r\"\\d\", s):\n",
    "            return True\n",
    "        if _upper_ratio(s) >= 0.70:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "_KV_GENERIC_RE = re.compile(r\"^\\s*(?P<k>[^:]{1,80}?)\\s{2,}(?P<v>\\S.+?)\\s*$\")\n",
    "\n",
    "def _looks_like_header_pair(k: str, v: str) -> bool:\n",
    "    k2 = (k or \"\").strip()\n",
    "    v2 = (v or \"\").strip()\n",
    "    if not k2 or not v2:\n",
    "        return False\n",
    "    if len(k2) <= 25 and len(v2) <= 25 and _upper_ratio(k2) >= 0.85 and _upper_ratio(v2) >= 0.85:\n",
    "        if not re.search(r\"\\d\", k2 + v2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _looks_like_addressish(line: str) -> bool:\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if re.search(r\"(rue|route|avenue|bd|boulevard|street|st\\.|road|zip|code\\s*postal|bp)\", s, flags=re.I):\n",
    "        return True\n",
    "    if len(s) >= 10 and not s.endswith(\":\") and re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _normalize_kv_generic(text: str) -> str:\n",
    "    out = []\n",
    "    for raw in (text or \"\").splitlines():\n",
    "        line = raw.rstrip(\"\\n\")\n",
    "        if not line.strip():\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if \":\" in line:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if _looks_like_addressish(line):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        m = _KV_GENERIC_RE.match(line)\n",
    "        if not m:\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        k = _collapse_ws(m.group(\"k\"))\n",
    "        v = _collapse_ws(m.group(\"v\"))\n",
    "        if _looks_like_header_pair(k, v):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        if not re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", k):\n",
    "            out.append(line.strip())\n",
    "            continue\n",
    "        out.append(f\"{k}: {v}\" if v else k)\n",
    "    return \"\\n\".join(out) + (\"\\n\" if (text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _strip_sep_lines(block_text: str) -> str:\n",
    "    if not block_text:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for ln in (block_text or \"\").splitlines():\n",
    "        if _SEP_RE.match(ln.strip()):\n",
    "            continue\n",
    "        out.append(ln.rstrip())\n",
    "    txt = \"\\n\".join(out).rstrip()\n",
    "    return txt + (\"\\n\" if (block_text or \"\").endswith(\"\\n\") else \"\")\n",
    "\n",
    "def _assign_to_centers(x: int, centers, tol: int):\n",
    "    if not centers:\n",
    "        return 0\n",
    "    best_i = 0\n",
    "    best_d = abs(x - centers[0])\n",
    "    for i in range(1, len(centers)):\n",
    "        d = abs(x - centers[i])\n",
    "        if d < best_d:\n",
    "            best_d = d\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def _merge_close_columns(centers, row_cells, merge_dist: int):\n",
    "    i = 0\n",
    "    while i < len(centers) - 1:\n",
    "        if (centers[i+1] - centers[i]) <= merge_dist:\n",
    "            both = 0\n",
    "            alone_next = 0\n",
    "            for r in row_cells:\n",
    "                hi = i in r\n",
    "                hj = (i+1) in r\n",
    "                if hj and hi:\n",
    "                    both += 1\n",
    "                elif hj and not hi:\n",
    "                    alone_next += 1\n",
    "            if both >= 1 and alone_next <= max(1, int(0.2 * (both + alone_next))):\n",
    "                for r in row_cells:\n",
    "                    if (i+1) in r:\n",
    "                        t2, sp2 = r.pop(i+1)\n",
    "                        if i in r:\n",
    "                            t1, sp1 = r[i]\n",
    "                            r[i] = ((t1 + \"  \" + t2).strip(), sp1 + sp2)\n",
    "                        else:\n",
    "                            r[i] = (t2, sp2)\n",
    "\n",
    "                centers.pop(i+1)\n",
    "\n",
    "                for r in row_cells:\n",
    "                    ks = sorted([k for k in r.keys() if k > i+1])\n",
    "                    for k in ks:\n",
    "                        r[k-1] = r.pop(k)\n",
    "                continue\n",
    "        i += 1\n",
    "    return centers, row_cells\n",
    "\n",
    "def _is_grid_like(row_cells, col_count: int):\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if not rows:\n",
    "        return False\n",
    "    n_rows = len(rows)\n",
    "    if n_rows > 5:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / n_rows\n",
    "    return dens >= 0.70\n",
    "\n",
    "def _is_micro_table_like(row_cells, col_count: int) -> bool:\n",
    "    if col_count < 2:\n",
    "        return False\n",
    "    rows = [r for r in row_cells if any((t.strip() for t, _ in r.values()))]\n",
    "    if len(rows) < 2:\n",
    "        return False\n",
    "    if len(rows) > MICROTABLE_MAX_ROWS:\n",
    "        return False\n",
    "    multi = sum(1 for r in rows if len(r) >= 2)\n",
    "    if multi < MICROTABLE_MIN_MULTIROW:\n",
    "        return False\n",
    "    dens = sum((len(r) / max(1, col_count)) for r in rows) / max(1, len(rows))\n",
    "    return dens >= MICROTABLE_MIN_DENS\n",
    "\n",
    "def _transpose_or_group_multicol(block_text: str, abs_start: int, gap_min: int, is_ocr: bool):\n",
    "    lines = []\n",
    "    segs_by_line = []\n",
    "\n",
    "    for ls, le in _iter_line_spans(block_text):\n",
    "        line_full = block_text[ls:le]\n",
    "        s = line_full[:-1] if line_full.endswith(\"\\n\") else line_full\n",
    "\n",
    "        lines.append((ls, le, line_full, s))\n",
    "\n",
    "        if _SEP_RE.match(s.strip()):\n",
    "            segs_by_line.append([])\n",
    "            continue\n",
    "\n",
    "        segs = _line_segments_by_gaps(s, gap_min)\n",
    "        segs = [g for g in segs if g.get(\"text\", \"\").strip()]\n",
    "        segs_by_line.append(segs)\n",
    "\n",
    "    xs = []\n",
    "    for segs in segs_by_line:\n",
    "        for g in segs:\n",
    "            txt = g[\"text\"].strip()\n",
    "            if len(txt) == 1 and txt in (\":\", \"|\", \"-\", \"_\"):\n",
    "                continue\n",
    "            xs.append(int(g[\"x\"]))\n",
    "\n",
    "    if not xs:\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    tol_cluster = 3 if is_ocr else 2\n",
    "    centers = _cluster_centers(xs, tol=tol_cluster, min_hits=1)\n",
    "    min_x = min(xs)\n",
    "    if min_x not in centers:\n",
    "        centers = sorted([min_x] + centers)\n",
    "    centers = centers[:8]\n",
    "\n",
    "    tol_assign = 6 if is_ocr else 4\n",
    "    row_cells = []\n",
    "    for (ls, le, line_full, s), segs in zip(lines, segs_by_line):\n",
    "        r = {}\n",
    "        for g in segs:\n",
    "            ci = _assign_to_centers(int(g[\"x\"]), centers, tol_assign)\n",
    "            a = abs_start + ls + int(g[\"a\"])\n",
    "            b = abs_start + ls + int(g[\"b\"])\n",
    "            txt = g[\"text\"].strip()\n",
    "\n",
    "            if ci in r:\n",
    "                t0, sp0 = r[ci]\n",
    "                r[ci] = ((t0 + \" \" + txt).strip(), sp0 + [(a, b)])\n",
    "            else:\n",
    "                r[ci] = (txt, [(a, b)])\n",
    "        row_cells.append(r)\n",
    "\n",
    "    merge_dist = MERGE_COL_DIST_OCR if is_ocr else MERGE_COL_DIST_NATIVE\n",
    "    centers, row_cells = _merge_close_columns(centers, row_cells, merge_dist=merge_dist)\n",
    "    col_count = len(centers)\n",
    "\n",
    "    if _is_micro_table_like(row_cells, col_count):\n",
    "        table_rows = []\n",
    "        for (ls, le, line_full, s) in lines:\n",
    "            table_rows.append({\"text\": line_full, \"spans\": [(abs_start + ls, abs_start + le)]})\n",
    "\n",
    "        table_cells = []\n",
    "        for r in row_cells:\n",
    "            row = []\n",
    "            for ci in range(col_count):\n",
    "                if ci in r:\n",
    "                    t, sp = r[ci]\n",
    "                    row.append({\"col\": ci, \"text\": t, \"spans\": [(int(a), int(b)) for a, b in sp if b > a]})\n",
    "                else:\n",
    "                    row.append({\"col\": ci, \"text\": \"\", \"spans\": []})\n",
    "            table_cells.append(row)\n",
    "\n",
    "        txt = _strip_sep_lines(block_text)\n",
    "        return [{\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"header\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "            \"table_rows\": table_rows,\n",
    "            \"table_cells\": table_cells,\n",
    "            \"header_source\": \"micro_multicol\",\n",
    "            \"column_centers\": centers,\n",
    "        }]\n",
    "\n",
    "    if _is_grid_like(row_cells, col_count):\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"multicol_grid\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    col_items = []\n",
    "    for ci in range(col_count):\n",
    "        out_lines = []\n",
    "        spans = []\n",
    "        for r in row_cells:\n",
    "            if ci in r:\n",
    "                t, sp = r[ci]\n",
    "                out_lines.append(t)\n",
    "                spans.extend(sp)\n",
    "            else:\n",
    "                out_lines.append(\"\")\n",
    "\n",
    "        while out_lines and not out_lines[0].strip():\n",
    "            out_lines.pop(0)\n",
    "        while out_lines and not out_lines[-1].strip():\n",
    "            out_lines.pop()\n",
    "\n",
    "        compact = []\n",
    "        blank = 0\n",
    "        for ln in out_lines:\n",
    "            if not ln.strip():\n",
    "                blank += 1\n",
    "                if blank <= 1:\n",
    "                    compact.append(\"\")\n",
    "            else:\n",
    "                blank = 0\n",
    "                compact.append(ln)\n",
    "\n",
    "        txt = \"\\n\".join(compact).rstrip() + (\"\\n\" if block_text.endswith(\"\\n\") else \"\")\n",
    "        txt = _normalize_kv_generic(txt)\n",
    "\n",
    "        if not _collapse_ws(txt).strip():\n",
    "            continue\n",
    "\n",
    "        if spans:\n",
    "            st = min(a for a, _ in spans)\n",
    "            en = max(b for _, b in spans)\n",
    "        else:\n",
    "            st = abs_start\n",
    "            en = abs_start + len(block_text)\n",
    "\n",
    "        col_items.append({\n",
    "            \"text\": txt,\n",
    "            \"spans\": [(int(a), int(b)) for (a, b) in spans if b > a],\n",
    "            \"start\": st,\n",
    "            \"end\": en,\n",
    "            \"layout_kind\": \"multicol_col\",\n",
    "            \"col_index\": ci,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        })\n",
    "\n",
    "    if not col_items:\n",
    "        return [{\n",
    "            \"text\": _strip_sep_lines(block_text),\n",
    "            \"spans\": [(abs_start, abs_start + len(block_text))],\n",
    "            \"start\": abs_start,\n",
    "            \"end\": abs_start + len(block_text),\n",
    "            \"layout_kind\": \"plain\",\n",
    "            \"col_index\": None,\n",
    "            \"block_start\": abs_start,\n",
    "            \"block_end\": abs_start + len(block_text),\n",
    "        }]\n",
    "\n",
    "    return col_items\n",
    "\n",
    "def _looks_like_paragraphish(line_full: str) -> bool:\n",
    "    s = (line_full or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if len(s) >= 120:\n",
    "        words = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", s)\n",
    "        if len(words) >= 10 and not _has_big_gap(s, 6, 1):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _is_address_continuation_line(line_full: str, gap_min: int, is_ocr: bool) -> bool:\n",
    "    s = (line_full or \"\").rstrip(\"\\n\")\n",
    "    st = s.strip()\n",
    "    if not st:\n",
    "        return True\n",
    "    if _SEP_RE.match(st):\n",
    "        return True\n",
    "    if _is_table_line(line_full, gap_min):\n",
    "        return False\n",
    "    if TABLE_HINT_RE.search(st):\n",
    "        return False\n",
    "    if _is_section_start_line(line_full):\n",
    "        return False\n",
    "    if _dec_tokens(st) > 0:\n",
    "        return False\n",
    "    if _num_tokens(st) > (4 if is_ocr else 6):\n",
    "        return False\n",
    "    if re.search(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", st) or _AR_RE.search(st):\n",
    "        return True\n",
    "    if re.match(r\"^\\d{4,6}$\", st):\n",
    "        return True\n",
    "    if re.search(r\"[@+/,-]\", st) and len(st) <= 120:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _collect_table_block(lines, start_i, gap_min):\n",
    "    n = len(lines)\n",
    "    i = start_i\n",
    "    blank_run = 0\n",
    "    seen_data = 0\n",
    "    collected = []\n",
    "\n",
    "    # FIX: reconnaître une \"wrap line\" indépendamment de la ligne précédente (utile pour chaîner wrap+wrap)\n",
    "    def _looks_like_wrap_line(s_raw: str) -> bool:\n",
    "        if not s_raw:\n",
    "            return False\n",
    "        # doit être indenté (comme dans ton exemple)\n",
    "        if not re.match(r\"^[ \\t]{2,}\\S\", s_raw):\n",
    "            return False\n",
    "        s_l = s_raw.lstrip(\" \\t\")\n",
    "        # pas une ligne structurante, pas de structure table\n",
    "        if _is_section_start_line(s_raw):\n",
    "            return False\n",
    "        if s_l.count(\"\\t\") >= 2:\n",
    "            return False\n",
    "        if _has_big_gap(s_l, gap_min, min_count=1):\n",
    "            return False\n",
    "        # très peu de signaux numériques (évite d'absorber TOTAL/TVA/etc)\n",
    "        if _dec_tokens(s_l) != 0:\n",
    "            return False\n",
    "        if _num_tokens(s_l) > 1:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    while i < n:\n",
    "        line_full, ls, le = lines[i]\n",
    "        s = line_full.rstrip(\"\\n\")\n",
    "\n",
    "        if not s.strip():\n",
    "            blank_run += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        is_tbl = _is_table_line(line_full, gap_min)\n",
    "\n",
    "        if is_tbl:\n",
    "            blank_run = 0\n",
    "            if _dec_tokens(s) >= 1 or _num_tokens(s) >= 3 or TABLE_HINT_RE.search(s):\n",
    "                seen_data += 1\n",
    "            collected.append((line_full, ls, le))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # FIX: accepter wrap, y compris wrap qui suit wrap (pas seulement table_line)\n",
    "        if seen_data >= 1 and _looks_like_wrap_line(s):\n",
    "            prev_nonblank = None\n",
    "            for plf, _, _ in reversed(collected):\n",
    "                if plf.strip():\n",
    "                    prev_nonblank = plf.rstrip(\"\\n\")\n",
    "                    break\n",
    "            # on continue si la ligne précédente est soit une ligne de table, soit déjà une wrap\n",
    "            if prev_nonblank and (_is_table_line(prev_nonblank, gap_min) or _looks_like_wrap_line(prev_nonblank)):\n",
    "                blank_run = 0\n",
    "                collected.append((line_full, ls, le))\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        if seen_data >= 2 and blank_run >= 2:\n",
    "            break\n",
    "        if seen_data >= 1 and blank_run >= 1:\n",
    "            break\n",
    "        break\n",
    "\n",
    "    while collected and not collected[-1][0].strip():\n",
    "        collected.pop()\n",
    "\n",
    "    return collected, i\n",
    "\n",
    "def _make_span_item(page_text, spans, text_override, kind, meta=None):\n",
    "    spans2 = [(int(a), int(b)) for (a, b) in (spans or []) if b > a]\n",
    "    if spans2:\n",
    "        st = min(a for a, _ in spans2)\n",
    "        en = max(b for _, b in spans2)\n",
    "    else:\n",
    "        st = 0\n",
    "        en = 0\n",
    "    it = {\"text\": text_override, \"spans\": spans2, \"start\": st, \"end\": en, \"layout_kind\": kind}\n",
    "    if meta:\n",
    "        it.update(meta)\n",
    "    return it\n",
    "\n",
    "def layout_items(page_text: str, lang: str, extraction: str = \"\"):\n",
    "    if not page_text:\n",
    "        return []\n",
    "\n",
    "    is_ocr = str(extraction or \"\").startswith(\"ocr:\")\n",
    "    gap_min = GAP_MIN_OCR if is_ocr else GAP_MIN_NATIVE\n",
    "\n",
    "    lines = []\n",
    "    for ls, le in _iter_line_spans(page_text):\n",
    "        lines.append((page_text[ls:le], ls, le))\n",
    "\n",
    "    items = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    def _starts_table(i0):\n",
    "        return _is_table_line(lines[i0][0], gap_min)\n",
    "\n",
    "    def _starts_multicol(i0):\n",
    "        return _is_multicol_candidate_line(lines[i0][0], gap_min=gap_min, is_ocr=is_ocr)\n",
    "\n",
    "    while i < n:\n",
    "        if _starts_table(i):\n",
    "            collected, j = _collect_table_block(lines, i, gap_min=gap_min)\n",
    "            if collected:\n",
    "                a0 = collected[0][1]\n",
    "                b0 = collected[-1][2]\n",
    "                block_text = page_text[a0:b0]\n",
    "                table_rows = [{\"text\": lf, \"spans\": [(lls, lle)]} for (lf, lls, lle) in collected]\n",
    "                items.append(_make_span_item(\n",
    "                    page_text,\n",
    "                    spans=[(a0, b0)],\n",
    "                    text_override=block_text,\n",
    "                    kind=\"table\",\n",
    "                    meta={\"table_rows\": table_rows}\n",
    "                ))\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        if _starts_multicol(i):\n",
    "            start = i\n",
    "\n",
    "            if start - 1 >= 0:\n",
    "                prev_line = lines[start - 1][0]\n",
    "                if _looks_like_title_line(prev_line) and not _starts_table(start - 1):\n",
    "                    start -= 1\n",
    "\n",
    "            j = i\n",
    "            saw_any = False\n",
    "            blank_run = 0\n",
    "            noncol_inside = 0\n",
    "\n",
    "            MAX_INBLOCK_BLANK = 6\n",
    "            MAX_INBLOCK_LINES = 140\n",
    "            MAX_NONCOL_INSIDE = 25\n",
    "            weak_gap = max(3, gap_min - (3 if is_ocr else 2))\n",
    "\n",
    "            while j < n and (j - start) < MAX_INBLOCK_LINES:\n",
    "                if _starts_table(j):\n",
    "                    break\n",
    "\n",
    "                lf, lls, lle = lines[j]\n",
    "                ss = lf.rstrip(\"\\n\")\n",
    "\n",
    "                if not ss.strip() or _SEP_RE.match(ss.strip()):\n",
    "                    blank_run += 1\n",
    "                    j += 1\n",
    "                    if saw_any and blank_run >= MAX_INBLOCK_BLANK:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                blank_run = 0\n",
    "\n",
    "                if _starts_multicol(j):\n",
    "                    saw_any = True\n",
    "                    noncol_inside = 0\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                if saw_any and noncol_inside < MAX_NONCOL_INSIDE:\n",
    "                    if _is_address_continuation_line(lf, gap_min=gap_min, is_ocr=is_ocr) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    if _has_big_gap(ss, weak_gap, min_count=1) and not _looks_like_paragraphish(lf):\n",
    "                        noncol_inside += 1\n",
    "                        j += 1\n",
    "                        continue\n",
    "\n",
    "                break\n",
    "\n",
    "            end = j if j > i else i + 1\n",
    "\n",
    "            a0 = lines[start][1]\n",
    "            b0 = lines[end-1][2] if end-1 >= start else lines[start][2]\n",
    "            block_text = page_text[a0:b0]\n",
    "\n",
    "            items.extend(_transpose_or_group_multicol(block_text, abs_start=a0, gap_min=gap_min, is_ocr=is_ocr))\n",
    "\n",
    "            i = end\n",
    "            continue\n",
    "\n",
    "        start = i\n",
    "        j = i\n",
    "        while j < n:\n",
    "            if _starts_table(j) or _starts_multicol(j):\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        a0 = lines[start][1]\n",
    "        b0 = lines[j-1][2] if j-1 >= start else lines[start][2]\n",
    "        plain_text = page_text[a0:b0]\n",
    "\n",
    "        chunks = chunk_layout_universal(plain_text, lang)\n",
    "        pos = 0\n",
    "        for ch in chunks:\n",
    "            ca = a0 + pos\n",
    "            cb = ca + len(ch)\n",
    "            pos += len(ch)\n",
    "            items.append(_make_span_item(page_text, spans=[(ca, cb)], text_override=ch, kind=\"plain\"))\n",
    "\n",
    "        i = j if j > start else i + 1\n",
    "\n",
    "    def _k(it):\n",
    "        if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\"):\n",
    "            return (it.get(\"block_start\", it.get(\"start\", 0)), it.get(\"col_index\", 0) if it.get(\"col_index\") is not None else -1)\n",
    "        return (it.get(\"start\", 0), 0)\n",
    "\n",
    "    items.sort(key=_k)\n",
    "    return items\n",
    "\n",
    "# ==================== Noise detection (audit) ====================\n",
    "_NOISE_LINE_RE = re.compile(\n",
    "    r\"(?i)^\\s*(sample|confidential|draft)\\s*$|\"\n",
    "    r\"^\\s*page\\s+\\d+\\s*(?:of|/)\\s*\\d+\\s*$|\"\n",
    "    r\"^\\s*\\d+\\s*(?:of|/)\\s*\\d+\\s*$\"\n",
    ")\n",
    "\n",
    "def build_noise_keys_for_doc(pages_text):\n",
    "    if not pages_text:\n",
    "        return set()\n",
    "    page_count = len(pages_text)\n",
    "    if page_count < 3:\n",
    "        return set()\n",
    "    min_pages = max(3, int(math.ceil(page_count * 0.30)))\n",
    "\n",
    "    counts = {}\n",
    "    counts_masked = {}\n",
    "\n",
    "    for txt in pages_text:\n",
    "        seen = set()\n",
    "        seen_m = set()\n",
    "        for ls, le in _iter_line_spans(txt or \"\"):\n",
    "            line = (txt[ls:le]).rstrip(\"\\n\")\n",
    "            key = _collapse_ws(line).lower()\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            if _SEP_RE.match(key) or _NOISE_LINE_RE.match(line):\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                continue\n",
    "\n",
    "            mkey = _mask_digits(key)\n",
    "\n",
    "            if key not in seen:\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "                seen.add(key)\n",
    "            if mkey not in seen_m:\n",
    "                counts_masked[mkey] = counts_masked.get(mkey, 0) + 1\n",
    "                seen_m.add(mkey)\n",
    "\n",
    "    noise_keys = set()\n",
    "    for k, c in counts.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(k)\n",
    "    for mk, c in counts_masked.items():\n",
    "        if c >= min_pages:\n",
    "            noise_keys.add(mk)\n",
    "\n",
    "    return noise_keys\n",
    "\n",
    "def chunk_is_noise(chunk_text: str, noise_keys: set) -> bool:\n",
    "    if not chunk_text:\n",
    "        return True\n",
    "\n",
    "    has_nonempty = False\n",
    "    for ls, le in _iter_line_spans(chunk_text):\n",
    "        line = chunk_text[ls:le].rstrip(\"\\n\")\n",
    "        st = line.strip()\n",
    "        if not st:\n",
    "            continue\n",
    "        if _SEP_RE.match(st):\n",
    "            continue\n",
    "\n",
    "        has_nonempty = True\n",
    "        key = _collapse_ws(line).lower()\n",
    "        mkey = _mask_digits(key)\n",
    "\n",
    "        if _NOISE_LINE_RE.match(line):\n",
    "            continue\n",
    "        if key in noise_keys or mkey in noise_keys:\n",
    "            continue\n",
    "\n",
    "        return False\n",
    "\n",
    "    return True if has_nonempty else True\n",
    "\n",
    "# ==================== Helpers emplacement (page) ====================\n",
    "_WS_RE = re.compile(r\"\\s+\", flags=re.UNICODE)\n",
    "\n",
    "def _nonspace_len(s: str) -> int:\n",
    "    return len(_WS_RE.sub(\"\", s or \"\"))\n",
    "\n",
    "def _line_col_from_offset(text: str, off: int):\n",
    "    if off < 0:\n",
    "        off = 0\n",
    "    if off > len(text):\n",
    "        off = len(text)\n",
    "    line = text.count(\"\\n\", 0, off) + 1\n",
    "    last_nl = text.rfind(\"\\n\", 0, off)\n",
    "    col = off if last_nl < 0 else (off - last_nl - 1)\n",
    "    return line, col\n",
    "\n",
    "# ==================== Metadonnées depuis DOCS / TEXT_DOCS ====================\n",
    "def _safe_str(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _unique_keep_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def _pdf_extract_pages_text(path: str):\n",
    "    PdfReader = _get_pdf_reader()  # défini dans ta cellule d'extraction\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        out = []\n",
    "        for p in reader.pages:\n",
    "            out.append(p.extract_text() or \"\")\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _pdf_page_count(path: str):\n",
    "    PdfReader = _get_pdf_reader()\n",
    "    if PdfReader is None:\n",
    "        return None\n",
    "    try:\n",
    "        return len(PdfReader(path).pages)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ==================== Vérifier FINAL_DOCS ====================\n",
    "if \"FINAL_DOCS\" not in globals() or not isinstance(FINAL_DOCS, list):\n",
    "    raise RuntimeError(\"FINAL_DOCS not found. Exécute d'abord la cellule précédente (celle qui imprime FINAL PRINT).\")\n",
    "\n",
    "# ==================== Construire une structure DOC -> PAGES ====================\n",
    "DOC_PACK = []\n",
    "\n",
    "# 1) OCR: DOCS (si dispo)\n",
    "if \"DOCS\" in globals() and isinstance(DOCS, list):\n",
    "    for d in DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        pages = d.get(\"pages\", []) or []\n",
    "        page_count_total = d.get(\"page_count_total\") if d.get(\"page_count_total\") else len(pages)\n",
    "\n",
    "        paths = []\n",
    "        for p in pages:\n",
    "            sp = p.get(\"source_path\") or p.get(\"path\")\n",
    "            if sp:\n",
    "                paths.append(_safe_str(sp))\n",
    "        paths = _unique_keep_order(paths)\n",
    "\n",
    "        pages_out = []\n",
    "        for p in pages:\n",
    "            pages_out.append({\n",
    "                \"page_index\": int(p.get(\"page_index\") or 1),\n",
    "                \"text\": p.get(\"ocr_text\") or \"\",\n",
    "                \"source_path\": _safe_str(p.get(\"source_path\") or p.get(\"path\") or \"\"),\n",
    "            })\n",
    "        pages_out.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"image_only\",\n",
    "            \"extraction\": \"ocr:tesseract\",\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 2) NATIVE: TEXT_DOCS (si dispo)\n",
    "if \"TEXT_DOCS\" in globals() and isinstance(TEXT_DOCS, list):\n",
    "    for d in TEXT_DOCS:\n",
    "        doc_id = d.get(\"doc_id\")\n",
    "        filename = d.get(\"filename\") or \"unknown\"\n",
    "        extraction = d.get(\"extraction\") or \"native:unknown\"\n",
    "        sp = d.get(\"source_path\") or \"\"\n",
    "        paths = _unique_keep_order([_safe_str(sp)]) if sp else []\n",
    "        full_text = d.get(\"text\") or \"\"\n",
    "\n",
    "        pages_out = []\n",
    "        page_count_total = d.get(\"page_count_total\", None)\n",
    "        pages_text = d.get(\"pages_text\", None)\n",
    "\n",
    "        if pages_text is not None and isinstance(pages_text, list) and len(pages_text) > 0:\n",
    "            page_count_total = page_count_total or len(pages_text)\n",
    "            for i2, txt in enumerate(pages_text, start=1):\n",
    "                pages_out.append({\n",
    "                    \"page_index\": i2,\n",
    "                    \"text\": txt or \"\",\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "        else:\n",
    "            if sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "                pages_text2 = _pdf_extract_pages_text(sp)\n",
    "                if pages_text2:\n",
    "                    page_count_total = page_count_total or len(pages_text2)\n",
    "                    for i2, txt in enumerate(pages_text2, start=1):\n",
    "                        pages_out.append({\n",
    "                            \"page_index\": i2,\n",
    "                            \"text\": txt or \"\",\n",
    "                            \"source_path\": _safe_str(sp),\n",
    "                        })\n",
    "                else:\n",
    "                    pages_out.append({\n",
    "                        \"page_index\": 1,\n",
    "                        \"text\": full_text,\n",
    "                        \"source_path\": _safe_str(sp),\n",
    "                    })\n",
    "                    page_count_total = page_count_total or 1\n",
    "            else:\n",
    "                pages_out.append({\n",
    "                    \"page_index\": 1,\n",
    "                    \"text\": full_text,\n",
    "                    \"source_path\": _safe_str(sp),\n",
    "                })\n",
    "                page_count_total = page_count_total or 1\n",
    "\n",
    "        if page_count_total is None and sp and str(sp).lower().endswith(\".pdf\") and Path(sp).exists():\n",
    "            pc = _pdf_page_count(sp)\n",
    "            if pc is not None:\n",
    "                page_count_total = pc\n",
    "\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": \"text\",\n",
    "            \"extraction\": extraction,\n",
    "            \"paths\": paths,\n",
    "            \"page_count_total\": page_count_total,\n",
    "            \"pages\": pages_out,\n",
    "        })\n",
    "\n",
    "# 3) Fallback à FINAL_DOCS\n",
    "if not DOC_PACK:\n",
    "    for d in FINAL_DOCS:\n",
    "        DOC_PACK.append({\n",
    "            \"doc_id\": d.get(\"doc_id\"),\n",
    "            \"filename\": d.get(\"filename\") or \"unknown\",\n",
    "            \"content\": d.get(\"content\"),\n",
    "            \"extraction\": d.get(\"extraction\"),\n",
    "            \"paths\": [],\n",
    "            \"page_count_total\": 1,\n",
    "            \"pages\": [{\"page_index\": 1, \"text\": d.get(\"text\") or \"\", \"source_path\": \"\"}],\n",
    "        })\n",
    "\n",
    "# ==================== Tokeniser: construire TOK_DOCS ====================\n",
    "TOK_DOCS = []\n",
    "\n",
    "for doc in DOC_PACK:\n",
    "    doc_id = doc.get(\"doc_id\")\n",
    "    filename = doc.get(\"filename\") or \"unknown\"\n",
    "    extraction = doc.get(\"extraction\") or \"\"\n",
    "    content_type = doc.get(\"content\")\n",
    "    paths = doc.get(\"paths\") or []\n",
    "    page_count_total = doc.get(\"page_count_total\")\n",
    "\n",
    "    pages_text_for_noise = [(p.get(\"text\") or \"\") for p in (doc.get(\"pages\") or [])]\n",
    "    noise_keys = build_noise_keys_for_doc(pages_text_for_noise)\n",
    "\n",
    "    pages_tok = []\n",
    "    doc_chars_total = 0\n",
    "    recompose_ok_doc = True\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        page_index = int(pg.get(\"page_index\") or 1)\n",
    "        page_text = pg.get(\"text\") or \"\"\n",
    "        doc_chars_total += len(page_text)\n",
    "\n",
    "        lang = detect_lang(page_text)\n",
    "\n",
    "        items = layout_items(page_text, lang, extraction=extraction)\n",
    "        recompose_ok = False if any(it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\") for it in items) else True\n",
    "        if not recompose_ok:\n",
    "            recompose_ok_doc = False\n",
    "\n",
    "        sent_items = []\n",
    "        for it in items:\n",
    "            chunk = it[\"text\"]\n",
    "            start = int(it.get(\"start\", 0))\n",
    "            end = int(it.get(\"end\", start + len(chunk)))\n",
    "\n",
    "            line, col = _line_col_from_offset(page_text, start)\n",
    "            nonspace = _nonspace_len(chunk)\n",
    "\n",
    "            is_noise = chunk_is_noise(chunk, noise_keys)\n",
    "\n",
    "            if it.get(\"layout_kind\") in (\"multicol_col\", \"multicol_grid\", \"table\", \"header\"):\n",
    "                is_sentence = (not is_noise) and (nonspace >= 1)\n",
    "            else:\n",
    "                is_sentence = (nonspace >= MIN_SENTENCE_NONSPACE) and (not is_noise)\n",
    "\n",
    "            sent_items.append({\n",
    "                \"text\": chunk,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"line\": line,\n",
    "                \"col\": col,\n",
    "                \"chars\": len(chunk),\n",
    "                \"nonspace\": nonspace,\n",
    "                \"is_noise\": is_noise,\n",
    "                \"is_sentence\": is_sentence,\n",
    "                \"spans\": it.get(\"spans\", []),\n",
    "                \"layout_kind\": it.get(\"layout_kind\", \"plain\"),\n",
    "                \"col_index\": it.get(\"col_index\", None),\n",
    "                \"table_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_rows\": it.get(\"table_rows\", None),\n",
    "                \"header_cells\": it.get(\"table_cells\", None),\n",
    "                \"header_source\": it.get(\"header_source\", None),\n",
    "            })\n",
    "\n",
    "        pages_tok.append({\n",
    "            \"page_index\": page_index,\n",
    "            \"source_path\": pg.get(\"source_path\") or \"\",\n",
    "            \"lang\": lang,\n",
    "            \"chars\": len(page_text),\n",
    "            \"recompose_ok\": recompose_ok,\n",
    "            \"sentences_layout\": sent_items,\n",
    "            \"page_text\": page_text,\n",
    "        })\n",
    "\n",
    "    pages_tok.sort(key=lambda x: x[\"page_index\"])\n",
    "\n",
    "    TOK_DOCS.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": filename,\n",
    "        \"paths\": paths,\n",
    "        \"page_count_total\": page_count_total,\n",
    "        \"content\": content_type,\n",
    "        \"extraction\": extraction,\n",
    "        \"pages\": pages_tok,\n",
    "        \"chars_total\": doc_chars_total,\n",
    "        \"recompose_ok\": recompose_ok_doc,\n",
    "    })\n",
    "\n",
    "def _sort_key(x):\n",
    "    p = (x.get(\"paths\") or [\"\"])[0]\n",
    "    return (x.get(\"filename\") or \"\", str(p))\n",
    "\n",
    "TOK_DOCS.sort(key=_sort_key)\n",
    "\n",
    "TOK_BY_ID = {d[\"doc_id\"]: d for d in TOK_DOCS if d.get(\"doc_id\")}\n",
    "TOK_BY_FILENAME = {}\n",
    "for d in TOK_DOCS:\n",
    "    TOK_BY_FILENAME.setdefault(d[\"filename\"], []).append(d)\n",
    "\n",
    "def _select_doc(target):\n",
    "    if target is None:\n",
    "        return TOK_DOCS\n",
    "    if isinstance(target, int):\n",
    "        if 0 <= target < len(TOK_DOCS):\n",
    "            return [TOK_DOCS[target]]\n",
    "        raise IndexError(f\"TARGET index out of range: {target} (0..{len(TOK_DOCS)-1})\")\n",
    "    if isinstance(target, str):\n",
    "        t = target.strip()\n",
    "        if t in TOK_BY_ID:\n",
    "            return [TOK_BY_ID[t]]\n",
    "        if t in TOK_BY_FILENAME:\n",
    "            return TOK_BY_FILENAME[t]\n",
    "        hits = []\n",
    "        for d in TOK_DOCS:\n",
    "            if t.lower() in (d.get(\"filename\",\"\").lower()):\n",
    "                hits.append(d)\n",
    "                continue\n",
    "            for p in d.get(\"paths\") or []:\n",
    "                if t.lower() in str(p).lower():\n",
    "                    hits.append(d)\n",
    "                    break\n",
    "        if hits:\n",
    "            return hits\n",
    "        raise ValueError(f\"No document matches TARGET='{target}' (by doc_id/filename/path).\")\n",
    "    raise TypeError(\"TARGET must be None, int, or str\")\n",
    "\n",
    "def print_one_doc(doc):\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"[doc] {doc['filename']}\")\n",
    "    print(f\"  doc_id       : {doc.get('doc_id')}\")\n",
    "    print(f\"  content      : {doc.get('content')}\")\n",
    "    print(f\"  extraction   : {doc.get('extraction')}\")\n",
    "    print(f\"  pages_total  : {doc.get('page_count_total')}\")\n",
    "    print(f\"  chars_total  : {doc.get('chars_total')}\")\n",
    "    print(f\"  recompose_ok : {doc.get('recompose_ok')}\")\n",
    "    print(\"  paths:\")\n",
    "    if doc.get(\"paths\"):\n",
    "        for p in doc[\"paths\"]:\n",
    "            print(f\"    - {p}\")\n",
    "    else:\n",
    "        print(\"    - (unknown)\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    if not PRINT_SENTENCES:\n",
    "        return\n",
    "\n",
    "    for pg in (doc.get(\"pages\") or []):\n",
    "        print(f\"[page {pg['page_index']}/{doc.get('page_count_total') or '?'}] source_path={pg.get('source_path')}\")\n",
    "        print(f\"  lang         : {pg.get('lang')}\")\n",
    "        print(f\"  chars        : {pg.get('chars')}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        if PRINT_PAGE_TEXT:\n",
    "            print(pg.get(\"page_text\") or \"\")\n",
    "            print(\"-\" * 120)\n",
    "\n",
    "        sent_items = pg.get(\"sentences_layout\") or []\n",
    "\n",
    "        total_all = len(sent_items)\n",
    "        total_noise = sum(1 for s in sent_items if s.get(\"is_noise\"))\n",
    "        total_sentence = sum(1 for s in sent_items if s.get(\"is_sentence\"))\n",
    "\n",
    "        if PRINT_ONLY_SENTENCES:\n",
    "            view = [s for s in sent_items if s.get(\"is_sentence\")]\n",
    "        else:\n",
    "            view = list(sent_items)\n",
    "\n",
    "        fallback_used = False\n",
    "        if PRINT_ONLY_SENTENCES and not view and total_all > 0:\n",
    "            view = list(sent_items)\n",
    "            fallback_used = True\n",
    "\n",
    "        total_view = len(view)\n",
    "        show = total_view if MAX_SENTENCES_PREVIEW is None else min(total_view, MAX_SENTENCES_PREVIEW)\n",
    "\n",
    "        print(\n",
    "            f\"  sentences_layout: {total_all} chunks total | \"\n",
    "            f\"sentences={total_sentence} | noise={total_noise} | \"\n",
    "            f\"showing {show}/{total_view} \"\n",
    "            f\"(filter_is_sentence={PRINT_ONLY_SENTENCES}, fallback={fallback_used}, min_nonspace={MIN_SENTENCE_NONSPACE})\"\n",
    "        )\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "        for i2 in range(show):\n",
    "            s = view[i2]\n",
    "            chunk = s[\"text\"]\n",
    "            print(\n",
    "                f\"[sent {i2+1}/{total_view}] page={pg['page_index']} start={s['start']} end={s['end']} \"\n",
    "                f\"line={s['line']} col={s['col']} chars={s['chars']} nonspace={s['nonspace']} \"\n",
    "                f\"is_noise={s.get('is_noise')} is_sentence={s['is_sentence']} layout={s.get('layout_kind')}\"\n",
    "            )\n",
    "            print(chunk, end=\"\" if chunk.endswith(\"\\n\") else \"\\n\")\n",
    "            if PRINT_REPR:\n",
    "                print(\"repr:\", repr(chunk))\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        if MAX_SENTENCES_PREVIEW is not None and total_view > show:\n",
    "            print(f\"... {total_view - show} chunks restants non affichés (MAX_SENTENCES_PREVIEW={MAX_SENTENCES_PREVIEW})\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# ==================== Exécution ====================\n",
    "selected = _select_doc(TARGET)\n",
    "\n",
    "if not selected:\n",
    "    print(\"[info] Aucun document à traiter.\")\n",
    "else:\n",
    "    for doc in selected:\n",
    "        print_one_doc(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b2b4a",
   "metadata": {},
   "source": [
    "### attribue une catégorie grammaticale // jeu d’étiquettes NN, NNS, VB, VBD, JJ ...///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10970649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python kernel: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps dir: (absent)\n",
      "Python: C:\\Users\\moura\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "Local deps: (absent)\n",
      "Type an Arabic sentence (empty line to stop). You can paste ONE sentence or MANY quoted sentences.\n",
      "\n",
      "========================================================================================================================\n",
      "RUN EN (engcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "========================================================================================================================\n",
      "RUN FR (frcode.py)\n",
      "========================================================================================================================\n",
      "\n",
      "##########################################################################################\n",
      "DOC=testword.docx | page=1 | sent=0 | lang=fr\n",
      "##########################################################################################\n",
      "==========================================================================================\n",
      "INPUT: Équipée d'un moteur V10 de 620 chevaux, l'Audi R8 passe de 0 à 100 km/h en moins de 3,5 secondes, affirmant ainsi sa place de supercar emblématique de la marque aux anneaux.\n",
      "     Équipée  NNP     lemma=équipée\n",
      "          d'  IN      lemma=de\n",
      "          un  DT      lemma=un\n",
      "      moteur  NN      lemma=moteur\n",
      "           V  NN      lemma=V\n",
      "           1  CD      lemma=1\n",
      "           0  CD      lemma=0\n",
      "          de  IN      lemma=de\n",
      "         620  CD      lemma=620\n",
      "     chevaux  JJ      lemma=cheval\n",
      "           ,  PUNCT   lemma=∅\n",
      "          l'  DT      lemma=le\n",
      "        Audi  NNP     lemma=Audi\n",
      "           R  NN      lemma=r\n",
      "           8  CD      lemma=8\n",
      "       passe  NN      lemma=passe\n",
      "          de  IN      lemma=de\n",
      "           0  CD      lemma=0\n",
      "           à  IN      lemma=à\n",
      "         100  CD      lemma=100\n",
      "        km/h  NNP     lemma=km/h\n",
      "          en  IN      lemma=en\n",
      "       moins  RB      lemma=moins\n",
      "          de  IN      lemma=de\n",
      "         3,5  CD      lemma=3,5\n",
      "    secondes  NN      lemma=seconder\n",
      "           ,  PUNCT   lemma=∅\n",
      "   affirmant  JJ      lemma=affirmer\n",
      "       ainsi  RB      lemma=ainsi\n",
      "          sa  DT      lemma=son\n",
      "       place  NN      lemma=place\n",
      "          de  IN      lemma=de\n",
      "    supercar  NN      lemma=supercar\n",
      "emblématique  JJ      lemma=emblématique\n",
      "          de  IN      lemma=de\n",
      "          la  DT      lemma=le\n",
      "      marque  NN      lemma=marque\n",
      "         aux  DT      lemma=aux\n",
      "     anneaux  JJ      lemma=anneau\n",
      "           .  PUNCT   lemma=∅\n",
      "\n",
      "NER (fallback (règles)) (token, label):\n",
      "[('Équipée', 'O'), (\"d'\", 'O'), ('un', 'O'), ('moteur', 'O'), ('V', 'O'), ('1', 'O'), ('0', 'O'), ('de', 'O'), ('620', 'O'), ('chevaux', 'O'), (',', 'O'), (\"l'\", 'O'), ('Audi', 'O'), ('R', 'O'), ('8', 'O'), ('passe', 'O'), ('de', 'O'), ('0', 'O'), ('à', 'O'), ('100', 'O'), ('km/h', 'O'), ('en', 'O'), ('moins', 'O'), ('de', 'O'), ('3,5', 'O'), ('secondes', 'O'), (',', 'O'), ('affirmant', 'O'), ('ainsi', 'O'), ('sa', 'O'), ('place', 'O'), ('de', 'O'), ('supercar', 'O'), ('emblématique', 'O'), ('de', 'O'), ('la', 'O'), ('marque', 'O'), ('aux', 'O'), ('anneaux', 'O'), ('.', 'O')]\n",
      "Entities:\n",
      "\n",
      "========================================================================================================================\n",
      "RUN AR (arabcode.py)\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Chemin vers tes .py\n",
    "# =========================\n",
    "import sys, types, re, importlib\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\"  # dossier qui contient engcode.py / frcode.py / arabcode.py\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# =========================\n",
    "# 2) Petit \"nb_utils\" en mémoire (pas besoin de créer nb_utils.py)\n",
    "#    -> utilisé par run_from_previous_cell() dans tes scripts\n",
    "# =========================\n",
    "nb_utils = types.ModuleType(\"nb_utils\")\n",
    "\n",
    "_AR_RE = re.compile(r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", flags=re.UNICODE)\n",
    "_FR_HINT = {\"le\",\"la\",\"les\",\"des\",\"une\",\"un\",\"est\",\"avec\",\"pour\",\"dans\",\"sur\",\"facture\",\"date\",\"total\",\"tva\",\"montant\"}\n",
    "_EN_HINT = {\"the\",\"and\",\"to\",\"of\",\"in\",\"is\",\"for\",\"with\",\"invoice\",\"date\",\"total\",\"vat\",\"amount\"}\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if _AR_RE.search(t):\n",
    "        return \"ar\"\n",
    "    words = [w.lower() for w in _WORD_RE.findall(t[:8000])]\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    fr_score = sum(1 for w in words if w in _FR_HINT)\n",
    "    en_score = sum(1 for w in words if w in _EN_HINT)\n",
    "    if re.search(r\"[éèêàùçôîï]\", t.lower()):\n",
    "        fr_score += 1\n",
    "    return \"fr\" if fr_score >= en_score else \"en\"\n",
    "\n",
    "def get_previous_cell_input():\n",
    "    g = globals()\n",
    "    for k in (\"selected\", \"TOK_DOCS\", \"FINAL_DOCS\", \"DOCS\", \"TEXT_DOCS\", \"_\"):\n",
    "        if k in g and g[k] is not None:\n",
    "            return g[k]\n",
    "    return None\n",
    "\n",
    "def iter_sentences_from_input(data):\n",
    "    \"\"\"\n",
    "    Yield: (doc_name, page_idx, sent_idx, sent_text)\n",
    "    Supporte: TOK_DOCS/selected (pages->sentences_layout), FINAL_DOCS (list[{text}]), etc.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    # Cas 1: liste de docs avec pages (TOK_DOCS / selected)\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"pages\" in data[0]:\n",
    "        for d_i, doc in enumerate(data):\n",
    "            doc_name = doc.get(\"filename\") or doc.get(\"doc_id\") or f\"doc#{d_i}\"\n",
    "            pages = doc.get(\"pages\") or []\n",
    "            for p_i, pg in enumerate(pages):\n",
    "                page_idx = pg.get(\"page_index\", pg.get(\"page\", p_i+1))\n",
    "                sent_items = pg.get(\"sentences_layout\") or pg.get(\"sentences\") or pg.get(\"chunks\") or []\n",
    "                for s_i, s in enumerate(sent_items):\n",
    "                    if isinstance(s, dict):\n",
    "                        if s.get(\"is_sentence\") is False:\n",
    "                            continue\n",
    "                        sent = s.get(\"text\") or \"\"\n",
    "                    else:\n",
    "                        sent = str(s)\n",
    "                    yield doc_name, page_idx, s_i, sent\n",
    "        return\n",
    "\n",
    "    # Cas 2: FINAL_DOCS : list[{text, filename?}]\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"text\" in data[0]:\n",
    "        for i, d in enumerate(data):\n",
    "            doc_name = d.get(\"filename\") or d.get(\"doc_id\") or f\"doc#{i}\"\n",
    "            yield doc_name, None, None, d.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 3: dict {text:...}\n",
    "    if isinstance(data, dict) and \"text\" in data:\n",
    "        doc_name = data.get(\"filename\") or data.get(\"doc_id\") or \"doc\"\n",
    "        yield doc_name, None, None, data.get(\"text\") or \"\"\n",
    "        return\n",
    "\n",
    "    # Cas 4: string direct\n",
    "    if isinstance(data, str):\n",
    "        yield \"text\", None, None, data\n",
    "        return\n",
    "\n",
    "    raise TypeError(f\"Format d'entrée non supporté: {type(data)}\")\n",
    "\n",
    "nb_utils.detect_lang = detect_lang\n",
    "nb_utils.get_previous_cell_input = get_previous_cell_input\n",
    "nb_utils.iter_sentences_from_input = iter_sentences_from_input\n",
    "sys.modules[\"nb_utils\"] = nb_utils  # rend \"import nb_utils\" possible\n",
    "\n",
    "# =========================\n",
    "# 3) Import + reload tes 3 modules\n",
    "# =========================\n",
    "import engcode\n",
    "import frcode\n",
    "\n",
    "# arabcode peut échouer si camel_tools n'est pas installé => on skip proprement\n",
    "try:\n",
    "    import arabcode\n",
    "    HAVE_AR = True\n",
    "except Exception as e:\n",
    "    HAVE_AR = False\n",
    "    print(\"[warn] arabcode.py non chargé (dépendances manquantes ?). Détail:\", e)\n",
    "\n",
    "importlib.reload(engcode)\n",
    "importlib.reload(frcode)\n",
    "if HAVE_AR:\n",
    "    importlib.reload(arabcode)\n",
    "\n",
    "# =========================\n",
    "# 4) Exécution: chaque script filtre sa langue et print son output\n",
    "# =========================\n",
    "data = get_previous_cell_input()\n",
    "if data is None:\n",
    "    raise RuntimeError(\"Je ne trouve pas de données d'entrée. Assure-toi que la cellule précédente crée 'selected' (ou FINAL_DOCS / TOK_DOCS).\")\n",
    "\n",
    "MAX_SENTENCES_PER_LANG = None  # ex: 30 pour debug, ou None pour tout\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN EN (engcode.py)\")\n",
    "print(\"=\"*120)\n",
    "engcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RUN FR (frcode.py)\")\n",
    "print(\"=\"*120)\n",
    "frcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n",
    "\n",
    "if HAVE_AR:\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"RUN AR (arabcode.py)\")\n",
    "    print(\"=\"*120)\n",
    "    arabcode.run_from_previous_cell(data=data, max_sentences=MAX_SENTENCES_PER_LANG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e083280",
   "metadata": {},
   "source": [
    "### topic extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6493d2",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59526b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[classification] arab.docx -> best=UNCLASSIFIED | status=REVIEW | scores: {'ARTICLE': 0, 'BON_DE_COMMANDE': 0, 'CONTRAT': 0, 'FACTURE': 0, 'FORMULAIRE': 2}\n",
      "[classification] englais.docx -> best=UNCLASSIFIED | status=REVIEW | scores: {'ARTICLE': 0, 'BON_DE_COMMANDE': 0, 'CONTRAT': 0, 'FACTURE': 0, 'FORMULAIRE': 0}\n",
      "[classification] francais.docx -> best=UNCLASSIFIED | status=REVIEW | scores: {'ARTICLE': 2, 'BON_DE_COMMANDE': 0, 'CONTRAT': 1, 'FACTURE': 0, 'FORMULAIRE': 2}\n"
     ]
    }
   ],
   "source": [
    "# Affiche :\n",
    "# [classification] <filename> -> best=<DOC_TYPE> | status=<OK/REVIEW> | scores: {...}\n",
    "\n",
    "import sys, json, re, unicodedata, uuid, ast\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE_DIR = r\"C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\"  # où sont tes fichiers + dossier \"classification\"\n",
    "CLASSIFICATION_DIR = (Path(BASE_DIR) / \"classification\") if (Path(BASE_DIR) / \"classification\").exists() else Path(\"classification\")\n",
    "COMMON_PATH = CLASSIFICATION_DIR / \"common.json\"\n",
    "\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s or \"\")\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = _strip_accents(s)\n",
    "    return \" \".join((s or \"\").upper().split())\n",
    "\n",
    "def _ensure_kw_dict(d: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    kw = d.get(\"keywords\")\n",
    "    if isinstance(kw, dict):\n",
    "        out = {\"strong\": [], \"medium\": [], \"weak\": [], \"negative\": [], \"strong_negative\": []}\n",
    "        for k, v in kw.items():\n",
    "            if isinstance(v, list):\n",
    "                kk = str(k).lower()\n",
    "                if kk in out:\n",
    "                    out[kk] = [str(x).upper() for x in v]\n",
    "        return out\n",
    "    if isinstance(kw, list):\n",
    "        return {\"strong\": [], \"medium\": [], \"weak\": [str(x).upper() for x in kw], \"negative\": [], \"strong_negative\": []}\n",
    "    if isinstance(d, dict) and all(isinstance(v, list) for v in d.values()):\n",
    "        flat = []\n",
    "        for v in d.values():\n",
    "            flat.extend(v)\n",
    "        return {\"strong\": [], \"medium\": [], \"weak\": [str(x).upper() for x in flat], \"negative\": [], \"strong_negative\": []}\n",
    "    return {\"strong\": [], \"medium\": [], \"weak\": [], \"negative\": [], \"strong_negative\": []}\n",
    "\n",
    "def load_classification_configs():\n",
    "    common = {\n",
    "        \"weights\": {\"strong\": 5, \"medium\": 2, \"weak\": 1},\n",
    "        \"global_penalties\": {\"negative\": -2, \"strong_negative\": -5},\n",
    "        \"threshold\": 6,\n",
    "        \"margin\": 3,\n",
    "        \"tie_breaker\": \"priority\",\n",
    "    }\n",
    "    if COMMON_PATH.exists():\n",
    "        d = _load_json(COMMON_PATH)\n",
    "        if isinstance(d, dict):\n",
    "            common.update(d)\n",
    "\n",
    "    configs = {}\n",
    "    if CLASSIFICATION_DIR.exists():\n",
    "        for p in sorted(CLASSIFICATION_DIR.glob(\"*.json\")):\n",
    "            d = _load_json(p)\n",
    "            if not isinstance(d, dict):\n",
    "                continue\n",
    "            doc_type = str(d.get(\"doc_type\") or p.stem).upper()\n",
    "            if doc_type == \"COMMON\":\n",
    "                continue\n",
    "            configs[doc_type] = {\n",
    "                \"doc_type\": doc_type,\n",
    "                \"keywords\": _ensure_kw_dict(d),\n",
    "                \"priority\": int(d.get(\"priority\", 0) or 0),\n",
    "            }\n",
    "    return common, configs\n",
    "\n",
    "def _get_previous_cell_input():\n",
    "    g = globals()\n",
    "    for k in (\"selected\", \"TOK_DOCS\", \"FINAL_DOCS\", \"DOCS\", \"TEXT_DOCS\", \"_\"):\n",
    "        if k in g and g[k] is not None:\n",
    "            return g[k]\n",
    "    return None\n",
    "\n",
    "def _build_DOCS_from_input(data) -> List[Dict[str, Any]]:\n",
    "    # Cas: list docs avec pages (TOK_DOCS/selected)\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"pages\" in data[0]:\n",
    "        out = []\n",
    "        for i, doc in enumerate(data):\n",
    "            name = doc.get(\"filename\") or doc.get(\"doc_id\") or f\"doc#{i}\"\n",
    "            pages_out = []\n",
    "            for p_i, pg in enumerate(doc.get(\"pages\") or []):\n",
    "                page_index = pg.get(\"page_index\", pg.get(\"page\", p_i + 1))\n",
    "                txt = pg.get(\"ocr_text\")\n",
    "                if not txt:\n",
    "                    sent_items = pg.get(\"sentences_layout\") or pg.get(\"sentences\") or pg.get(\"chunks\") or []\n",
    "                    parts = []\n",
    "                    for s in sent_items:\n",
    "                        if isinstance(s, dict):\n",
    "                            if s.get(\"is_sentence\") is False:\n",
    "                                continue\n",
    "                            parts.append(s.get(\"text\") or \"\")\n",
    "                        else:\n",
    "                            parts.append(str(s))\n",
    "                    txt = \"\\n\".join([x for x in parts if x])\n",
    "                pages_out.append({\"page_index\": page_index, \"ocr_text\": txt or \"\"})\n",
    "            out.append({\"filename\": name, \"pages\": pages_out})\n",
    "        return out\n",
    "\n",
    "    # Cas: FINAL_DOCS list[{text,...}]\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"text\" in data[0]:\n",
    "        out = []\n",
    "        for i, d in enumerate(data):\n",
    "            name = d.get(\"filename\") or d.get(\"doc_id\") or f\"doc#{i}\"\n",
    "            out.append({\"filename\": name, \"pages\": [{\"page_index\": 1, \"ocr_text\": d.get(\"text\") or \"\"}]})\n",
    "        return out\n",
    "\n",
    "    # Cas: dict {text:...}\n",
    "    if isinstance(data, dict) and \"text\" in data:\n",
    "        name = data.get(\"filename\") or data.get(\"doc_id\") or \"doc\"\n",
    "        return [{\"filename\": name, \"pages\": [{\"page_index\": 1, \"ocr_text\": data.get(\"text\") or \"\"}]}]\n",
    "\n",
    "    # Cas: string\n",
    "    if isinstance(data, str):\n",
    "        return [{\"filename\": \"text\", \"pages\": [{\"page_index\": 1, \"ocr_text\": data}]}]\n",
    "\n",
    "    raise TypeError(f\"Format d'entrée non supporté: {type(data)}\")\n",
    "\n",
    "def classify_scores(DOCS: List[Dict[str, Any]], common: Dict[str, Any], configs: Dict[str, Any]) -> None:\n",
    "    weights = common.get(\"weights\", {\"strong\": 5, \"medium\": 2, \"weak\": 1})\n",
    "    penalties = common.get(\"global_penalties\", {\"negative\": -2, \"strong_negative\": -5})\n",
    "\n",
    "    def add_score(text: str, keywords: List[str], delta: int) -> int:\n",
    "        if not keywords:\n",
    "            return 0\n",
    "        s = 0\n",
    "        for k in keywords:\n",
    "            k = str(k).upper()\n",
    "            if k and k in text:\n",
    "                s += delta\n",
    "        return s\n",
    "\n",
    "    for doc in DOCS:\n",
    "        scores_doc = {dt: 0 for dt in configs.keys()}\n",
    "        for page in doc.get(\"pages\", []):\n",
    "            text = _norm_text(page.get(\"ocr_text\", \"\"))\n",
    "            for dt, cfg in configs.items():\n",
    "                kw = cfg[\"keywords\"]\n",
    "                score = 0\n",
    "                score += add_score(text, kw.get(\"strong\", []), int(weights.get(\"strong\", 5)))\n",
    "                score += add_score(text, kw.get(\"medium\", []), int(weights.get(\"medium\", 2)))\n",
    "                score += add_score(text, kw.get(\"weak\", []), int(weights.get(\"weak\", 1)))\n",
    "                score += add_score(text, kw.get(\"negative\", []), int(penalties.get(\"negative\", -2)))\n",
    "                score += add_score(text, kw.get(\"strong_negative\", []), int(penalties.get(\"strong_negative\", -5)))\n",
    "                scores_doc[dt] += score\n",
    "        doc[\"scores\"] = scores_doc\n",
    "\n",
    "# ========= DECISION =========\n",
    "def decide(scores: Dict[str, int], configs: Dict[str, Any], common: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    THRESHOLD = int(common.get(\"threshold\", 6))\n",
    "    MARGIN = int(common.get(\"margin\", 3))\n",
    "\n",
    "    PRIORITY = {dt: int((configs.get(dt, {}) or {}).get(\"priority\", 0) or 0) for dt in configs.keys()}\n",
    "    scores_stable = {dt: int(scores.get(dt, 0)) for dt in configs.keys()}\n",
    "\n",
    "    # tri stable: score desc, priority desc, name asc\n",
    "    items = sorted(\n",
    "        scores_stable.items(),\n",
    "        key=lambda kv: (-kv[1], -PRIORITY.get(kv[0], 0), kv[0])\n",
    "    )\n",
    "    top_type, top_score = items[0] if items else (\"UNCLASSIFIED\", 0)\n",
    "    second_score = items[1][1] if len(items) > 1 else 0\n",
    "    diff = top_score - second_score\n",
    "\n",
    "    confident = (top_score > 0) and (top_score >= THRESHOLD) and (diff >= MARGIN)\n",
    "    best = top_type if confident else \"UNCLASSIFIED\"\n",
    "    status = \"OK\" if confident else \"REVIEW\"\n",
    "\n",
    "    return {\n",
    "        \"best\": best,\n",
    "        \"status\": status,\n",
    "        \"top_score\": top_score,\n",
    "        \"second_score\": second_score,\n",
    "        \"diff\": diff,\n",
    "        \"scores_stable\": scores_stable\n",
    "    }\n",
    "\n",
    "# ========= RUN =========\n",
    "data = _get_previous_cell_input()\n",
    "if data is None:\n",
    "    raise RuntimeError(\"Je ne trouve pas de données d'entrée (selected / TOK_DOCS / FINAL_DOCS / DOCS / TEXT_DOCS).\")\n",
    "\n",
    "common, configs = load_classification_configs()\n",
    "if not configs:\n",
    "    raise RuntimeError(f\"Aucune classe trouvée dans: {CLASSIFICATION_DIR}\")\n",
    "\n",
    "DOCS = _build_DOCS_from_input(data)\n",
    "classify_scores(DOCS, common, configs)\n",
    "\n",
    "preferred = [\"ARTICLE\", \"BON_DE_COMMANDE\", \"CONTRAT\", \"FACTURE\", \"FORMULAIRE\"]\n",
    "order = [c for c in preferred if c in configs] + [c for c in configs.keys() if c not in preferred]\n",
    "\n",
    "RESULTS = []\n",
    "for doc in DOCS:\n",
    "    scores = doc.get(\"scores\", {}) or {}\n",
    "    ordered_scores = {k: int(scores.get(k, 0)) for k in order}\n",
    "\n",
    "    d = decide(scores, configs, common)\n",
    "    best, status = d[\"best\"], d[\"status\"]\n",
    "\n",
    "    # ---- sortie lisible + compacte (tu vois enfin la classe)\n",
    "    print(f\"[classification] {doc.get('filename')} -> best={best} | status={status} | scores: {ordered_scores}\")\n",
    "\n",
    "    # (optionnel) stocker le json détaillé sans l'imprimer\n",
    "    doc[\"result\"] = {\n",
    "        \"doc_id\": doc.get(\"doc_id\") or str(uuid.uuid4()),\n",
    "        \"filename\": doc.get(\"filename\"),\n",
    "        \"doc_type\": best,\n",
    "        \"status\": status,\n",
    "        \"scores\": d[\"scores_stable\"],\n",
    "        \"threshold\": int(common.get(\"threshold\", 6)),\n",
    "        \"margin\": int(common.get(\"margin\", 3)),\n",
    "        \"decision_debug\": {\n",
    "            \"top_score\": d[\"top_score\"],\n",
    "            \"second_score\": d[\"second_score\"],\n",
    "            \"diff\": d[\"diff\"],\n",
    "        }\n",
    "    }\n",
    "    RESULTS.append(doc[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2391d",
   "metadata": {},
   "source": [
    "## Produire une sortie JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87474e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
