{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b6c389",
   "metadata": {},
   "source": [
    "## Pipeline OCR (Tesseract + OpenCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d3df9",
   "metadata": {},
   "source": [
    "Langue (par défaut FR, mais bascule en EN si détecté)\n",
    "\n",
    "Par défaut, l’OCR est en français :\n",
    "\n",
    "DEFAULT_LANG = \"fra\" (côté Tesseract)\n",
    "\n",
    "spacy.load(\"fr_core_news_sm\", ...) (côté spaCy)\n",
    "\n",
    "Si tu détectes que le texte est en anglais, tu fais basculer :\n",
    "\n",
    "DEFAULT_LANG = \"eng\" (ou fra+eng si tu veux tolérer les deux)\n",
    "\n",
    "spacy.load(\"en_core_web_sm\", ...)\n",
    "\n",
    "Trucs à modifier quand tu changes de langue :\n",
    "\n",
    "la constante DEFAULT_LANG\n",
    "\n",
    "le modèle spaCy chargé (fr_core_news_sm ↔ en_core_web_sm)\n",
    "\n",
    "---\n",
    "\n",
    "Fonctionnement global du script\n",
    "\n",
    "Prendre une image (INPUT_FILE)\n",
    "\n",
    "L’améliorer via le prétraitement (gris, upscale, contraste/sharpness, seuil, etc.)\n",
    "\n",
    "Lancer Tesseract sur l’image prétraitée pour extraire le texte (OCR_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1b1ed",
   "metadata": {},
   "source": [
    "### importation img et prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b767eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "INPUT_FILE: Optional[str] = \"image2tab.webp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c916ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] Using INPUT_FILE=C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\image2tab.webp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies:\n",
    "  * Python 3.8+\n",
    "  * pytesseract\n",
    "  * pillow\n",
    "  * Tesseract binary with tessdata\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    np = None\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In notebooks __file__ is undefined; fall back to current working directory.\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_LANG = \"fra\"\n",
    "DEFAULT_CONTRAST = 1.5\n",
    "DEFAULT_SHARPNESS = 1.2\n",
    "DEFAULT_BRIGHTNESS = 1.0\n",
    "DEFAULT_UPSCALE = 1.5\n",
    "DEFAULT_DPI = 300\n",
    "\n",
    " #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "SHOW_PREPROCESSED = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhanceOptions:\n",
    "    contrast: float = DEFAULT_CONTRAST\n",
    "    sharpness: float = DEFAULT_SHARPNESS\n",
    "    brightness: float = DEFAULT_BRIGHTNESS\n",
    "    upscale: float = DEFAULT_UPSCALE\n",
    "    gamma: Optional[float] = None  # gamma correction; <1 brightens darks, >1 darkens\n",
    "    pad: int = 0  # pixels to pad around the image\n",
    "    median: Optional[int] = None  # kernel size for median filter (odd int, e.g., 3)\n",
    "    unsharp_radius: Optional[float] = None  # e.g., 1.0\n",
    "    unsharp_percent: int = 150\n",
    "    invert: bool = False\n",
    "    autocontrast_cutoff: Optional[int] = None  # 0-100; percentage to clip for autocontrast\n",
    "    equalize: bool = False  # histogram equalization\n",
    "    auto_rotate: bool = False  # attempt orientation detection + rotate\n",
    "    otsu: bool = False  # auto-threshold with Otsu (requires numpy)\n",
    "    threshold: Optional[int] = None  # 0-255; if set, applies a binary threshold\n",
    "\n",
    "\n",
    "def build_config(\n",
    "    oem: Optional[int],\n",
    "    psm: Optional[int],\n",
    "    base_flags: Iterable[str],\n",
    "    dpi: Optional[int],\n",
    "    tessdata_dir: Optional[Path],\n",
    "    user_words: Optional[Path],\n",
    "    user_patterns: Optional[Path],\n",
    ") -> str:\n",
    "    parts: List[str] = []\n",
    "    if oem is not None:\n",
    "        parts.append(f\"--oem {oem}\")\n",
    "    if psm is not None:\n",
    "        parts.append(f\"--psm {psm}\")\n",
    "    if dpi is not None:\n",
    "        parts.append(f\"--dpi {dpi}\")\n",
    "    if tessdata_dir is not None:\n",
    "        parts.append(f'--tessdata-dir \"{tessdata_dir}\"')\n",
    "    if user_words is not None:\n",
    "        parts.append(f'--user-words \"{user_words}\"')\n",
    "    if user_patterns is not None:\n",
    "        parts.append(f'--user-patterns \"{user_patterns}\"')\n",
    "    parts.extend(base_flags)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def ensure_environment(lang: str) -> None:\n",
    "    try:\n",
    "        _ = pytesseract.get_tesseract_version()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        sys.exit(\"Tesseract binary not found on PATH. Install it and its language data.\")\n",
    "    if lang:\n",
    "        try:\n",
    "            available = set(pytesseract.get_languages(config=\"\"))\n",
    "            requested = set(lang.split(\"+\"))\n",
    "            missing = requested - available\n",
    "            if missing:\n",
    "                print(\n",
    "                    f\"Warning: missing languages: {', '.join(sorted(missing))}. \"\n",
    "                    f\"Available: {', '.join(sorted(available))}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "        except pytesseract.TesseractError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def auto_rotate_if_needed(img: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    if not enhance.auto_rotate:\n",
    "        return img\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        angle = None\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                try:\n",
    "                    angle = int(line.split(\":\")[1].strip())\n",
    "                except ValueError:\n",
    "                    angle = None\n",
    "                break\n",
    "        if angle is not None and angle % 360 != 0:\n",
    "            return img.rotate(-angle, expand=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    img = image.convert(\"L\")\n",
    "    img = auto_rotate_if_needed(img, enhance)\n",
    "\n",
    "    if enhance.invert:\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    if enhance.pad and enhance.pad > 0:\n",
    "        img = ImageOps.expand(img, border=enhance.pad, fill=255)\n",
    "\n",
    "    if enhance.autocontrast_cutoff is not None:\n",
    "        cutoff = max(0, min(100, enhance.autocontrast_cutoff))\n",
    "        img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "\n",
    "    if enhance.equalize:\n",
    "        img = ImageOps.equalize(img)\n",
    "\n",
    "    if enhance.upscale and enhance.upscale != 1.0:\n",
    "        w, h = img.size\n",
    "        img = img.resize((int(w * enhance.upscale), int(h * enhance.upscale)), Image.LANCZOS)\n",
    "\n",
    "    if enhance.gamma and enhance.gamma > 0:\n",
    "        inv_gamma = 1.0 / enhance.gamma\n",
    "        lut = [pow(x / 255.0, inv_gamma) * 255 for x in range(256)]\n",
    "        img = img.point(lut)\n",
    "\n",
    "    if enhance.brightness and enhance.brightness != 1.0:\n",
    "        img = ImageEnhance.Brightness(img).enhance(enhance.brightness)\n",
    "\n",
    "    if enhance.contrast and enhance.contrast != 1.0:\n",
    "        img = ImageEnhance.Contrast(img).enhance(enhance.contrast)\n",
    "\n",
    "    if enhance.sharpness and enhance.sharpness != 1.0:\n",
    "        img = ImageEnhance.Sharpness(img).enhance(enhance.sharpness)\n",
    "\n",
    "    if enhance.unsharp_radius:\n",
    "        img = img.filter(\n",
    "            ImageFilter.UnsharpMask(\n",
    "                radius=enhance.unsharp_radius,\n",
    "                percent=enhance.unsharp_percent,\n",
    "                threshold=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if enhance.median and enhance.median > 1 and enhance.median % 2 == 1:\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=enhance.median))\n",
    "\n",
    "    if enhance.threshold is not None:\n",
    "        thr = max(0, min(255, enhance.threshold))\n",
    "        img = img.point(lambda p, t=thr: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "    elif enhance.otsu and np is not None:\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        hist, _ = np.histogram(arr, bins=256, range=(0, 256))\n",
    "        total = arr.size\n",
    "        sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "        sum_b = 0.0\n",
    "        w_b = 0.0\n",
    "        max_var = 0.0\n",
    "        threshold = 0\n",
    "\n",
    "        for i in range(256):\n",
    "            w_b += hist[i]\n",
    "            if w_b == 0:\n",
    "                continue\n",
    "            w_f = total - w_b\n",
    "            if w_f == 0:\n",
    "                break\n",
    "            sum_b += i * hist[i]\n",
    "            m_b = sum_b / w_b\n",
    "            m_f = (sum_total - sum_b) / w_f\n",
    "            var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "            if var_between > max_var:\n",
    "                max_var = var_between\n",
    "                threshold = i\n",
    "\n",
    "        img = img.point(lambda p, t=threshold: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-l\", \"--lang\", default=DEFAULT_LANG)\n",
    "    parser.add_argument(\"--oem\", type=int, choices=range(0, 4), default=None)\n",
    "    parser.add_argument(\"--psm\", type=int, choices=range(0, 14), default=None)\n",
    "    parser.add_argument(\"--dpi\", type=int, default=DEFAULT_DPI)\n",
    "    parser.add_argument(\"--tessdata-dir\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-words\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-patterns\", type=Path, default=None)\n",
    "    parser.add_argument(\"--whitelist\", type=str, default=None)\n",
    "    parser.add_argument(\"--blacklist\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--contrast\", type=float, default=DEFAULT_CONTRAST)\n",
    "    parser.add_argument(\"--sharpness\", type=float, default=DEFAULT_SHARPNESS)\n",
    "    parser.add_argument(\"--brightness\", type=float, default=DEFAULT_BRIGHTNESS)\n",
    "    parser.add_argument(\"--upscale\", type=float, default=DEFAULT_UPSCALE)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--pad\", type=int, default=0)\n",
    "    parser.add_argument(\"--threshold\", type=int, default=None)\n",
    "    parser.add_argument(\"--median\", type=int, default=None)\n",
    "    parser.add_argument(\"--unsharp-radius\", type=float, default=None)\n",
    "    parser.add_argument(\"--unsharp-percent\", type=int, default=150)\n",
    "    parser.add_argument(\"--invert\", action=\"store_true\")\n",
    "    parser.add_argument(\"--autocontrast-cutoff\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--auto-rotate\", action=\"store_true\")\n",
    "    parser.add_argument(\"--otsu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"*\",\n",
    "        default=[],\n",
    "        metavar=\"CFG\",\n",
    "        help=\"Additional configuration flags passed verbatim to tesseract (e.g., -c foo=bar).\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args(list(argv) if argv is not None else [])\n",
    "\n",
    "\n",
    "# --------- Exécution Cellule 1 (jusqu’à l’affichage) ---------\n",
    "\n",
    "args = parse_args()\n",
    "ensure_environment(args.lang)\n",
    "\n",
    "enhance = EnhanceOptions(\n",
    "    contrast=args.contrast,\n",
    "    sharpness=args.sharpness,\n",
    "    brightness=args.brightness,\n",
    "    upscale=args.upscale,\n",
    "    gamma=args.gamma,\n",
    "    pad=args.pad,\n",
    "    median=args.median,\n",
    "    unsharp_radius=args.unsharp_radius,\n",
    "    unsharp_percent=args.unsharp_percent,\n",
    "    invert=args.invert,\n",
    "    autocontrast_cutoff=args.autocontrast_cutoff,\n",
    "    equalize=args.equalize,\n",
    "    auto_rotate=args.auto_rotate,\n",
    "    otsu=args.otsu,\n",
    "    threshold=args.threshold,\n",
    ")\n",
    "\n",
    "config_flags: List[str] = list(args.config)\n",
    "if args.whitelist:\n",
    "    config_flags.append(f\"-c tessedit_char_whitelist={args.whitelist}\")\n",
    "if args.blacklist:\n",
    "    config_flags.append(f\"-c tessedit_char_blacklist={args.blacklist}\")\n",
    "\n",
    "if not INPUT_FILE:\n",
    "    sys.exit(\"INPUT_FILE is not set. Put your image filename in INPUT_FILE.\")\n",
    "\n",
    "path = Path(INPUT_FILE)\n",
    "if not path.is_absolute():\n",
    "    path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "if not path.exists():\n",
    "    sys.exit(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "original = Image.open(path)\n",
    "prepped = preprocess_image(original, enhance)\n",
    "\n",
    "# Afficher les 2 images (original + prétraitée)\n",
    "original.show(title=\"original\")\n",
    "if \"SHOW_PREPROCESSED\" not in globals() or SHOW_PREPROCESSED:\n",
    "    prepped.show(title=\"preprocessed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6eeae",
   "metadata": {},
   "source": [
    "### tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9199410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTURE\n",
      "\n",
      "CODE CLENT NUMERO\n",
      "FCo0o1 4/20/2016 0002\n",
      "Ma petite entreprise CLIENT\n",
      "19,rue de place 1° mai SARL EL HANA\n",
      "16000 Alger Centre IROUTE DE BEJAIA SETIF\n",
      "Tel : 00-00-52-12- 119000\n",
      "Ident Fiscal : 160\n",
      "N°art : 160100000000\n",
      "Mode de paiement : Espèce\n",
      "Date Échéance : 5/20/2016\n",
      "Référence Description Produit Quantité P.Unitaire Valeur\n",
      "cl001 _Produit1 1000 1.00 1,000.00\n",
      "c1002 _ |Produit 2 1001 2.00 2,002.00\n",
      "c1003 _ jProduit 3 1002 3.00 3,006.00\n",
      "c1004 _ |Produit4 1003 4.00 4,012.00\n",
      "c1005 __|Produit5 1004 5.00 5,020.00\n",
      "c1006 _ |Produit 6 1005 6.00 6,030.00\n",
      "c1007 _ |Produit 7 1006 11.00 11,066.00\n",
      "c1008 Produit8 1007 118.00 118,826.00\n",
      "c1009 Produit 9 1008 19.00 19,152.00\n",
      "c1010 _ |Produit 10 1009 10.00 10,090.00\n",
      "Non assujetti à latva [Montant à payer 180,204.00\n",
      "[rimbre 1,802.00\n",
      "Montant à payer ttc 182,006.00\n",
      "\n",
      "Monatnt Facture enLettre … Cinq mille huit cent quatre vingt huit Dinars Algériens\n",
      "\n",
      "Cachet & Signature\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = build_config(\n",
    "    args.oem,\n",
    "    args.psm,\n",
    "    config_flags,\n",
    "    args.dpi,\n",
    "    args.tessdata_dir,\n",
    "    args.user_words,\n",
    "    args.user_patterns,\n",
    ")\n",
    "\n",
    "OCR_TEXT = pytesseract.image_to_string(prepped, lang=args.lang, config=config)\n",
    "print(OCR_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d74de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c33836",
   "metadata": {},
   "source": [
    "### Pipeline SpaCy de base & Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1b445",
   "metadata": {},
   "source": [
    "modifer pour la langue :fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cbf1730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phrase : FACTURE\n",
      "\n",
      "CODE CLENT NUMERO\n",
      "FCo0o1 4/20/2016 0002\n",
      "Ma petite entreprise CLIENT\n",
      "19,rue de place 1° mai SARL EL HANA\n",
      "16000 Alger Centre IROUTE DE BEJAIA SETIF\n",
      "Tel : 00-00-52-12- 119000\n",
      "Ident Fiscal : 160\n",
      "N°art : 160100000000\n",
      "Mode de paiement : Espèce\n",
      "Date Échéance : 5/20/2016\n",
      "Référence Description Produit Quantité P.Unitaire Valeur\n",
      "cl001 _Produit1 1000 1.00 1,000.00\n",
      "c1002 _ |Produit 2 1001 2.00 2,002.00\n",
      "c1003 _ jProduit 3 1002 3.00 3,006.00\n",
      "c1004 _ |Produit4 1003 4.00 4,012.00\n",
      "c1005 __|Produit5 1004 5.00 5,020.00\n",
      "c1006 _ |Produit 6 1005 6.00 6,030.00\n",
      "c1007 _ |Produit 7 1006 11.00 11,066.00\n",
      "c1008 Produit8 1007 118.00 118,826.00\n",
      "c1009 Produit 9 1008 19.00 19,152.00\n",
      "c1010 _ |Produit 10 1009 10.00 10,090.00\n",
      "Non assujetti à latva [Montant à payer 180,204.00\n",
      "[rimbre 1,802.00\n",
      "Montant à payer ttc 182,006.00\n",
      "\n",
      "Monatnt Facture enLettre … Cinq mille huit cent quatre vingt huit Dinars Algériens\n",
      "\n",
      "Cachet & Signature\n",
      "Langue : fr\n",
      "Tokens : ['FACTURE', '\\n\\n', 'CODE', 'CLENT', 'NUMERO', '\\n', 'FCo0o1', '4/20/2016', '0002', '\\n', 'Ma', 'petite', 'entreprise', 'CLIENT', '\\n', '19,rue', 'de', 'place', '1', '°', 'mai', 'SARL', 'EL', 'HANA', '\\n', '16000', 'Alger', 'Centre', 'IROUTE', 'DE', 'BEJAIA', 'SETIF', '\\n', 'Tel', ':', '00', '-', '00', '-', '52', '-', '12-', '119000', '\\n', 'Ident', 'Fiscal', ':', '160', '\\n', 'N', '°', 'art', ':', '160100000000', '\\n', 'Mode', 'de', 'paiement', ':', 'Espèce', '\\n', 'Date', 'Échéance', ':', '5/20/2016', '\\n', 'Référence', 'Description', 'Produit', 'Quantité', 'P.Unitaire', 'Valeur', '\\n', 'cl001', '_', 'Produit1', '1000', '1.00', '1,000.00', '\\n', 'c1002', '_', '|Produit', '2', '1001', '2.00', '2,002.00', '\\n', 'c1003', '_', 'jProduit', '3', '1002', '3.00', '3,006.00', '\\n', 'c1004', '_', '|Produit4', '1003', '4.00', '4,012.00', '\\n', 'c1005', '_', '_', '|Produit5', '1004', '5.00', '5,020.00', '\\n', 'c1006', '_', '|Produit', '6', '1005', '6.00', '6,030.00', '\\n', 'c1007', '_', '|Produit', '7', '1006', '11.00', '11,066.00', '\\n', 'c1008', 'Produit8', '1007', '118.00', '118,826.00', '\\n', 'c1009', 'Produit', '9', '1008', '19.00', '19,152.00', '\\n', 'c1010', '_', '|Produit', '10', '1009', '10.00', '10,090.00', '\\n', 'Non', 'assujetti', 'à', 'latva', '[', 'Montant', 'à', 'payer', '180,204.00', '\\n', '[', 'rimbre', '1,802.00', '\\n', 'Montant', 'à', 'payer', 'ttc', '182,006.00', '\\n\\n', 'Monatnt', 'Facture', 'enLettre', '…', 'Cinq', 'mille', 'huit', 'cent', 'quatre', 'vingt', 'huit', 'Dinars', 'Algériens', '\\n\\n', 'Cachet', '&', 'Signature']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "texte = OCR_TEXT\n",
    "\n",
    "# 1) détecter la langue sur un gros extrait (plus stable et plus rapide)\n",
    "sample = texte[:2000]  \n",
    "try:\n",
    "    doc_lang = detect(sample)\n",
    "except:\n",
    "    doc_lang = \"fr\"  # si probleme avec detection de langue (on forcer fr)\n",
    " \n",
    "# 2) charger UN seul modèle\n",
    "nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "# 3) split phrases rapide\n",
    "sent_split = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "for phrase in sent_split.split(texte):\n",
    "    phrase = phrase.strip()\n",
    "    if len(phrase) < 20:\n",
    "        continue\n",
    "\n",
    "    # tokenisation spaCy (mais pipeline ultra léger)\n",
    "    doc = nlp.make_doc(phrase)\n",
    "    print(\"\\nPhrase :\", phrase)\n",
    "    print(\"Langue :\", doc_lang)\n",
    "    print(\"Tokens :\", [t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae6df1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38b5574",
   "metadata": {},
   "source": [
    "## Schéma de BDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da0aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"mmd-87e775658fb348b8a74d39591f97caae\" class=\"mermaid\">\n",
       "%%{init: {\"theme\": \"redux-dark-color\", \"layout\": \"elk\"} }%%\n",
       "erDiagram\n",
       "    ROLES {\n",
       "        INT id PK\n",
       "        VARCHAR name\n",
       "    }\n",
       "\n",
       "    USERS {\n",
       "        INT id PK\n",
       "        VARCHAR username\n",
       "        VARCHAR email\n",
       "        VARCHAR password_hash\n",
       "        INT role_id FK\n",
       "        DATETIME created_at\n",
       "    }\n",
       "\n",
       "    DOMAINS {\n",
       "        INT id PK\n",
       "        VARCHAR name\n",
       "    }\n",
       "\n",
       "    RULE_CONFIGS {\n",
       "        INT id PK\n",
       "        INT domain_id FK\n",
       "        VARCHAR version_label_Regex\n",
       "        JSONB Regex_json\n",
       "        INT created_by FK\n",
       "        DATETIME created_at\n",
       "        BOOLEAN is_active\n",
       "    }\n",
       "\n",
       "    %% Nouvelle table: \"API\" = profil/config par domaine (langue, règles, paramètres)\n",
       "    APIS {\n",
       "        INT id PK\n",
       "        INT domain_id FK\n",
       "        INT rule_config_id FK\n",
       "        VARCHAR name\n",
       "        VARCHAR language_code          \n",
       "        JSONB settings_json           \n",
       "        BOOLEAN is_active\n",
       "        INT created_by FK\n",
       "        DATETIME created_at\n",
       "    }\n",
       "\n",
       "    %% Clés par API (une API/profil peut avoir plusieurs clés d'accès)\n",
       "    API_KEYS {\n",
       "        INT id PK\n",
       "        INT api_id FK\n",
       "        VARCHAR key_hash              \n",
       "        JSONB scopes                  \n",
       "        DATETIME created_at\n",
       "        DATETIME last_used_at\n",
       "        DATETIME expires_at\n",
       "        DATETIME revoked_at\n",
       "    }\n",
       "\n",
       "    DOCUMENTS {\n",
       "        UUID id PK\n",
       "        VARCHAR filename\n",
       "        INT domain_id FK\n",
       "        VARCHAR status\n",
       "        VARCHAR empreinte_numerique\n",
       "        DATETIME uploaded_at\n",
       "        INT uploaded_by FK\n",
       "        INT api_id FK                 \n",
       "    }\n",
       "\n",
       "    FILE_STORAGE {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        VARCHAR object_path\n",
       "        INT size\n",
       "        VARCHAR empreinte_numerique\n",
       "        DATETIME stored_at\n",
       "    }\n",
       "\n",
       "    EXTRACTIONS {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        INT rule_config_id FK\n",
       "        VARCHAR field_name\n",
       "        TEXT extracted_value\n",
       "        JSONB coordinates\n",
       "        BOOLEAN is_valid\n",
       "        BOOLEAN is_overridden\n",
       "        INT overridden_by FK\n",
       "        DATETIME overridden_at\n",
       "    }\n",
       "\n",
       "    QUALITY_GATE_LOGS {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        BOOLEAN is_passed\n",
       "        TEXT failure_reason\n",
       "        DATETIME checked_at\n",
       "        VARCHAR check_origin          \n",
       "        INT checked_by FK            \n",
       "        BOOLEAN is_final_decision    \n",
       "        TEXT decision_comment\n",
       "    }\n",
       "\n",
       "    AUDIT_LOGS {\n",
       "        INT id PK\n",
       "        INT user_id FK\n",
       "        UUID document_id FK\n",
       "        VARCHAR action\n",
       "        VARCHAR entity_type\n",
       "        INT entity_id\n",
       "        JSONB changes\n",
       "        DATETIME timestamp\n",
       "        VARCHAR ip_address\n",
       "    }\n",
       "\n",
       "    %% Relations façon Workbench\n",
       "    ROLES ||--o{ USERS : \"role_id\"\n",
       "    USERS ||--o{ RULE_CONFIGS : \"created_by\"\n",
       "    USERS ||--o{ AUDIT_LOGS : \"user_id\"\n",
       "\n",
       "    DOMAINS ||--o{ RULE_CONFIGS : \"domain_id\"\n",
       "\n",
       "    %% Domaine -> APIs (plusieurs APIs dans le même domaine)\n",
       "    DOMAINS ||--o{ APIS : \"domain_id\"\n",
       "    RULE_CONFIGS ||--o{ APIS : \"rule_config_id\"\n",
       "    USERS ||--o{ APIS : \"created_by\"\n",
       "\n",
       "    %% API -> API_KEYS (plusieurs clés par API)\n",
       "    APIS ||--o{ API_KEYS : \"api_id\"\n",
       "\n",
       "    %% Domaine -> Documents\n",
       "    DOMAINS ||--o{ DOCUMENTS : \"domain_id\"\n",
       "    USERS ||--o{ DOCUMENTS : \"uploaded_by\"\n",
       "    APIS ||--o{ DOCUMENTS : \"api_id\"\n",
       "\n",
       "    %% Documents -> stockage + extractions + quality\n",
       "    DOCUMENTS ||--|| FILE_STORAGE : \"document_id\"\n",
       "    DOCUMENTS ||--o{ EXTRACTIONS : \"document_id\"\n",
       "    RULE_CONFIGS ||--o{ EXTRACTIONS : \"rule_config_id\"\n",
       "    USERS ||--o{ EXTRACTIONS : \"overridden_by\"\n",
       "\n",
       "    DOCUMENTS ||--o{ QUALITY_GATE_LOGS : \"document_id\"\n",
       "    USERS ||--o{ QUALITY_GATE_LOGS : \"checked_by\"\n",
       "\n",
       "    DOCUMENTS ||--o{ AUDIT_LOGS : \"document_id\"\n",
       "</div>\n",
       "\n",
       "<script type=\"module\">\n",
       "  const render = async () => {\n",
       "    if (!window.__mermaid_loaded__) {\n",
       "      const mermaid = (await import(\"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\")).default;\n",
       "      window.mermaid = mermaid;\n",
       "      window.__mermaid_loaded__ = true;\n",
       "      mermaid.initialize({\n",
       "        startOnLoad: false,\n",
       "        securityLevel: \"loose\"\n",
       "      });\n",
       "    }\n",
       "    await window.mermaid.run({\n",
       "      nodes: [document.getElementById(\"mmd-87e775658fb348b8a74d39591f97caae\")]\n",
       "    });\n",
       "  };\n",
       "  render();\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re, json, uuid\n",
    "\n",
    "raw = r\"\"\"\n",
    "---\n",
    "config:\n",
    "  layout: elk\n",
    "  theme: redux-dark-color\n",
    "---\n",
    "\n",
    "erDiagram\n",
    "    ROLES {\n",
    "        INT id PK\n",
    "        VARCHAR name\n",
    "    }\n",
    "\n",
    "    USERS {\n",
    "        INT id PK\n",
    "        VARCHAR username\n",
    "        VARCHAR email\n",
    "        VARCHAR password_hash\n",
    "        INT role_id FK\n",
    "        DATETIME created_at\n",
    "    }\n",
    "\n",
    "    DOMAINS {\n",
    "        INT id PK\n",
    "        VARCHAR name\n",
    "    }\n",
    "\n",
    "    RULE_CONFIGS {\n",
    "        INT id PK\n",
    "        INT domain_id FK\n",
    "        VARCHAR version_label_Regex\n",
    "        JSONB Regex_json\n",
    "        INT created_by FK\n",
    "        DATETIME created_at\n",
    "        BOOLEAN is_active\n",
    "    }\n",
    "\n",
    "    %% Nouvelle table: \"API\" = profil/config par domaine (langue, règles, paramètres)\n",
    "    APIS {\n",
    "        INT id PK\n",
    "        INT domain_id FK\n",
    "        INT rule_config_id FK\n",
    "        VARCHAR name\n",
    "        VARCHAR language_code          \n",
    "        JSONB settings_json           \n",
    "        BOOLEAN is_active\n",
    "        INT created_by FK\n",
    "        DATETIME created_at\n",
    "    }\n",
    "\n",
    "    %% Clés par API (une API/profil peut avoir plusieurs clés d'accès)\n",
    "    API_KEYS {\n",
    "        INT id PK\n",
    "        INT api_id FK\n",
    "        VARCHAR key_hash              \n",
    "        JSONB scopes                  \n",
    "        DATETIME created_at\n",
    "        DATETIME last_used_at\n",
    "        DATETIME expires_at\n",
    "        DATETIME revoked_at\n",
    "    }\n",
    "\n",
    "    DOCUMENTS {\n",
    "        UUID id PK\n",
    "        VARCHAR filename\n",
    "        INT domain_id FK\n",
    "        VARCHAR status\n",
    "        VARCHAR empreinte_numerique\n",
    "        DATETIME uploaded_at\n",
    "        INT uploaded_by FK\n",
    "        INT api_id FK                 \n",
    "    }\n",
    "\n",
    "    FILE_STORAGE {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        VARCHAR object_path\n",
    "        INT size\n",
    "        VARCHAR empreinte_numerique\n",
    "        DATETIME stored_at\n",
    "    }\n",
    "\n",
    "    EXTRACTIONS {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        INT rule_config_id FK\n",
    "        VARCHAR field_name\n",
    "        TEXT extracted_value\n",
    "        JSONB coordinates\n",
    "        BOOLEAN is_valid\n",
    "        BOOLEAN is_overridden\n",
    "        INT overridden_by FK\n",
    "        DATETIME overridden_at\n",
    "    }\n",
    "\n",
    "    QUALITY_GATE_LOGS {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        BOOLEAN is_passed\n",
    "        TEXT failure_reason\n",
    "        DATETIME checked_at\n",
    "        VARCHAR check_origin          \n",
    "        INT checked_by FK            \n",
    "        BOOLEAN is_final_decision    \n",
    "        TEXT decision_comment\n",
    "    }\n",
    "\n",
    "    AUDIT_LOGS {\n",
    "        INT id PK\n",
    "        INT user_id FK\n",
    "        UUID document_id FK\n",
    "        VARCHAR action\n",
    "        VARCHAR entity_type\n",
    "        INT entity_id\n",
    "        JSONB changes\n",
    "        DATETIME timestamp\n",
    "        VARCHAR ip_address\n",
    "    }\n",
    "\n",
    "    %% Relations façon Workbench\n",
    "    ROLES ||--o{ USERS : \"role_id\"\n",
    "    USERS ||--o{ RULE_CONFIGS : \"created_by\"\n",
    "    USERS ||--o{ AUDIT_LOGS : \"user_id\"\n",
    "\n",
    "    DOMAINS ||--o{ RULE_CONFIGS : \"domain_id\"\n",
    "\n",
    "    %% Domaine -> APIs (plusieurs APIs dans le même domaine)\n",
    "    DOMAINS ||--o{ APIS : \"domain_id\"\n",
    "    RULE_CONFIGS ||--o{ APIS : \"rule_config_id\"\n",
    "    USERS ||--o{ APIS : \"created_by\"\n",
    "\n",
    "    %% API -> API_KEYS (plusieurs clés par API)\n",
    "    APIS ||--o{ API_KEYS : \"api_id\"\n",
    "\n",
    "    %% Domaine -> Documents\n",
    "    DOMAINS ||--o{ DOCUMENTS : \"domain_id\"\n",
    "    USERS ||--o{ DOCUMENTS : \"uploaded_by\"\n",
    "    APIS ||--o{ DOCUMENTS : \"api_id\"\n",
    "\n",
    "    %% Documents -> stockage + extractions + quality\n",
    "    DOCUMENTS ||--|| FILE_STORAGE : \"document_id\"\n",
    "    DOCUMENTS ||--o{ EXTRACTIONS : \"document_id\"\n",
    "    RULE_CONFIGS ||--o{ EXTRACTIONS : \"rule_config_id\"\n",
    "    USERS ||--o{ EXTRACTIONS : \"overridden_by\"\n",
    "\n",
    "    DOCUMENTS ||--o{ QUALITY_GATE_LOGS : \"document_id\"\n",
    "    USERS ||--o{ QUALITY_GATE_LOGS : \"checked_by\"\n",
    "\n",
    "    DOCUMENTS ||--o{ AUDIT_LOGS : \"document_id\"\n",
    "\"\"\"\n",
    "\n",
    "def extract_front_matter(mermaid_text: str):\n",
    "    s = mermaid_text.strip(\"\\n\")\n",
    "    if not s.lstrip().startswith(\"---\"):\n",
    "        return {}, s\n",
    "\n",
    "    m = re.match(r\"^\\s*---\\s*(.*?)\\s*---\\s*(.*)$\", s, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}, s\n",
    "\n",
    "    front = m.group(1)\n",
    "    body = m.group(2)\n",
    "\n",
    "    theme = None\n",
    "    layout = None\n",
    "    for line in front.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"theme:\"):\n",
    "            theme = line.split(\":\", 1)[1].strip()\n",
    "        if line.startswith(\"layout:\"):\n",
    "            layout = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    init = {}\n",
    "    if theme:\n",
    "        init[\"theme\"] = theme\n",
    "    if layout:\n",
    "        init[\"layout\"] = layout\n",
    "\n",
    "    return init, body.strip(\"\\n\")\n",
    "\n",
    "init_cfg, diagram = extract_front_matter(raw)\n",
    "init_directive = \"\"\n",
    "if init_cfg:\n",
    "    init_directive = f\"%%{{init: {json.dumps(init_cfg)} }}%%\\n\"\n",
    "\n",
    "diagram_final = init_directive + diagram\n",
    "div_id = f\"mmd-{uuid.uuid4().hex}\"\n",
    "\n",
    "html = f\"\"\"\n",
    "<div id=\"{div_id}\" class=\"mermaid\">\n",
    "{diagram_final}\n",
    "</div>\n",
    "\n",
    "<script type=\"module\">\n",
    "  const render = async () => {{\n",
    "    if (!window.__mermaid_loaded__) {{\n",
    "      const mermaid = (await import(\"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\")).default;\n",
    "      window.mermaid = mermaid;\n",
    "      window.__mermaid_loaded__ = true;\n",
    "      mermaid.initialize({{\n",
    "        startOnLoad: false,\n",
    "        securityLevel: \"loose\"\n",
    "      }});\n",
    "    }}\n",
    "    await window.mermaid.run({{\n",
    "      nodes: [document.getElementById(\"{div_id}\")]\n",
    "    }});\n",
    "  }};\n",
    "  render();\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f87e0",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22847b6f",
   "metadata": {},
   "source": [
    "lire les mot cle (token) du document et dire de quelle classe apartien selon les mot cle quil a :\n",
    "\n",
    "    \"BON_DE_COMMANDE\": [\n",
    "        \"BON DE COMMANDE\",\n",
    "        \"COMMANDE\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"TVA\"\n",
    "    ],\n",
    "    \"PURCHASE_ORDER\": [\n",
    "        \"PURCHASE ORDER\",\n",
    "        \"PO NUMBER\",\n",
    "        \"UNIT PRICE\",\n",
    "        \"QUANTITY\",\n",
    "        \"TOTAL AMOUNT\"\n",
    "    ],\n",
    "    \"CONTRAT\": [\n",
    "        \"CONTRAT\",\n",
    "        \"IL A ÉTÉ CONVENU\",\n",
    "        \"ENTRE LES SOUSSIGNÉS\",\n",
    "        \"RÉSILIATION\",\n",
    "        \"SIGNATURE\"\n",
    "    ],\n",
    "    \"ARTICLE\": [\n",
    "        \"ARTICLE\",\n",
    "        \"VU LA LOI\",\n",
    "        \"CONSIDÉRANT\",\n",
    "        \"DÉCRET\",\n",
    "        \"DISPOSITION\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e96f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue détectée : fr\n",
      "Classe détectée : FACTURE\n",
      "\n",
      "Scores détaillés :\n",
      "  FACTURE -> 7\n",
      "  BON_DE_COMMANDE -> 4\n",
      "  CONTRAT -> 2\n",
      "  ARTICLE -> 2\n",
      "  FORMULAIRE -> 5\n",
      "\n",
      "Mots-clés détectés :\n",
      "  FACTURE -> ['FACTURE', 'TVA', 'MODE DE PAIEMENT', 'PAIEMENT', 'CLIENT', 'DESCRIPTION', 'TTC']\n",
      "  BON_DE_COMMANDE -> ['MONTANT', 'TVA', 'SIGNATURE', 'DESCRIPTION']\n",
      "  CONTRAT -> ['SIGNATURE', 'SIGNATURE']\n",
      "  ARTICLE -> ['CODE', 'ACT']\n",
      "  FORMULAIRE -> ['DATE', 'SIGNATURE', 'CACHET', 'DATE', 'SIGNATURE']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from collections import defaultdict\n",
    "\n",
    "# TEXTE OCR\n",
    "texte = OCR_TEXT\n",
    "\n",
    "#  MOTS-CLÉS PAR CLASSE \n",
    "KEYWORDS = {\n",
    "\n",
    "    \"FACTURE\": [\n",
    "        # FR forts\n",
    "        \"FACTURE\",\n",
    "        \"NUMERO DE FACTURE\",\n",
    "        \"N° FACTURE\",\n",
    "        \"REFERENCE FACTURE\",\n",
    "        \"DATE DE FACTURE\",\n",
    "        \"ECHEANCE\",\n",
    "        \"DATE D'ECHEANCE\",\n",
    "        \"MONTANT A PAYER\",\n",
    "        \"MONTANT A PAYER TTC\",\n",
    "        \"MONTANT TTC\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"TOTAL HT\",\n",
    "        \"MONTANT HT\",\n",
    "        \"TVA\",\n",
    "        \"TAUX DE TVA\",\n",
    "        \"MONTANT TVA\",\n",
    "        \"SOUS-TOTAL\",\n",
    "        \"NET A PAYER\",\n",
    "        \"NET A PAYER TTC\",\n",
    "        \"SOLDE DU\",\n",
    "        \"A REGLER\",\n",
    "        \"MODE DE PAIEMENT\",\n",
    "        \"REGLEMENT\",\n",
    "        \"PAIEMENT\",\n",
    "        \"IBAN\",\n",
    "        \"BIC\",\n",
    "        \"RIB\",\n",
    "        \"VIREMENT\",\n",
    "        \"CHEQUE\",\n",
    "        \"ESPECES\",\n",
    "        \"BANQUE\",\n",
    "        \"REFERENCE CLIENT\",\n",
    "        \"CODE CLIENT\",\n",
    "        \"CLIENT\",\n",
    "        \"ADRESSE DE FACTURATION\",\n",
    "        \"ADRESSE DE LIVRAISON\",\n",
    "        \"SIRET\",\n",
    "        \"SIREN\",\n",
    "        \"RCS\",\n",
    "        \"N° TVA\",\n",
    "        \"TVA INTRACOMMUNAUTAIRE\",\n",
    "        \"N° TVA INTRACOMMUNAUTAIRE\",\n",
    "        \"BON DE LIVRAISON\",\n",
    "        \"BL\",\n",
    "        \"COMMANDE\",\n",
    "        \"N° COMMANDE\",\n",
    "        \"REFERENCE COMMANDE\",\n",
    "        \"DESIGNATION\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"QUANTITE\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"P.U.\",\n",
    "        \"MONTANT LIGNE\",\n",
    "        \"TOTAL LIGNE\",\n",
    "        \"REMISE\",\n",
    "        \"DISCOUNT\",\n",
    "        \"FRAIS DE PORT\",\n",
    "        \"LIVRAISON\",\n",
    "        \"PENALITES DE RETARD\",\n",
    "        \"CONDITIONS DE PAIEMENT\",\n",
    "        \"TTC\",\n",
    "        \"HT\",\n",
    "        \"TIMBRE\",\n",
    "        # EN\n",
    "        \"INVOICE\",\n",
    "        \"INVOICE NUMBER\",\n",
    "        \"INVOICE NO\",\n",
    "        \"BILL TO\",\n",
    "        \"SHIP TO\",\n",
    "        \"DUE DATE\",\n",
    "        \"PAYMENT TERMS\",\n",
    "        \"SUBTOTAL\",\n",
    "        \"TAX\",\n",
    "        \"VAT\",\n",
    "        \"TOTAL\",\n",
    "        \"TOTAL AMOUNT\",\n",
    "        \"AMOUNT DUE\",\n",
    "        \"BALANCE DUE\",\n",
    "        \"BANK TRANSFER\",\n",
    "        \"IBAN\",\n",
    "        \"BIC\",\n",
    "        \"SWIFT\"\n",
    "    ],\n",
    "\n",
    "    \"BON_DE_COMMANDE\": [\n",
    "        # FR forts\n",
    "        \"BON DE COMMANDE\",\n",
    "        \"BC\",\n",
    "        \"N° BC\",\n",
    "        \"NUMERO DE COMMANDE\",\n",
    "        \"N° COMMANDE\",\n",
    "        \"REFERENCE COMMANDE\",\n",
    "        \"DATE DE COMMANDE\",\n",
    "        \"COMMANDE\",\n",
    "        \"ACHETEUR\",\n",
    "        \"FOURNISSEUR\",\n",
    "        \"ADRESSE DE LIVRAISON\",\n",
    "        \"ADRESSE DE FACTURATION\",\n",
    "        \"LIVRAISON\",\n",
    "        \"DATE DE LIVRAISON\",\n",
    "        \"CONDITIONS DE LIVRAISON\",\n",
    "        \"INCOTERM\",\n",
    "        \"INCOTERMS\",\n",
    "        \"DESIGNATION\",\n",
    "        \"ARTICLE\",\n",
    "        \"REFERENCE\",\n",
    "        \"REF\",\n",
    "        \"CODE ARTICLE\",\n",
    "        \"CODE PRODUIT\",\n",
    "        \"SKU\",\n",
    "        \"QUANTITE\",\n",
    "        \"QTE\",\n",
    "        \"UNITE\",\n",
    "        \"PU\",\n",
    "        \"P.U.\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"PRIX UNITARE\",  # typo OCR fréquent\n",
    "        \"MONTANT\",\n",
    "        \"TOTAL\",\n",
    "        \"TOTAL HT\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"TVA\",\n",
    "        \"SOUS-TOTAL\",\n",
    "        \"REMISE\",\n",
    "        \"CONDITIONS DE PAIEMENT\",\n",
    "        \"DELAI DE PAIEMENT\",\n",
    "        \"SIGNATURE\",\n",
    "        \"VALIDATION\",\n",
    "        \"APPROBATION\",\n",
    "        \"BON POUR ACCORD\",\n",
    "        # EN\n",
    "        \"PURCHASE ORDER\",\n",
    "        \"PO\",\n",
    "        \"PO NUMBER\",\n",
    "        \"ORDER NUMBER\",\n",
    "        \"ORDER DATE\",\n",
    "        \"BUYER\",\n",
    "        \"VENDOR\",\n",
    "        \"SUPPLIER\",\n",
    "        \"SHIP TO\",\n",
    "        \"BILL TO\",\n",
    "        \"DELIVERY DATE\",\n",
    "        \"DELIVERY TERMS\",\n",
    "        \"INCOTERMS\",\n",
    "        \"ITEM\",\n",
    "        \"ITEM CODE\",\n",
    "        \"SKU\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"QUANTITY\",\n",
    "        \"QTY\",\n",
    "        \"UNIT PRICE\",\n",
    "        \"PRICE\",\n",
    "        \"SUBTOTAL\",\n",
    "        \"TAX\",\n",
    "        \"VAT\",\n",
    "        \"TOTAL AMOUNT\",\n",
    "        \"AUTHORIZED SIGNATURE\"\n",
    "    ],\n",
    "\n",
    "    \"CONTRAT\": [\n",
    "        # FR forts\n",
    "        \"CONTRAT\",\n",
    "        \"CONVENTION\",\n",
    "        \"ACCORD\",\n",
    "        \"IL A ETE CONVENU\",\n",
    "        \"ENTRE LES SOUSSIGNES\",\n",
    "        \"LES PARTIES\",\n",
    "        \"PARTIE\",\n",
    "        \"PREAMBULE\",\n",
    "        \"OBJET DU CONTRAT\",\n",
    "        \"OBJET\",\n",
    "        \"DUREE\",\n",
    "        \"DATE D'EFFET\",\n",
    "        \"ENTREE EN VIGUEUR\",\n",
    "        \"RENOUVELLEMENT\",\n",
    "        \"RESILIATION\",\n",
    "        \"RESILIATION ANTICIPEE\",\n",
    "        \"CLAUSE\",\n",
    "        \"ARTICLE 1\",\n",
    "        \"ARTICLE 2\",\n",
    "        \"OBLIGATIONS\",\n",
    "        \"ENGAGEMENTS\",\n",
    "        \"RESPONSABILITE\",\n",
    "        \"CONFIDENTIALITE\",\n",
    "        \"NON-DIVULGATION\",\n",
    "        \"PROPRIETE INTELLECTUELLE\",\n",
    "        \"FORCE MAJEURE\",\n",
    "        \"LITIGE\",\n",
    "        \"JURIDICTION\",\n",
    "        \"TRIBUNAL COMPETENT\",\n",
    "        \"DROIT APPLICABLE\",\n",
    "        \"LOI APPLICABLE\",\n",
    "        \"INDEMNISATION\",\n",
    "        \"PENALITES\",\n",
    "        \"GARANTIE\",\n",
    "        \"ANNEXE\",\n",
    "        \"AVENANT\",\n",
    "        \"SIGNATURE\",\n",
    "        \"FAIT A\",\n",
    "        \"LE PRESENT CONTRAT\",\n",
    "        # EN\n",
    "        \"CONTRACT\",\n",
    "        \"AGREEMENT\",\n",
    "        \"THIS AGREEMENT\",\n",
    "        \"WHEREAS\",\n",
    "        \"BETWEEN THE UNDERSIGNED\",\n",
    "        \"PARTIES\",\n",
    "        \"TERM\",\n",
    "        \"EFFECTIVE DATE\",\n",
    "        \"COMMENCEMENT\",\n",
    "        \"RENEWAL\",\n",
    "        \"TERMINATION\",\n",
    "        \"CONFIDENTIALITY\",\n",
    "        \"NONDISCLOSURE\",\n",
    "        \"INTELLECTUAL PROPERTY\",\n",
    "        \"GOVERNING LAW\",\n",
    "        \"JURISDICTION\",\n",
    "        \"LIABILITY\",\n",
    "        \"INDEMNIFICATION\",\n",
    "        \"FORCE MAJEURE\",\n",
    "        \"AMENDMENT\",\n",
    "        \"APPENDIX\",\n",
    "        \"SIGNATURE\"\n",
    "    ],\n",
    "\n",
    "    \"ARTICLE\": [\n",
    "        # FR\n",
    "        \"ARTICLE\",\n",
    "        \"ART.\",\n",
    "        \"VU LA LOI\",\n",
    "        \"VU LE CODE\",\n",
    "        \"CODE\",\n",
    "        \"CONSIDERANT\",\n",
    "        \"CONSIDÉRANT\",\n",
    "        \"ATTENDU QUE\",\n",
    "        \"DECRET\",\n",
    "        \"DÉCRET\",\n",
    "        \"ARRETE\",\n",
    "        \"ARRÊTÉ\",\n",
    "        \"LOI\",\n",
    "        \"ORDONNANCE\",\n",
    "        \"CIRCULAIRE\",\n",
    "        \"DISPOSITION\",\n",
    "        \"ALINEA\",\n",
    "        \"PARAGRAPHE\",\n",
    "        \"CHAPITRE\",\n",
    "        \"SECTION\",\n",
    "        \"TITRE\",\n",
    "        \"JOURNAL OFFICIEL\",\n",
    "        \"REPUBLIC\",\n",
    "        \"REPUBLIQUE\",\n",
    "        \"MINISTERE\",\n",
    "        \"MINISTÈRE\",\n",
    "        \"TRIBUNAL\",\n",
    "        \"COUR D'APPEL\",\n",
    "        \"CONSEIL D'ETAT\",\n",
    "        \"CONSEIL D’ÉTAT\",\n",
    "        \"DECISION\",\n",
    "        \"DÉCISION\",\n",
    "        \"JURISPRUDENCE\",\n",
    "        \"PROCEDURE\",\n",
    "        \"PROCÉDURE\",\n",
    "        \"SANCTION\",\n",
    "        \"AMENDE\",\n",
    "        \"AMENDEMENT\",\n",
    "        \"CONFORMEMENT A\",\n",
    "        \"EN APPLICATION DE\",\n",
    "        \"A COMPTER DU\",\n",
    "        \"ENTREE EN VIGUEUR\",\n",
    "        # EN\n",
    "        \"ARTICLE\",\n",
    "        \"SECTION\",\n",
    "        \"CHAPTER\",\n",
    "        \"WHEREAS\",\n",
    "        \"ACT\",\n",
    "        \"DECREE\",\n",
    "        \"REGULATION\",\n",
    "        \"LAW\",\n",
    "        \"PROVISION\",\n",
    "        \"Pursuant to\",\n",
    "        \"In accordance with\",\n",
    "        \"ENTRY INTO FORCE\",\n",
    "        \"EFFECTIVE\"\n",
    "    ],\n",
    "\n",
    "    \"FORMULAIRE\": [\n",
    "        # FR\n",
    "        \"FORMULAIRE\",\n",
    "        \"DEMANDE\",\n",
    "        \"DEMANDEUR\",\n",
    "        \"BENEFICIAIRE\",\n",
    "        \"BÉNÉFICIAIRE\",\n",
    "        \"NOM\",\n",
    "        \"PRENOM\",\n",
    "        \"PRÉNOM\",\n",
    "        \"DATE\",\n",
    "        \"DATE DE L'EXAMEN\",\n",
    "        \"SIGNATURE\",\n",
    "        \"CACHET\",\n",
    "        \"SIGNATURE ET CACHET\",\n",
    "        \"JE CERTIFIE\",\n",
    "        \"CERTIFIE QUE\",\n",
    "        \"CERTIFICATION\",\n",
    "        \"REPRESENTANT\",\n",
    "        \"REPRÉSENTANT\",\n",
    "        \"REPRESENTANT DU CLUB\",\n",
    "        \"CLUB\",\n",
    "        \"LICENCIE\",\n",
    "        \"LICENCIÉ\",\n",
    "        \"PIECES FOURNIES\",\n",
    "        \"DOCUMENT\",\n",
    "        \"INFORMATIONS FIGURANT\",\n",
    "        \"CACHEt DOIT ETRE LISIBLE\",\n",
    "        \"CADRE RESERVE\",\n",
    "        \"A REMPLIR\",\n",
    "        # EN\n",
    "        \"FORM\",\n",
    "        \"APPLICATION\",\n",
    "        \"APPLICANT\",\n",
    "        \"BENEFICIARY\",\n",
    "        \"NAME\",\n",
    "        \"FIRST NAME\",\n",
    "        \"DATE\",\n",
    "        \"SIGNATURE\",\n",
    "        \"STAMP\",\n",
    "        \"I CERTIFY\",\n",
    "        \"CERTIFY THAT\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#  DETECTION LANGUE \n",
    "sample = texte[:2000]\n",
    "try:\n",
    "    doc_lang = detect(sample)\n",
    "except:\n",
    "    doc_lang = \"fr\"\n",
    "\n",
    "#  CHARGER SPACY LEGER \n",
    "nlp = spacy.load(\n",
    "    \"fr_core_news_sm\",\n",
    "    disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"]\n",
    ")\n",
    "\n",
    "#  TOKENISATION GLOBALE \n",
    "doc = nlp.make_doc(texte)\n",
    "\n",
    "tokens = [t.text.upper() for t in doc if not t.is_space]\n",
    "\n",
    "# texte normalisé pour détection de phrases clés\n",
    "text_upper = \" \".join(tokens)\n",
    "\n",
    "#  SCORING DETERMINISTE \n",
    "scores = defaultdict(int)\n",
    "matched_keywords = defaultdict(list)\n",
    "\n",
    "for doc_type, keywords in KEYWORDS.items():\n",
    "    for kw in keywords:\n",
    "        if kw in text_upper:\n",
    "            scores[doc_type] += 1\n",
    "            matched_keywords[doc_type].append(kw)\n",
    "\n",
    "#  DECISION \n",
    "if scores:\n",
    "    detected_class = max(scores, key=scores.get)\n",
    "else:\n",
    "    detected_class = \"UNKNOWN\"\n",
    "\n",
    "#  RESULTAT \n",
    "print(\"Langue détectée :\", doc_lang)\n",
    "print(\"Classe détectée :\", detected_class)\n",
    "print(\"\\nScores détaillés :\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"  {k} -> {v}\")\n",
    "\n",
    "print(\"\\nMots-clés détectés :\")\n",
    "for k, v in matched_keywords.items():\n",
    "    print(f\"  {k} -> {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5fd1a",
   "metadata": {},
   "source": [
    "## Produire une sortie JSON stable (contrat de ton système)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13e5f42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "  \"doc_type\": \"FACTURE\",\n",
      "  \"status\": \"OK\",\n",
      "  \"scores\": {\n",
      "    \"FACTURE\": 7,\n",
      "    \"BON_DE_COMMANDE\": 4,\n",
      "    \"CONTRAT\": 2,\n",
      "    \"ARTICLE\": 2,\n",
      "    \"FORMULAIRE\": 5\n",
      "  },\n",
      "  \"matched_keywords\": {\n",
      "    \"FACTURE\": [\n",
      "      \"FACTURE\",\n",
      "      \"TVA\",\n",
      "      \"MODE DE PAIEMENT\",\n",
      "      \"PAIEMENT\",\n",
      "      \"CLIENT\",\n",
      "      \"DESCRIPTION\",\n",
      "      \"TTC\"\n",
      "    ],\n",
      "    \"BON_DE_COMMANDE\": [\n",
      "      \"MONTANT\",\n",
      "      \"TVA\",\n",
      "      \"SIGNATURE\",\n",
      "      \"DESCRIPTION\"\n",
      "    ],\n",
      "    \"CONTRAT\": [\n",
      "      \"SIGNATURE\",\n",
      "      \"SIGNATURE\"\n",
      "    ],\n",
      "    \"ARTICLE\": [\n",
      "      \"CODE\",\n",
      "      \"ACT\"\n",
      "    ],\n",
      "    \"FORMULAIRE\": [\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\",\n",
      "      \"CACHET\",\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\"\n",
      "    ]\n",
      "  },\n",
      "  \"threshold\": 3,\n",
      "  \"margin\": 2,\n",
      "  \"language_hint\": \"fr\",\n",
      "  \"decision_debug\": {\n",
      "    \"top_score\": 7,\n",
      "    \"second_score\": 5,\n",
      "    \"diff\": 2\n",
      "  },\n",
      "  \"routing\": {\n",
      "    \"rules_dir\": \"rules\",\n",
      "    \"doc_type\": \"FACTURE\",\n",
      "    \"template_id\": null,\n",
      "    \"ruleset\": {\n",
      "      \"ruleset_id\": \"INLINE_FACTURE_V1\",\n",
      "      \"extractors\": {},\n",
      "      \"validators\": {}\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule: Produire une sortie JSON stable (contrat de ton système)\n",
    "# + (6.5) Routage JSON/YAML (chargeur de règles par doc_type / template_id)\n",
    "# ============================\n",
    "\n",
    "import uuid\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# =========================\n",
    "# PARAMS de décision (stables)\n",
    "# =========================\n",
    "THRESHOLD = 3\n",
    "MARGIN = 2\n",
    "\n",
    "# =========================\n",
    "# 1) Normaliser la langue -> language_hint\n",
    "# (doc_lang vient de ta cellule précédente)\n",
    "# =========================\n",
    "if isinstance(doc_lang, str):\n",
    "    if doc_lang.startswith(\"fr\"):\n",
    "        language_hint = \"fr\"\n",
    "    elif doc_lang.startswith(\"en\"):\n",
    "        language_hint = \"en\"\n",
    "    else:\n",
    "        language_hint = \"mix\"\n",
    "else:\n",
    "    language_hint = \"mix\"\n",
    "\n",
    "# =========================\n",
    "# 2) Stabiliser scores/matched_keywords\n",
    "# - On veut que toutes les classes apparaissent, même à 0 / []\n",
    "# - KEYWORDS vient de ta cellule précédente\n",
    "# =========================\n",
    "scores_stable = {cls: int(scores.get(cls, 0)) for cls in KEYWORDS.keys()}\n",
    "matched_stable = {cls: list(matched_keywords.get(cls, [])) for cls in KEYWORDS.keys()}\n",
    "\n",
    "# =========================\n",
    "# 3) Calculer top_score et second_score\n",
    "# =========================\n",
    "sorted_items = sorted(scores_stable.items(), key=lambda kv: kv[1], reverse=True)\n",
    "top_type, top_score = sorted_items[0] if sorted_items else (\"UNKNOWN\", 0)\n",
    "second_score = sorted_items[1][1] if len(sorted_items) > 1 else 0\n",
    "\n",
    "# =========================\n",
    "# 4) Décision OK/REVIEW + doc_type final\n",
    "# - si aucun signal -> UNKNOWN + REVIEW\n",
    "# - sinon OK si seuil+margin, sinon REVIEW (classe = meilleure hypothèse)\n",
    "# =========================\n",
    "if top_score == 0:\n",
    "    doc_type_final = \"UNKNOWN\"\n",
    "    status = \"REVIEW\"\n",
    "else:\n",
    "    doc_type_final = top_type\n",
    "    if top_score >= THRESHOLD and (top_score - second_score) >= MARGIN:\n",
    "        status = \"OK\"\n",
    "    else:\n",
    "        status = \"REVIEW\"\n",
    "\n",
    "# =========================\n",
    "# 5) Construire le JSON stable (contrat)\n",
    "# =========================\n",
    "result: Dict[str, Any] = {\n",
    "    \"doc_id\": str(uuid.uuid4()),\n",
    "    \"doc_type\": doc_type_final,\n",
    "    \"status\": status,\n",
    "    \"scores\": scores_stable,\n",
    "    \"matched_keywords\": matched_stable,\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"margin\": MARGIN,\n",
    "    \"language_hint\": language_hint\n",
    "}\n",
    "\n",
    "# Debug utile pendant dev (optionnel)\n",
    "result[\"decision_debug\"] = {\n",
    "    \"top_score\": top_score,\n",
    "    \"second_score\": second_score,\n",
    "    \"diff\": top_score - second_score\n",
    "}\n",
    "\n",
    "# ==========================================================\n",
    "# (6.5) Routage JSON/YAML (déterministe)\n",
    "# Objectif: préparer le \"cerveau\" qui charge les règles\n",
    "# par doc_type + template_id (sans duplication de code).\n",
    "#\n",
    "# Notes:\n",
    "# - Si PyYAML n'est pas installé, fallback JSON (ou dict en dur).\n",
    "# - Pour notebook MVP, on peut garder des \"règles inline\" si fichiers absents.\n",
    "# ==========================================================\n",
    "\n",
    "def safe_load_yaml_or_json(path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Charge un fichier YAML ou JSON et retourne un dict.\n",
    "    Retourne None si fichier introuvable/erreur.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "\n",
    "        if path.lower().endswith(\".json\"):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        # YAML\n",
    "        try:\n",
    "            import yaml  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return yaml.safe_load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def merge_rules(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Merge simple et déterministe:\n",
    "    - dict: override keys\n",
    "    - list: concat (base + override) sans dédup (tu peux dédup plus tard si besoin)\n",
    "    - autres: override\n",
    "    \"\"\"\n",
    "    out = dict(base)\n",
    "    for k, v in override.items():\n",
    "        if k not in out:\n",
    "            out[k] = v\n",
    "            continue\n",
    "        if isinstance(out[k], dict) and isinstance(v, dict):\n",
    "            out[k] = merge_rules(out[k], v)\n",
    "        elif isinstance(out[k], list) and isinstance(v, list):\n",
    "            out[k] = out[k] + v\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def route_rules(\n",
    "    doc_type: str,\n",
    "    template_id: Optional[str] = None,\n",
    "    rules_dir: str = \"rules\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Résolution déterministe:\n",
    "    1) rules/common.(yaml|json) (facultatif)\n",
    "    2) rules/{doc_type}.(yaml|json) (facultatif)\n",
    "    3) rules/templates/{template_id}.(yaml|json) (facultatif)\n",
    "    Fallback: règles inline minimales.\n",
    "    \"\"\"\n",
    "    # 1) common\n",
    "    common = (\n",
    "        safe_load_yaml_or_json(f\"{rules_dir}/common.yaml\")\n",
    "        or safe_load_yaml_or_json(f\"{rules_dir}/common.json\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    # 2) doc_type specific\n",
    "    dt_rules = (\n",
    "        safe_load_yaml_or_json(f\"{rules_dir}/{doc_type}.yaml\")\n",
    "        or safe_load_yaml_or_json(f\"{rules_dir}/{doc_type}.json\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    merged = merge_rules(common, dt_rules)\n",
    "\n",
    "    # 3) template specific\n",
    "    if template_id:\n",
    "        tpl_rules = (\n",
    "            safe_load_yaml_or_json(f\"{rules_dir}/templates/{template_id}.yaml\")\n",
    "            or safe_load_yaml_or_json(f\"{rules_dir}/templates/{template_id}.json\")\n",
    "            or {}\n",
    "        )\n",
    "        merged = merge_rules(merged, tpl_rules)\n",
    "\n",
    "    # Fallback minimal si rien trouvé: règles vides, mais structure stable\n",
    "    if not merged:\n",
    "        merged = {\n",
    "            \"ruleset_id\": f\"INLINE_{doc_type}_V1\",\n",
    "            \"extractors\": {},\n",
    "            \"validators\": {}\n",
    "        }\n",
    "\n",
    "    # Ajout meta de routage (traçabilité)\n",
    "    merged.setdefault(\"ruleset_id\", f\"RULESET_{doc_type}_V1\")\n",
    "    merged.setdefault(\"extractors\", {})\n",
    "    merged.setdefault(\"validators\", {})\n",
    "    return merged\n",
    "\n",
    "# On attache une \"config active\" au result (pour l'étape API ensuite)\n",
    "# template_id peut ne pas exister encore (calculé plus tard), donc None ici\n",
    "result[\"routing\"] = {\n",
    "    \"rules_dir\": \"rules\",\n",
    "    \"doc_type\": result[\"doc_type\"],\n",
    "    \"template_id\": result.get(\"template_id\", None),\n",
    "    \"ruleset\": route_rules(result[\"doc_type\"], template_id=result.get(\"template_id\", None))\n",
    "}\n",
    "\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb612b1",
   "metadata": {},
   "source": [
    "## METADATA “empreinte de mise en page”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2ceef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "  \"doc_type\": \"FACTURE\",\n",
      "  \"status\": \"OK\",\n",
      "  \"scores\": {\n",
      "    \"FACTURE\": 7,\n",
      "    \"BON_DE_COMMANDE\": 4,\n",
      "    \"CONTRAT\": 2,\n",
      "    \"ARTICLE\": 2,\n",
      "    \"FORMULAIRE\": 5\n",
      "  },\n",
      "  \"matched_keywords\": {\n",
      "    \"FACTURE\": [\n",
      "      \"FACTURE\",\n",
      "      \"TVA\",\n",
      "      \"MODE DE PAIEMENT\",\n",
      "      \"PAIEMENT\",\n",
      "      \"CLIENT\",\n",
      "      \"DESCRIPTION\",\n",
      "      \"TTC\"\n",
      "    ],\n",
      "    \"BON_DE_COMMANDE\": [\n",
      "      \"MONTANT\",\n",
      "      \"TVA\",\n",
      "      \"SIGNATURE\",\n",
      "      \"DESCRIPTION\"\n",
      "    ],\n",
      "    \"CONTRAT\": [\n",
      "      \"SIGNATURE\",\n",
      "      \"SIGNATURE\"\n",
      "    ],\n",
      "    \"ARTICLE\": [\n",
      "      \"CODE\",\n",
      "      \"ACT\"\n",
      "    ],\n",
      "    \"FORMULAIRE\": [\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\",\n",
      "      \"CACHET\",\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\"\n",
      "    ]\n",
      "  },\n",
      "  \"threshold\": 3,\n",
      "  \"margin\": 2,\n",
      "  \"language_hint\": \"fr\",\n",
      "  \"decision_debug\": {\n",
      "    \"top_score\": 7,\n",
      "    \"second_score\": 5,\n",
      "    \"diff\": 2\n",
      "  },\n",
      "  \"routing\": {\n",
      "    \"rules_dir\": \"rules\",\n",
      "    \"doc_type\": \"FACTURE\",\n",
      "    \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "    \"ruleset\": {\n",
      "      \"ruleset_id\": \"INLINE_FACTURE_V1\",\n",
      "      \"extractors\": {},\n",
      "      \"validators\": {}\n",
      "    }\n",
      "  },\n",
      "  \"layout_fingerprint\": {\n",
      "    \"page_count\": 1,\n",
      "    \"page_sizes\": [\n",
      "      {\n",
      "        \"w\": 1152,\n",
      "        \"h\": 1536\n",
      "      }\n",
      "    ],\n",
      "    \"orientation_deg\": 0,\n",
      "    \"skew_angle_deg\": [\n",
      "      0.0\n",
      "    ],\n",
      "    \"has_table\": [\n",
      "      true\n",
      "    ],\n",
      "    \"table_line_counts\": [\n",
      "      {\n",
      "        \"h_lines\": 115,\n",
      "        \"v_lines\": 13\n",
      "      }\n",
      "    ],\n",
      "    \"text_density\": [\n",
      "      {\n",
      "        \"text_density_alnum_chars_per_mpx\": 390.5119719328704,\n",
      "        \"image_area_mpx\": 1.769472\n",
      "      }\n",
      "    ],\n",
      "    \"ocr_confidence\": [\n",
      "      {\n",
      "        \"ocr_avg_conf\": 85.55555555555556,\n",
      "        \"ocr_words\": 180\n",
      "      }\n",
      "    ],\n",
      "    \"aspect_ratio\": [\n",
      "      0.75\n",
      "    ],\n",
      "    \"grid_type\": \"ITEM_TABLE\"\n",
      "  },\n",
      "  \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "  \"template_signature\": {\n",
      "    \"page_count\": 1,\n",
      "    \"w_rounded\": 1150,\n",
      "    \"h_rounded\": 1550,\n",
      "    \"aspect_ratio_rounded\": 0.75,\n",
      "    \"orientation_deg\": 0,\n",
      "    \"has_table\": [\n",
      "      true\n",
      "    ],\n",
      "    \"h_lines_bucket\": \">15\",\n",
      "    \"v_lines_bucket\": \"6-15\",\n",
      "    \"grid_type\": \"ITEM_TABLE\"\n",
      "  },\n",
      "  \"quality_gate\": {\n",
      "    \"status\": \"PASS\",\n",
      "    \"reasons\": [],\n",
      "    \"thresholds\": {\n",
      "      \"min_ocr_avg_conf\": 75,\n",
      "      \"min_ocr_words\": 40,\n",
      "      \"max_abs_skew_deg\": 5.0,\n",
      "      \"min_text_density_alnum_chars_per_mpx\": 120\n",
      "    }\n",
      "  },\n",
      "  \"metadata_summary\": {\n",
      "    \"page_count\": 1,\n",
      "    \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "    \"orientation_deg\": 0,\n",
      "    \"avg_skew_deg\": 0.0,\n",
      "    \"has_table_any\": true,\n",
      "    \"ocr_avg_conf_mean\": 85.55555555555556,\n",
      "    \"grid_type\": \"ITEM_TABLE\",\n",
      "    \"quality_gate_status\": \"PASS\",\n",
      "    \"status_after_quality_gate\": \"OK\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule: METADATA “empreinte de mise en page”\n",
    "# + Quality gate appliqué sur status (enforced)\n",
    "# ============================\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# suppose: prepped (PIL image), OCR_TEXT (str), result (dict) existent déjà\n",
    "\n",
    "def pil_to_gray_np(pil_img):\n",
    "    return np.array(pil_img.convert(\"L\"))\n",
    "\n",
    "def get_orientation_deg(pil_img) -> int:\n",
    "    \"\"\"\n",
    "    Orientation 0/90/180/270 via Tesseract OSD.\n",
    "    Si OSD échoue, retourne 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(pil_img)\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                return int(line.split(\":\")[1].strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "def rotate_pil_by_deg(pil_img, deg: int):\n",
    "    if deg % 360 == 0:\n",
    "        return pil_img\n",
    "    return pil_img.rotate(-deg, expand=True)\n",
    "\n",
    "def estimate_skew_hough_deg(gray: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estime un skew petit angle via lignes Hough.\n",
    "    Retour typiquement dans [-10, +10] si doc normal.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    _, bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    edges = cv2.Canny(bw, 50, 150)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges,\n",
    "        1,\n",
    "        np.pi / 180,\n",
    "        threshold=120,\n",
    "        minLineLength=max(60, int(min(gray.shape) * 0.12)),\n",
    "        maxLineGap=10\n",
    "    )\n",
    "\n",
    "    if lines is None:\n",
    "        return 0.0\n",
    "\n",
    "    angles = []\n",
    "    for x1, y1, x2, y2 in lines[:, 0]:\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        if dx == 0:\n",
    "            continue\n",
    "        angle = np.degrees(np.arctan2(dy, dx))\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        if angle > 45:\n",
    "            angle -= 90\n",
    "        if -15 <= angle <= 15:\n",
    "            angles.append(angle)\n",
    "\n",
    "    if not angles:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.median(angles))\n",
    "\n",
    "def has_table_hough(gray: np.ndarray) -> dict:\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    _, bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    edges = cv2.Canny(bw, 50, 150)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges, 1, np.pi / 180,\n",
    "        threshold=80,\n",
    "        minLineLength=max(40, int(min(gray.shape) * 0.08)),\n",
    "        maxLineGap=10\n",
    "    )\n",
    "\n",
    "    h_count = 0\n",
    "    v_count = 0\n",
    "    if lines is not None:\n",
    "        for x1, y1, x2, y2 in lines[:, 0]:\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            if abs(dy) <= max(2, 0.1 * abs(dx)):\n",
    "                h_count += 1\n",
    "            if abs(dx) <= max(2, 0.1 * abs(dy)):\n",
    "                v_count += 1\n",
    "\n",
    "    has_table = (h_count >= 6 and v_count >= 4)\n",
    "    return {\"has_table\": bool(has_table), \"h_lines\": int(h_count), \"v_lines\": int(v_count)}\n",
    "\n",
    "def ocr_confidence_stats(pil_img) -> dict:\n",
    "    try:\n",
    "        data = pytesseract.image_to_data(\n",
    "            pil_img,\n",
    "            lang=getattr(args, \"lang\", None) or \"fra\",\n",
    "            config=globals().get(\"config\", \"\"),\n",
    "            output_type=pytesseract.Output.DICT\n",
    "        )\n",
    "        confs = []\n",
    "        for c in data.get(\"conf\", []):\n",
    "            try:\n",
    "                c = float(c)\n",
    "                if c >= 0:\n",
    "                    confs.append(c)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not confs:\n",
    "            return {\"ocr_avg_conf\": None, \"ocr_words\": 0}\n",
    "\n",
    "        return {\"ocr_avg_conf\": float(sum(confs) / len(confs)), \"ocr_words\": int(len(confs))}\n",
    "    except Exception:\n",
    "        return {\"ocr_avg_conf\": None, \"ocr_words\": 0}\n",
    "\n",
    "def text_density_stats(ocr_text: str, pil_img) -> dict:\n",
    "    w, h = pil_img.size\n",
    "    area_mpx = (w * h) / 1_000_000.0\n",
    "    area_mpx = area_mpx if area_mpx > 0 else 1.0\n",
    "    alnum_chars = sum(1 for ch in ocr_text if ch.isalnum())\n",
    "    return {\n",
    "        \"text_density_alnum_chars_per_mpx\": float(alnum_chars / area_mpx),\n",
    "        \"image_area_mpx\": float(area_mpx)\n",
    "    }\n",
    "\n",
    "def stable_template_id(fingerprint: dict) -> str:\n",
    "    canon = json.dumps(fingerprint, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "    digest = hashlib.sha256(canon.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"tpl_{digest[:16]}\"\n",
    "\n",
    "def bucketize_lines(n: int) -> str:\n",
    "    if n <= 0:\n",
    "        return \"0\"\n",
    "    if 1 <= n <= 5:\n",
    "        return \"1-5\"\n",
    "    if 6 <= n <= 15:\n",
    "        return \"6-15\"\n",
    "    return \">15\"\n",
    "\n",
    "def round_size(x: int, base: int = 50) -> int:\n",
    "    return int(base * round(x / base))\n",
    "\n",
    "def round_ratio(x: float, decimals: int = 2) -> float:\n",
    "    return float(round(x, decimals))\n",
    "\n",
    "def compute_grid_type_from_counts(h_lines: int, v_lines: int) -> str:\n",
    "    \"\"\"\n",
    "    Heuristique déterministe:\n",
    "      - ITEM_TABLE: tableau d'articles (beaucoup de lignes H et V)\n",
    "      - BOXED_FORM: formulaire encadré (beaucoup de H, peu de V)\n",
    "      - NONE: pas de structure de grille notable\n",
    "    \"\"\"\n",
    "    if h_lines >= 20 and v_lines >= 8:\n",
    "        return \"ITEM_TABLE\"\n",
    "    if h_lines >= 20 and v_lines < 8:\n",
    "        return \"BOXED_FORM\"\n",
    "    return \"NONE\"\n",
    "\n",
    "def compute_quality_gate(layout_fp: dict) -> dict:\n",
    "    reasons = []\n",
    "\n",
    "    # OCR confidence\n",
    "    ocr_list = layout_fp.get(\"ocr_confidence\", [])\n",
    "    ocr_avg = None\n",
    "    ocr_words = None\n",
    "    if ocr_list:\n",
    "        ocr_avg = ocr_list[0].get(\"ocr_avg_conf\", None)\n",
    "        ocr_words = ocr_list[0].get(\"ocr_words\", None)\n",
    "\n",
    "    if ocr_avg is not None and ocr_avg < 75:\n",
    "        reasons.append(\"LOW_OCR_CONF\")\n",
    "    if ocr_words is not None and ocr_words < 40:\n",
    "        reasons.append(\"TOO_FEW_WORDS\")\n",
    "\n",
    "    # skew\n",
    "    skew_list = layout_fp.get(\"skew_angle_deg\", [])\n",
    "    if skew_list:\n",
    "        skew = float(skew_list[0])\n",
    "        if abs(skew) > 5.0:\n",
    "            reasons.append(\"HIGH_SKEW\")\n",
    "\n",
    "    # densité\n",
    "    dens_list = layout_fp.get(\"text_density\", [])\n",
    "    if dens_list:\n",
    "        dens = dens_list[0].get(\"text_density_alnum_chars_per_mpx\", None)\n",
    "        if dens is not None and dens < 120:\n",
    "            reasons.append(\"LOW_TEXT_DENSITY\")\n",
    "\n",
    "    status = \"PASS\" if len(reasons) == 0 else \"REVIEW\"\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"reasons\": reasons,\n",
    "        \"thresholds\": {\n",
    "            \"min_ocr_avg_conf\": 75,\n",
    "            \"min_ocr_words\": 40,\n",
    "            \"max_abs_skew_deg\": 5.0,\n",
    "            \"min_text_density_alnum_chars_per_mpx\": 120\n",
    "        }\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Orientation + correction\n",
    "# -----------------------------\n",
    "orientation_deg = get_orientation_deg(prepped)\n",
    "prepped_oriented = rotate_pil_by_deg(prepped, orientation_deg)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Calcul features par page (1 page ici)\n",
    "# -----------------------------\n",
    "pages = [prepped_oriented]\n",
    "\n",
    "page_sizes = []\n",
    "skew_angles = []\n",
    "table_flags = []\n",
    "ocr_confs = []\n",
    "densities = []\n",
    "\n",
    "for p in pages:\n",
    "    w, h = p.size\n",
    "    page_sizes.append({\"w\": int(w), \"h\": int(h)})\n",
    "\n",
    "    gray = pil_to_gray_np(p)\n",
    "\n",
    "    skew_angles.append(estimate_skew_hough_deg(gray))\n",
    "    tbl = has_table_hough(gray)\n",
    "    table_flags.append(tbl)\n",
    "\n",
    "    ocr_confs.append(ocr_confidence_stats(p))\n",
    "    densities.append(text_density_stats(OCR_TEXT, p))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) layout_fingerprint complet\n",
    "# -----------------------------\n",
    "layout_fingerprint = {\n",
    "    \"page_count\": int(len(pages)),\n",
    "    \"page_sizes\": page_sizes,\n",
    "    \"orientation_deg\": int(orientation_deg),\n",
    "    \"skew_angle_deg\": [float(a) for a in skew_angles],\n",
    "    \"has_table\": [bool(t[\"has_table\"]) for t in table_flags],\n",
    "    \"table_line_counts\": [{\"h_lines\": t[\"h_lines\"], \"v_lines\": t[\"v_lines\"]} for t in table_flags],\n",
    "    \"text_density\": densities,\n",
    "    \"ocr_confidence\": ocr_confs,\n",
    "    \"aspect_ratio\": [float(ps[\"w\"] / ps[\"h\"]) for ps in page_sizes]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 3bis) grid_type\n",
    "# -----------------------------\n",
    "h_lines0 = int(layout_fingerprint[\"table_line_counts\"][0][\"h_lines\"]) if layout_fingerprint[\"table_line_counts\"] else 0\n",
    "v_lines0 = int(layout_fingerprint[\"table_line_counts\"][0][\"v_lines\"]) if layout_fingerprint[\"table_line_counts\"] else 0\n",
    "grid_type = compute_grid_type_from_counts(h_lines0, v_lines0)\n",
    "layout_fingerprint[\"grid_type\"] = grid_type\n",
    "\n",
    "# -----------------------------\n",
    "# 3ter) quality_gate\n",
    "# -----------------------------\n",
    "quality_gate = compute_quality_gate(layout_fingerprint)\n",
    "\n",
    "# ============================\n",
    "# ENFORCE: appliquer la quality gate sur status\n",
    "# ============================\n",
    "# - si gate FAIL/REVIEW => on force status REVIEW (même si classification OK)\n",
    "# - on laisse doc_type inchangé (hypothèse), mais status devient REVIEW\n",
    "if isinstance(result, dict) and quality_gate.get(\"status\") == \"REVIEW\":\n",
    "    result[\"status\"] = \"REVIEW\"\n",
    "    result.setdefault(\"quality_gate_enforced\", {})\n",
    "    result[\"quality_gate_enforced\"] = {\n",
    "        \"forced_status\": \"REVIEW\",\n",
    "        \"reason_count\": len(quality_gate.get(\"reasons\", [])),\n",
    "        \"reasons\": list(quality_gate.get(\"reasons\", [])),\n",
    "        \"rule_id\": \"RULE_QUALITY_GATE_V1\"\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 4) template_id stable bucketed\n",
    "# -----------------------------\n",
    "w0 = int(page_sizes[0][\"w\"]) if page_sizes else 0\n",
    "h0_size = int(page_sizes[0][\"h\"]) if page_sizes else 0\n",
    "ratio0 = (w0 / h0_size) if (w0 and h0_size) else 0.0\n",
    "\n",
    "layout_signature_bucketed = {\n",
    "    \"page_count\": layout_fingerprint[\"page_count\"],\n",
    "    \"w_rounded\": round_size(w0, base=50),\n",
    "    \"h_rounded\": round_size(h0_size, base=50),\n",
    "    \"aspect_ratio_rounded\": round_ratio(ratio0, 2),\n",
    "    \"orientation_deg\": layout_fingerprint[\"orientation_deg\"],\n",
    "    \"has_table\": layout_fingerprint[\"has_table\"],\n",
    "    \"h_lines_bucket\": bucketize_lines(int(layout_fingerprint[\"table_line_counts\"][0][\"h_lines\"])) if layout_fingerprint[\"table_line_counts\"] else \"0\",\n",
    "    \"v_lines_bucket\": bucketize_lines(int(layout_fingerprint[\"table_line_counts\"][0][\"v_lines\"])) if layout_fingerprint[\"table_line_counts\"] else \"0\",\n",
    "    \"grid_type\": grid_type\n",
    "}\n",
    "template_id = stable_template_id(layout_signature_bucketed)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Attacher au JSON principal\n",
    "# -----------------------------\n",
    "result[\"layout_fingerprint\"] = layout_fingerprint\n",
    "result[\"template_id\"] = template_id\n",
    "result[\"template_signature\"] = layout_signature_bucketed\n",
    "result[\"quality_gate\"] = quality_gate\n",
    "\n",
    "# Update routing si présent (doc_type déjà, maintenant template_id est connu)\n",
    "if \"routing\" in result and isinstance(result[\"routing\"], dict):\n",
    "    result[\"routing\"][\"template_id\"] = template_id\n",
    "    # On peut re-router avec template_id (si tu as des règles template)\n",
    "    try:\n",
    "        ruleset = result[\"routing\"].get(\"ruleset\", {})\n",
    "        # Si route_rules existe (cellule précédente), on recharge:\n",
    "        if \"route_rules\" in globals():\n",
    "            result[\"routing\"][\"ruleset\"] = route_rules(result.get(\"doc_type\", \"UNKNOWN\"), template_id=template_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# metadata_summary\n",
    "summary = result.get(\"metadata_summary\", {})\n",
    "summary.update({\n",
    "    \"page_count\": layout_fingerprint[\"page_count\"],\n",
    "    \"template_id\": template_id,\n",
    "    \"orientation_deg\": orientation_deg,\n",
    "    \"avg_skew_deg\": float(sum(skew_angles) / len(skew_angles)) if skew_angles else 0.0,\n",
    "    \"has_table_any\": bool(any(layout_fingerprint[\"has_table\"])),\n",
    "    \"ocr_avg_conf_mean\": (\n",
    "        float(\n",
    "            sum(c[\"ocr_avg_conf\"] for c in ocr_confs if c[\"ocr_avg_conf\"] is not None) /\n",
    "            max(1, len([c for c in ocr_confs if c[\"ocr_avg_conf\"] is not None]))\n",
    "        )\n",
    "        if ocr_confs else None\n",
    "    ),\n",
    "    \"grid_type\": grid_type,\n",
    "    \"quality_gate_status\": quality_gate[\"status\"],\n",
    "    \"status_after_quality_gate\": result.get(\"status\")\n",
    "})\n",
    "result[\"metadata_summary\"] = summary\n",
    "\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f5c08",
   "metadata": {},
   "source": [
    "## Chunking par page avec identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c7e54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks générés: 2\n",
      "Aperçu 2 premiers chunks:\n",
      "- page=1 chunk_index=0 start=0 end=800 len=800 id=chk_7b97d3c6ee1b4b0b\n",
      "  excerpt: FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 ...\n",
      "- page=1 chunk_index=1 start=650 end=907 len=257 id=chk_9bda55a9a431402f\n",
      "  excerpt:  19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Chunking par page avec identifiants (base du chat + citations)\n",
    "# ============================\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Cette cellule suppose que tu as déjà :\n",
    "# - result (dict) : contient au moins result[\"doc_id\"] + result[\"status\"]\n",
    "# - OCR_TEXT (str) : texte OCR complet (pour 1 page dans ton pipeline actuel)\n",
    "#\n",
    "# Améliorations :\n",
    "# - skip si status=REVIEW (quality_gate enforced)\n",
    "# - excerpt (pour citations)\n",
    "# - char_len (debug/qualité)\n",
    "# - nettoyage minimal stable\n",
    "# - garde page + start/end pour citer précisément\n",
    "\n",
    "CHUNK_SIZE = 800      # 600-900 conseillé\n",
    "CHUNK_OVERLAP = 150   # 100-150 conseillé\n",
    "EXCERPT_LEN = 220     # extrait court pour citations JSON\n",
    "\n",
    "\n",
    "def clean_text_stable(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text_fixed(text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Découpe en chunks de taille fixe avec overlap.\n",
    "    Retourne: {chunk_index, start_char, end_char, text}\n",
    "    \"\"\"\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size doit être > 0\")\n",
    "    if overlap < 0 or overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap doit être >= 0 et < chunk_size\")\n",
    "\n",
    "    text = clean_text_stable(text)\n",
    "    n = len(text)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    step = chunk_size - overlap\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    idx = 0\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk_str = text[start:end]\n",
    "\n",
    "        if len(chunk_str.strip()) >= 20:\n",
    "            chunks.append({\n",
    "                \"chunk_index\": idx,\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": end,\n",
    "                \"text\": chunk_str\n",
    "            })\n",
    "\n",
    "        idx += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        start += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks_for_pages(\n",
    "    doc_id: str,\n",
    "    pages_text: List[str],\n",
    "    chunk_size: int,\n",
    "    overlap: int,\n",
    "    excerpt_len: int\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Construit des chunks par page.\n",
    "    Format chunk:\n",
    "      chunk_id, doc_id, page, chunk_index, start_char, end_char, char_len, text, excerpt\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    for page_num, page_text in enumerate(pages_text, start=1):\n",
    "        cleaned = clean_text_stable(page_text)\n",
    "        page_chunks = chunk_text_fixed(cleaned, chunk_size, overlap)\n",
    "\n",
    "        for c in page_chunks:\n",
    "            chunk_id = f\"chk_{uuid.uuid4().hex[:16]}\"\n",
    "            chunk_text = c[\"text\"]\n",
    "            excerpt = chunk_text[:excerpt_len]\n",
    "\n",
    "            all_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"page\": page_num,\n",
    "                \"chunk_index\": c[\"chunk_index\"],\n",
    "                \"start_char\": c[\"start_char\"],\n",
    "                \"end_char\": c[\"end_char\"],\n",
    "                \"char_len\": int(c[\"end_char\"] - c[\"start_char\"]),\n",
    "                \"text\": chunk_text,\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Enforced: si status=REVIEW -> pas de chunking\n",
    "# -----------------------------------\n",
    "if result.get(\"status\") == \"REVIEW\":\n",
    "    result[\"chunking\"] = {\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"overlap\": CHUNK_OVERLAP,\n",
    "        \"excerpt_len\": EXCERPT_LEN,\n",
    "        \"chunks_count\": 0,\n",
    "        \"skipped_reason\": \"status=REVIEW (quality_gate enforced)\"\n",
    "    }\n",
    "    result[\"chunks\"] = []\n",
    "    print(\"[skip] status=REVIEW (quality_gate enforced) -> pas de chunking\")\n",
    "else:\n",
    "    doc_id = result[\"doc_id\"]\n",
    "    pages_text = [OCR_TEXT]  # 1 page (plus tard: liste de textes par page)\n",
    "\n",
    "    chunks = build_chunks_for_pages(\n",
    "        doc_id=doc_id,\n",
    "        pages_text=pages_text,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=CHUNK_OVERLAP,\n",
    "        excerpt_len=EXCERPT_LEN\n",
    "    )\n",
    "\n",
    "    result[\"chunking\"] = {\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"overlap\": CHUNK_OVERLAP,\n",
    "        \"excerpt_len\": EXCERPT_LEN,\n",
    "        \"chunks_count\": len(chunks)\n",
    "    }\n",
    "    result[\"chunks\"] = chunks\n",
    "\n",
    "    print(f\"Chunks générés: {len(chunks)}\")\n",
    "    print(\"Aperçu 2 premiers chunks:\")\n",
    "    for c in chunks[:2]:\n",
    "        preview = c[\"excerpt\"].replace(\"\\n\", \" \")\n",
    "        print(f\"- page={c['page']} chunk_index={c['chunk_index']} start={c['start_char']} end={c['end_char']} len={c['char_len']} id={c['chunk_id']}\")\n",
    "        print(f\"  excerpt: {preview}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c5a13",
   "metadata": {},
   "source": [
    "## Embeddings + recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1393e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[index] docs=2 chunks=4 dim=1240\n",
      "[retrieve] query: tel ?\n",
      "- score=0.0467 doc_id=cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf page=1 chunk_id=chk_54b2fa7ac9304a27\n",
      "  excerpt: FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 ...\n",
      "- score=0.0467 doc_id=fe57bb6d-e02e-4d49-ad20-cf93b795ed78 page=1 chunk_id=chk_7b97d3c6ee1b4b0b\n",
      "  excerpt: FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 ...\n",
      "- score=0.0000 doc_id=cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf page=1 chunk_id=chk_4521a4edb87f46fd\n",
      "  excerpt:  19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Embeddings + recherche (TF-IDF char n-grams)\n",
    "# ============================\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Cette cellule suppose que tu as déjà :\n",
    "# - result (dict) avec chunks[]\n",
    "# - result[\"chunks\"] contient: chunk_id, doc_id, page, text, excerpt\n",
    "# - result[\"status\"] existe (enforced)\n",
    "\n",
    "# -------------------------\n",
    "# 0) Corpus en mémoire\n",
    "# -------------------------\n",
    "if \"CORPUS\" not in globals():\n",
    "    CORPUS = []\n",
    "\n",
    "existing_ids = {d.get(\"doc_id\") for d in CORPUS}\n",
    "if result.get(\"doc_id\") not in existing_ids:\n",
    "    CORPUS.append(result)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Index TF-IDF (char n-grams) - robuste OCR\n",
    "# -------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "_vectorizer = None\n",
    "_embeddings = None\n",
    "_chunks_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def _normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1.0, norms)\n",
    "    return x / norms\n",
    "\n",
    "\n",
    "def _collect_all_chunks(corpus: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    all_chunks = []\n",
    "    for doc in corpus:\n",
    "        # Enforced: ne pas indexer les docs en REVIEW\n",
    "        if doc.get(\"status\") == \"REVIEW\":\n",
    "            continue\n",
    "\n",
    "        doc_id = doc.get(\"doc_id\")\n",
    "        doc_type = doc.get(\"doc_type\")\n",
    "        template_id = doc.get(\"template_id\")\n",
    "        lang = doc.get(\"language_hint\")\n",
    "        status = doc.get(\"status\")\n",
    "\n",
    "        for ch in doc.get(\"chunks\", []):\n",
    "            all_chunks.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_type\": doc_type,\n",
    "                \"template_id\": template_id,\n",
    "                \"language_hint\": lang,\n",
    "                \"status\": status,\n",
    "                \"page\": ch.get(\"page\"),\n",
    "                \"chunk_id\": ch.get(\"chunk_id\"),\n",
    "                \"text\": ch.get(\"text\", \"\"),\n",
    "                \"excerpt\": ch.get(\"excerpt\", \"\")\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def build_index(corpus: List[Dict[str, Any]]):\n",
    "    global _vectorizer, _embeddings, _chunks_meta\n",
    "\n",
    "    _chunks_meta = _collect_all_chunks(corpus)\n",
    "    texts = [c[\"text\"] for c in _chunks_meta]\n",
    "\n",
    "    if len(texts) == 0:\n",
    "        _embeddings = np.zeros((0, 1), dtype=np.float32)\n",
    "        return\n",
    "\n",
    "    _vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3, 5),\n",
    "        max_features=20000\n",
    "    )\n",
    "    X = _vectorizer.fit_transform(texts)\n",
    "    emb = X.toarray().astype(np.float32)\n",
    "    _embeddings = _normalize_rows(emb)\n",
    "\n",
    "\n",
    "def _embed_query(query: str) -> np.ndarray:\n",
    "    if _embeddings is None or len(_chunks_meta) == 0:\n",
    "        return np.zeros((1, 1), dtype=np.float32)\n",
    "\n",
    "    q = _vectorizer.transform([query]).toarray().astype(np.float32)\n",
    "    q = _normalize_rows(q)\n",
    "    return q\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    filters: Optional[Dict[str, Any]] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retourne top-k chunks avec {doc_id, page, chunk_id, excerpt, score}\n",
    "    filters possibles:\n",
    "      - doc_type: str ou [str]\n",
    "      - template_id: str ou [str]\n",
    "      - language_hint: str ou [str]\n",
    "      - status: str ou [str]  (optionnel)\n",
    "    \"\"\"\n",
    "    if _embeddings is None or len(_chunks_meta) == 0:\n",
    "        return []\n",
    "\n",
    "    q = _embed_query(query)\n",
    "    sims = (_embeddings @ q.T).reshape(-1)\n",
    "\n",
    "    idxs = np.arange(len(_chunks_meta))\n",
    "\n",
    "    if filters:\n",
    "        def _as_set(v):\n",
    "            if v is None:\n",
    "                return None\n",
    "            if isinstance(v, list):\n",
    "                return set(v)\n",
    "            return {v}\n",
    "\n",
    "        dt = _as_set(filters.get(\"doc_type\"))\n",
    "        tpl = _as_set(filters.get(\"template_id\"))\n",
    "        lang = _as_set(filters.get(\"language_hint\"))\n",
    "        st = _as_set(filters.get(\"status\"))\n",
    "\n",
    "        mask = np.ones(len(_chunks_meta), dtype=bool)\n",
    "        if dt is not None:\n",
    "            mask &= np.array([c[\"doc_type\"] in dt for c in _chunks_meta], dtype=bool)\n",
    "        if tpl is not None:\n",
    "            mask &= np.array([c[\"template_id\"] in tpl for c in _chunks_meta], dtype=bool)\n",
    "        if lang is not None:\n",
    "            mask &= np.array([c[\"language_hint\"] in lang for c in _chunks_meta], dtype=bool)\n",
    "        if st is not None:\n",
    "            mask &= np.array([c[\"status\"] in st for c in _chunks_meta], dtype=bool)\n",
    "\n",
    "        idxs = idxs[mask]\n",
    "        sims = sims[mask]\n",
    "\n",
    "    if len(idxs) == 0:\n",
    "        return []\n",
    "\n",
    "    k = min(top_k, len(idxs))\n",
    "    top_local = np.argpartition(-sims, k - 1)[:k]\n",
    "    top_sorted = top_local[np.argsort(-sims[top_local])]\n",
    "\n",
    "    out = []\n",
    "    for j in top_sorted:\n",
    "        meta = _chunks_meta[idxs[j]]\n",
    "        out.append({\n",
    "            \"doc_id\": meta[\"doc_id\"],\n",
    "            \"page\": meta[\"page\"],\n",
    "            \"chunk_id\": meta[\"chunk_id\"],\n",
    "            \"excerpt\": meta[\"excerpt\"],\n",
    "            \"score\": float(sims[j])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Construire l'index + test\n",
    "# -------------------------\n",
    "build_index(CORPUS)\n",
    "print(f\"[index] docs={len(CORPUS)} chunks={len(_chunks_meta)} dim={_embeddings.shape[1] if _embeddings is not None else 'NA'}\")\n",
    "\n",
    "test_query = \"tel ?\"\n",
    "hits = retrieve(test_query, top_k=3, filters={\"doc_type\": \"FACTURE\"})\n",
    "print(\"[retrieve] query:\", test_query)\n",
    "for h in hits:\n",
    "    print(f\"- score={h['score']:.4f} doc_id={h['doc_id']} page={h['page']} chunk_id={h['chunk_id']}\")\n",
    "    print(f\"  excerpt: {h['excerpt'][:180]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd69369",
   "metadata": {},
   "source": [
    "## API “chat” minimal : answer + sources (citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31f87c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"Quel est le montant TTC à payer ?\",\n",
      "  \"mode\": \"fast\",\n",
      "  \"filters_applied\": {\n",
      "    \"doc_type\": \"FACTURE\"\n",
      "  },\n",
      "  \"answer\": \"Entreprise : sarl el hana 16000 alger centre iroute de bejaia setif tel\\nMontant à payer TTC : 182,006.00 (norm=182006.00)\\nTimbre : 1,802.00 (norm=1802.00)\\nDate : 4/20/2016 (iso=2016-04-20)\",\n",
      "  \"fields\": {\n",
      "    \"invoice\": {\n",
      "      \"montant_a_payer_ttc\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"182,006.00\",\n",
      "          \"norm\": \"182006.00\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_001_TTC\",\n",
      "        \"evidence\": \"montant a payer ttc 182,006.00 \"\n",
      "      },\n",
      "      \"timbre\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"1,802.00\",\n",
      "          \"norm\": \"1802.00\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_003_TIMBRE\",\n",
      "        \"evidence\": \"rimbre 1,802.00 \"\n",
      "      },\n",
      "      \"date\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"4/20/2016\",\n",
      "          \"iso\": \"2016-04-20\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_005_DATE\",\n",
      "        \"evidence\": \"4/20/2016\"\n",
      "      },\n",
      "      \"entreprise_nom\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel\",\n",
      "          \"norm\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_007_VENDOR_NAME\",\n",
      "        \"evidence\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel \"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"audit\": {\n",
      "    \"montant_a_payer_ttc\": {\n",
      "      \"rule_id\": \"R_INV_001_TTC\",\n",
      "      \"evidence\": \"montant a payer ttc 182,006.00 \"\n",
      "    },\n",
      "    \"timbre\": {\n",
      "      \"rule_id\": \"R_INV_003_TIMBRE\",\n",
      "      \"evidence\": \"rimbre 1,802.00 \"\n",
      "    },\n",
      "    \"date\": {\n",
      "      \"rule_id\": \"R_INV_005_DATE\",\n",
      "      \"evidence\": \"4/20/2016\"\n",
      "    },\n",
      "    \"entreprise_nom\": {\n",
      "      \"rule_id\": \"R_INV_007_VENDOR_NAME\",\n",
      "      \"evidence\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel \"\n",
      "    }\n",
      "  },\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"doc_id\": \"cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_4521a4edb87f46fd\",\n",
      "      \"excerpt\": \" 19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui\",\n",
      "      \"score\": 0.34096306562423706\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_9bda55a9a431402f\",\n",
      "      \"excerpt\": \" 19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui\",\n",
      "      \"score\": 0.34096306562423706\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_54b2fa7ac9304a27\",\n",
      "      \"excerpt\": \"FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 \",\n",
      "      \"score\": 0.13209936022758484\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_7b97d3c6ee1b4b0b\",\n",
      "      \"excerpt\": \"FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 \",\n",
      "      \"score\": 0.13209936022758484\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"query\": \"Quel est le nom de l'entreprise ?\",\n",
      "  \"mode\": \"fast\",\n",
      "  \"filters_applied\": {\n",
      "    \"doc_type\": \"FACTURE\"\n",
      "  },\n",
      "  \"answer\": \"Entreprise : sarl el hana 16000 alger centre iroute de bejaia setif tel\\nMontant à payer TTC : 182,006.00 (norm=182006.00)\\nTimbre : 1,802.00 (norm=1802.00)\\nDate : 4/20/2016 (iso=2016-04-20)\",\n",
      "  \"fields\": {\n",
      "    \"invoice\": {\n",
      "      \"montant_a_payer_ttc\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"182,006.00\",\n",
      "          \"norm\": \"182006.00\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_001_TTC\",\n",
      "        \"evidence\": \"montant a payer ttc 182,006.00 \"\n",
      "      },\n",
      "      \"timbre\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"1,802.00\",\n",
      "          \"norm\": \"1802.00\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_003_TIMBRE\",\n",
      "        \"evidence\": \"rimbre 1,802.00 \"\n",
      "      },\n",
      "      \"date\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"4/20/2016\",\n",
      "          \"iso\": \"2016-04-20\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_005_DATE\",\n",
      "        \"evidence\": \"4/20/2016\"\n",
      "      },\n",
      "      \"entreprise_nom\": {\n",
      "        \"value\": {\n",
      "          \"raw\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel\",\n",
      "          \"norm\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel\"\n",
      "        },\n",
      "        \"rule_id\": \"R_INV_007_VENDOR_NAME\",\n",
      "        \"evidence\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel \"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"audit\": {\n",
      "    \"montant_a_payer_ttc\": {\n",
      "      \"rule_id\": \"R_INV_001_TTC\",\n",
      "      \"evidence\": \"montant a payer ttc 182,006.00 \"\n",
      "    },\n",
      "    \"timbre\": {\n",
      "      \"rule_id\": \"R_INV_003_TIMBRE\",\n",
      "      \"evidence\": \"rimbre 1,802.00 \"\n",
      "    },\n",
      "    \"date\": {\n",
      "      \"rule_id\": \"R_INV_005_DATE\",\n",
      "      \"evidence\": \"4/20/2016\"\n",
      "    },\n",
      "    \"entreprise_nom\": {\n",
      "      \"rule_id\": \"R_INV_007_VENDOR_NAME\",\n",
      "      \"evidence\": \"sarl el hana 16000 alger centre iroute de bejaia setif tel \"\n",
      "    }\n",
      "  },\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"doc_id\": \"cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_54b2fa7ac9304a27\",\n",
      "      \"excerpt\": \"FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 \",\n",
      "      \"score\": 0.13585996627807617\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_7b97d3c6ee1b4b0b\",\n",
      "      \"excerpt\": \"FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 \",\n",
      "      \"score\": 0.13585996627807617\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"cbcebf7f-14b0-4b0e-9bf9-36c96a7ee9bf\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_4521a4edb87f46fd\",\n",
      "      \"excerpt\": \" 19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui\",\n",
      "      \"score\": 0.02716449275612831\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"fe57bb6d-e02e-4d49-ad20-cf93b795ed78\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_9bda55a9a431402f\",\n",
      "      \"excerpt\": \" 19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui\",\n",
      "      \"score\": 0.02716449275612831\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"query\": \"Donne les lignes d'articles et vérifie les totaux\",\n",
      "  \"mode\": \"fast\",\n",
      "  \"filters_applied\": {\n",
      "    \"doc_type\": \"BON_DE_COMMANDE\"\n",
      "  },\n",
      "  \"answer\": \"Je n’ai trouvé aucun passage pertinent dans le corpus.\",\n",
      "  \"fields\": {},\n",
      "  \"audit\": {},\n",
      "  \"sources\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# API “chat” minimal : answer + sources (citations) + Rule IDs + PO line-items + vérif totaux\n",
    "# ============================\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# Cette cellule suppose que tu as déjà :\n",
    "# - retrieve(query, top_k, filters) (cellule précédente)\n",
    "# - CORPUS / result existants\n",
    "# - chunks contiennent excerpt\n",
    "# - (enforced) docs status=REVIEW ne sont pas indexés par retrieve() (cellule embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers normalisation\n",
    "# -------------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = (s or \"\").replace(\"’\", \"'\")\n",
    "    s = strip_accents(s)\n",
    "    s = s.lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def parse_number_pair(raw: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retourne {raw, norm} où norm est un float-string stable (machine).\n",
    "    \"\"\"\n",
    "    r = (raw or \"\").strip()\n",
    "    compact = r.replace(\" \", \"\")\n",
    "\n",
    "    # Cas 182,006.00 -> 182006.00\n",
    "    if \",\" in compact and \".\" in compact:\n",
    "        norm_val = compact.replace(\",\", \"\")\n",
    "        return {\"raw\": r, \"norm\": norm_val}\n",
    "\n",
    "    # Cas 182,006 -> 182.006\n",
    "    if \",\" in compact and \".\" not in compact:\n",
    "        norm_val = compact.replace(\",\", \".\")\n",
    "        return {\"raw\": r, \"norm\": norm_val}\n",
    "\n",
    "    # Cas 182006.00\n",
    "    return {\"raw\": r, \"norm\": compact}\n",
    "\n",
    "def to_float_loose(num_norm: str) -> Optional[float]:\n",
    "    if num_norm is None:\n",
    "        return None\n",
    "    s = str(num_norm).strip()\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_date_iso(date_str: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Essaie de normaliser vers ISO 8601: YYYY-MM-DD\n",
    "    Support simple:\n",
    "      - dd/mm/yyyy\n",
    "      - mm/dd/yyyy (si ambigu, on garde raw et iso null)\n",
    "    \"\"\"\n",
    "    raw = (date_str or \"\").strip()\n",
    "\n",
    "    m = re.match(r\"^(\\d{1,2})[\\/\\-](\\d{1,2})[\\/\\-](\\d{2,4})$\", raw)\n",
    "    if not m:\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    a = int(m.group(1))\n",
    "    b = int(m.group(2))\n",
    "    y = int(m.group(3))\n",
    "    if y < 100:\n",
    "        y += 2000\n",
    "\n",
    "    # Heuristique déterministe:\n",
    "    # - si a > 12 => dd/mm\n",
    "    # - si b > 12 => mm/dd\n",
    "    # - sinon ambigu => iso None\n",
    "    if a > 12 and 1 <= b <= 12:\n",
    "        dd, mm = a, b\n",
    "    elif b > 12 and 1 <= a <= 12:\n",
    "        mm, dd = a, b\n",
    "    else:\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    if not (1 <= mm <= 12 and 1 <= dd <= 31):\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    return {\"raw\": raw, \"iso\": f\"{y:04d}-{mm:02d}-{dd:02d}\"}\n",
    "\n",
    "# -------------------------\n",
    "# Audit: Rule IDs (par champ)\n",
    "# -------------------------\n",
    "RULES = {\n",
    "    \"invoice.montant_a_payer_ttc\": \"R_INV_001_TTC\",\n",
    "    \"invoice.montant_a_payer\": \"R_INV_002_PAY\",\n",
    "    \"invoice.timbre\": \"R_INV_003_TIMBRE\",\n",
    "    \"invoice.total_ttc\": \"R_INV_004_TOTAL_TTC\",\n",
    "    \"invoice.date\": \"R_INV_005_DATE\",\n",
    "    \"invoice.numero_facture\": \"R_INV_006_NUM\",\n",
    "    \"invoice.entreprise_nom\": \"R_INV_007_VENDOR_NAME\",\n",
    "    \"po.line_item\": \"R_PO_001_LINE_ROW\",\n",
    "    \"po.total_ht\": \"R_PO_010_TOTAL_HT\",\n",
    "    \"po.total_tva\": \"R_PO_011_TOTAL_TVA\",\n",
    "    \"po.total_ttc\": \"R_PO_012_TOTAL_TTC\",\n",
    "    \"po.validate_totals\": \"R_PO_090_VALIDATE_TOTALS\"\n",
    "}\n",
    "\n",
    "def mk_field(value: Any, rule_id: str, evidence: Optional[str] = None) -> Dict[str, Any]:\n",
    "    out = {\"value\": value, \"rule_id\": rule_id}\n",
    "    if evidence is not None:\n",
    "        out[\"evidence\"] = evidence\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Extraction FACTURE (déterministe + Rule IDs)\n",
    "# -------------------------\n",
    "def extract_invoice_fields(context: str) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    c_raw = context or \"\"\n",
    "    c = norm(c_raw)\n",
    "\n",
    "    # Montant à payer TTC\n",
    "    m = re.search(r\"montant\\s+a\\s+payer\\s+ttc\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\", c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"montant_a_payer_ttc\"] = mk_field(\n",
    "            value=val,\n",
    "            rule_id=RULES[\"invoice.montant_a_payer_ttc\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Montant à payer (si TTC absent)\n",
    "    m = re.search(r\"montant\\s+a\\s+payer\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\", c)\n",
    "    if m and \"montant_a_payer_ttc\" not in out:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"montant_a_payer\"] = mk_field(\n",
    "            value=val,\n",
    "            rule_id=RULES[\"invoice.montant_a_payer\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Timbre (OCR peut écrire rimbre)\n",
    "    m = re.search(r\"(timbre|rimbre)\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\", c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(2))\n",
    "        out[\"timbre\"] = mk_field(\n",
    "            value=val,\n",
    "            rule_id=RULES[\"invoice.timbre\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Total TTC\n",
    "    m = re.search(r\"total\\s+ttc\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\", c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"total_ttc\"] = mk_field(\n",
    "            value=val,\n",
    "            rule_id=RULES[\"invoice.total_ttc\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Date\n",
    "    m = re.search(r\"\\b(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4})\\b\", c)\n",
    "    if m:\n",
    "        d = normalize_date_iso(m.group(1))\n",
    "        out[\"date\"] = mk_field(\n",
    "            value=d,\n",
    "            rule_id=RULES[\"invoice.date\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Numéro facture (heuristique OCR)\n",
    "    m = re.search(r\"(numero\\s+facture|n[°o]\\s*facture|invoice\\s+number)\\s*[:\\]\\[]?\\s*([a-z0-9\\-\\/]+)\", c)\n",
    "    if m:\n",
    "        num = m.group(2)\n",
    "        out[\"numero_facture\"] = mk_field(\n",
    "            value={\"raw\": num, \"norm\": num},\n",
    "            rule_id=RULES[\"invoice.numero_facture\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "\n",
    "    # Nom entreprise (vendeur) : heuristique simple autour d'entête\n",
    "    # Cherche une ligne type \"ma petite entreprise\" / \"SARL ...\" / \"EURL ...\" / \"SPA ...\" près du début\n",
    "    head = c_raw[:700]\n",
    "    head_n = norm(head)\n",
    "    m = re.search(r\"\\b(sarl|eurl|spa|s\\.a\\.r\\.l|s\\.p\\.a)\\s+([a-z0-9' \\-]{2,})\", head_n)\n",
    "    if m:\n",
    "        vendor = (m.group(1) + \" \" + m.group(2)).strip()\n",
    "        out[\"entreprise_nom\"] = mk_field(\n",
    "            value={\"raw\": vendor, \"norm\": vendor},\n",
    "            rule_id=RULES[\"invoice.entreprise_nom\"],\n",
    "            evidence=m.group(0)\n",
    "        )\n",
    "    else:\n",
    "        # fallback: motif \"ma petite entreprise\" (exemple OCR)\n",
    "        m = re.search(r\"\\b(ma\\s+petite\\s+entreprise)\\b\", head_n)\n",
    "        if m:\n",
    "            vendor = m.group(1).strip()\n",
    "            out[\"entreprise_nom\"] = mk_field(\n",
    "                value={\"raw\": vendor, \"norm\": vendor},\n",
    "                rule_id=RULES[\"invoice.entreprise_nom\"],\n",
    "                evidence=m.group(0)\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Extraction PO line-items (6.1) + vérification totaux (déterministe)\n",
    "# -------------------------\n",
    "def _tokenize_lines(text: str) -> List[str]:\n",
    "    # Split stable (OCR)\n",
    "    t = (text or \"\").replace(\"\\r\", \"\\n\")\n",
    "    lines = [ln.strip() for ln in t.split(\"\\n\")]\n",
    "    return [ln for ln in lines if ln]\n",
    "\n",
    "def extract_po_line_items(context: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Heuristique déterministe (MVP) :\n",
    "      - détecte des lignes avec: (code?) + description + qty + pu + total\n",
    "      - renvoie items[] et totaux extraits si présents\n",
    "    Important:\n",
    "      - sur OCR bruité, on vise une extraction \"suffisante\" + audit, pas parfaite\n",
    "    \"\"\"\n",
    "    c_raw = context or \"\"\n",
    "    lines = _tokenize_lines(c_raw)\n",
    "\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    totals: Dict[str, Any] = {}\n",
    "\n",
    "    # Patterns nombres (tolérant aux séparateurs)\n",
    "    num_pat = r\"([0-9][0-9\\s,\\.]{0,})\"\n",
    "\n",
    "    # Totaux (HT/TVA/TTC) si présents dans le texte\n",
    "    c_n = norm(c_raw)\n",
    "    m = re.search(r\"\\btotal\\s+ht\\s*[:\\]\\[]?\\s*\" + num_pat, c_n)\n",
    "    if m:\n",
    "        totals[\"total_ht\"] = mk_field(parse_number_pair(m.group(1)), RULES[\"po.total_ht\"], m.group(0))\n",
    "    m = re.search(r\"\\b(total\\s+tva|montant\\s+tva|tva)\\s*[:\\]\\[]?\\s*\" + num_pat, c_n)\n",
    "    if m:\n",
    "        totals[\"total_tva\"] = mk_field(parse_number_pair(m.group(2) if m.lastindex and m.lastindex >= 2 else m.group(1)),\n",
    "                                       RULES[\"po.total_tva\"], m.group(0))\n",
    "    m = re.search(r\"\\btotal\\s+ttc\\s*[:\\]\\[]?\\s*\" + num_pat, c_n)\n",
    "    if m:\n",
    "        totals[\"total_ttc\"] = mk_field(parse_number_pair(m.group(1)), RULES[\"po.total_ttc\"], m.group(0))\n",
    "\n",
    "    # Ligne item (MVP) :\n",
    "    #  - on cherche 3 nombres sur la ligne (qty, pu, total) + du texte\n",
    "    #  - exemple OCR: \"Produit 10 1009 10.00 10,090.00\"\n",
    "    for ln in lines:\n",
    "        ln_n = norm(ln)\n",
    "\n",
    "        # Heuristique: contient mot produit/article/item ou bien contient au moins 3 nombres\n",
    "        nums = re.findall(r\"[0-9][0-9\\s,\\.]{0,}\", ln_n)\n",
    "        if len(nums) < 3:\n",
    "            continue\n",
    "\n",
    "        # on essaye de détecter qty/pu/total en prenant les 3 derniers nombres\n",
    "        qty_raw = nums[-3]\n",
    "        pu_raw = nums[-2]\n",
    "        total_raw = nums[-1]\n",
    "\n",
    "        qty = parse_number_pair(qty_raw)\n",
    "        pu = parse_number_pair(pu_raw)\n",
    "        total = parse_number_pair(total_raw)\n",
    "\n",
    "        # description = ligne sans les 3 derniers nombres (simple)\n",
    "        # (déterministe: on enlève occurrences exactes qty/pu/total dans la ligne originale)\n",
    "        desc = ln\n",
    "        for r in [qty_raw, pu_raw, total_raw]:\n",
    "            desc = re.sub(re.escape(r), \" \", desc, count=1)\n",
    "        desc = \" \".join(desc.split()).strip()\n",
    "\n",
    "        # garde-fou: description minimale\n",
    "        if len(desc) < 3:\n",
    "            continue\n",
    "\n",
    "        items.append({\n",
    "            \"description\": mk_field({\"raw\": desc, \"norm\": norm(desc)}, RULES[\"po.line_item\"], evidence=ln),\n",
    "            \"qty\": mk_field(qty, RULES[\"po.line_item\"], evidence=ln),\n",
    "            \"unit_price\": mk_field(pu, RULES[\"po.line_item\"], evidence=ln),\n",
    "            \"line_total\": mk_field(total, RULES[\"po.line_item\"], evidence=ln),\n",
    "        })\n",
    "\n",
    "    return {\"items\": items, \"totals\": totals}\n",
    "\n",
    "def validate_totals(po: Dict[str, Any], tolerance: float = 0.01) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vérif déterministe :\n",
    "      - somme(line_total) ≈ total_ht ou total_ttc si dispo\n",
    "      - renvoie {status: PASS/REVIEW, reasons[], rule_id}\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    items = po.get(\"items\", [])\n",
    "    totals = po.get(\"totals\", {})\n",
    "\n",
    "    # somme des lignes\n",
    "    sum_lines = 0.0\n",
    "    sum_ok = False\n",
    "    for it in items:\n",
    "        v = it.get(\"line_total\", {}).get(\"value\", {})\n",
    "        f = to_float_loose(v.get(\"norm\"))\n",
    "        if f is not None:\n",
    "            sum_lines += f\n",
    "            sum_ok = True\n",
    "\n",
    "    if not sum_ok:\n",
    "        return {\n",
    "            \"status\": \"REVIEW\",\n",
    "            \"reasons\": [\"NO_LINE_TOTALS_PARSED\"],\n",
    "            \"rule_id\": RULES[\"po.validate_totals\"]\n",
    "        }\n",
    "\n",
    "    # Compare à total_ht si présent, sinon total_ttc\n",
    "    target = None\n",
    "    target_name = None\n",
    "\n",
    "    if \"total_ht\" in totals:\n",
    "        target = to_float_loose(totals[\"total_ht\"][\"value\"][\"norm\"])\n",
    "        target_name = \"total_ht\"\n",
    "    elif \"total_ttc\" in totals:\n",
    "        target = to_float_loose(totals[\"total_ttc\"][\"value\"][\"norm\"])\n",
    "        target_name = \"total_ttc\"\n",
    "\n",
    "    if target is None:\n",
    "        return {\n",
    "            \"status\": \"REVIEW\",\n",
    "            \"reasons\": [\"NO_TOTAL_FIELD_FOUND\"],\n",
    "            \"rule_id\": RULES[\"po.validate_totals\"]\n",
    "        }\n",
    "\n",
    "    diff = abs(sum_lines - target)\n",
    "    if diff > tolerance:\n",
    "        reasons.append(f\"TOTAL_MISMATCH({target_name}): sum_lines={sum_lines:.2f} target={target:.2f} diff={diff:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"status\": \"PASS\" if len(reasons) == 0 else \"REVIEW\",\n",
    "        \"reasons\": reasons,\n",
    "        \"rule_id\": RULES[\"po.validate_totals\"],\n",
    "        \"details\": {\"sum_lines\": sum_lines, \"target\": target, \"target_name\": target_name, \"tolerance\": tolerance}\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Router extraction selon doc_type (déterministe)\n",
    "# -------------------------\n",
    "def extract_fields_for_doc(doc_type: str, context: str) -> Dict[str, Any]:\n",
    "    dt = (doc_type or \"\").upper().strip()\n",
    "    if dt == \"FACTURE\":\n",
    "        return {\"invoice\": extract_invoice_fields(context)}\n",
    "    if dt in (\"BON_DE_COMMANDE\", \"PO\", \"PURCHASE_ORDER\"):\n",
    "        po = extract_po_line_items(context)\n",
    "        po_validation = validate_totals(po)\n",
    "        po[\"validation\"] = po_validation\n",
    "        return {\"po\": po}\n",
    "    # fallback: rien\n",
    "    return {}\n",
    "\n",
    "# -------------------------\n",
    "# Build answer\n",
    "# -------------------------\n",
    "def build_answer(query: str, hits: List[Dict[str, Any]], mode: str, doc_type_hint: Optional[str]) -> Dict[str, Any]:\n",
    "    if not hits:\n",
    "        return {\"answer\": \"Je n’ai trouvé aucun passage pertinent dans le corpus.\", \"fields\": {}, \"audit\": {}}\n",
    "\n",
    "    context = \"\\n\".join([h[\"excerpt\"] for h in hits])\n",
    "\n",
    "    extracted = extract_fields_for_doc(doc_type_hint or \"\", context)\n",
    "\n",
    "    # FACTURE -> format réponse simple\n",
    "    if \"invoice\" in extracted and extracted[\"invoice\"]:\n",
    "        inv = extracted[\"invoice\"]\n",
    "        lines = []\n",
    "        # champs si présents\n",
    "        if \"entreprise_nom\" in inv:\n",
    "            v = inv[\"entreprise_nom\"][\"value\"]\n",
    "            lines.append(f\"Entreprise : {v['raw']}\")\n",
    "        if \"montant_a_payer_ttc\" in inv:\n",
    "            v = inv[\"montant_a_payer_ttc\"][\"value\"]\n",
    "            lines.append(f\"Montant à payer TTC : {v['raw']} (norm={v['norm']})\")\n",
    "        if \"timbre\" in inv:\n",
    "            v = inv[\"timbre\"][\"value\"]\n",
    "            lines.append(f\"Timbre : {v['raw']} (norm={v['norm']})\")\n",
    "        if \"date\" in inv:\n",
    "            d = inv[\"date\"][\"value\"]\n",
    "            if d.get(\"iso\"):\n",
    "                lines.append(f\"Date : {d['raw']} (iso={d['iso']})\")\n",
    "            else:\n",
    "                lines.append(f\"Date : {d['raw']} (iso=UNKNOWN)\")\n",
    "        if \"numero_facture\" in inv:\n",
    "            lines.append(f\"Numéro de facture : {inv['numero_facture']['value']['raw']}\")\n",
    "\n",
    "        audit = {k: {\"rule_id\": inv[k][\"rule_id\"], \"evidence\": inv[k].get(\"evidence\")} for k in inv.keys()}\n",
    "        return {\"answer\": \"\\n\".join(lines) if lines else \"Champs facture non trouvés.\", \"fields\": extracted, \"audit\": audit}\n",
    "\n",
    "    # PO -> format réponse simple\n",
    "    if \"po\" in extracted and extracted[\"po\"]:\n",
    "        po = extracted[\"po\"]\n",
    "        items = po.get(\"items\", [])\n",
    "        val = po.get(\"validation\", {})\n",
    "        lines = []\n",
    "        lines.append(f\"Lignes articles détectées : {len(items)}\")\n",
    "        # preview top 5\n",
    "        for it in items[:5]:\n",
    "            desc = it[\"description\"][\"value\"][\"raw\"]\n",
    "            qty = it[\"qty\"][\"value\"][\"raw\"]\n",
    "            pu = it[\"unit_price\"][\"value\"][\"raw\"]\n",
    "            tot = it[\"line_total\"][\"value\"][\"raw\"]\n",
    "            lines.append(f\"- {desc} | qty={qty} | pu={pu} | total={tot}\")\n",
    "\n",
    "        if val:\n",
    "            lines.append(f\"Vérif totaux : {val.get('status')} (rule_id={val.get('rule_id')})\")\n",
    "            for r in val.get(\"reasons\", []):\n",
    "                lines.append(f\"  - {r}\")\n",
    "\n",
    "        audit = {\n",
    "            \"po\": {\n",
    "                \"line_item_rule_id\": RULES[\"po.line_item\"],\n",
    "                \"validate_totals_rule_id\": RULES[\"po.validate_totals\"]\n",
    "            }\n",
    "        }\n",
    "        return {\"answer\": \"\\n\".join(lines), \"fields\": extracted, \"audit\": audit}\n",
    "\n",
    "    # fallback extractif\n",
    "    n = 3 if mode == \"fast\" else 5\n",
    "    answer = \"Passages pertinents trouvés :\\n\" + \"\\n\".join(\n",
    "        [f\"- {h['excerpt'][:220]}...\" for h in hits[:n]]\n",
    "    )\n",
    "    return {\"answer\": answer, \"fields\": {}, \"audit\": {}}\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    mode: str = \"fast\",\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    "    top_k_fast: int = 5,\n",
    "    top_k_normal: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    mode = (mode or \"fast\").lower().strip()\n",
    "    top_k = top_k_fast if mode == \"fast\" else top_k_normal\n",
    "\n",
    "    hits = retrieve(query, top_k=top_k, filters=filters)\n",
    "\n",
    "    # doc_type_hint (déterministe) : si filter doc_type présent, on l'utilise\n",
    "    doc_type_hint = None\n",
    "    if filters and \"doc_type\" in filters:\n",
    "        dt = filters[\"doc_type\"]\n",
    "        if isinstance(dt, str):\n",
    "            doc_type_hint = dt\n",
    "        elif isinstance(dt, list) and dt:\n",
    "            doc_type_hint = dt[0]\n",
    "\n",
    "    built = build_answer(query, hits, mode, doc_type_hint=doc_type_hint)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"mode\": mode,\n",
    "        \"filters_applied\": filters or {},\n",
    "        \"answer\": built[\"answer\"],\n",
    "        \"fields\": built[\"fields\"],\n",
    "        \"audit\": built[\"audit\"],      # Rule IDs + evidence\n",
    "        \"sources\": hits               # citations: doc_id/page/chunk_id/excerpt/score\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Tests\n",
    "# -------------------------\n",
    "q1 = \"Quel est le montant TTC à payer ?\"\n",
    "resp1 = ask(q1, mode=\"fast\", filters={\"doc_type\": \"FACTURE\"})\n",
    "print(json.dumps(resp1, ensure_ascii=False, indent=2))\n",
    "\n",
    "q2 = \"Quel est le nom de l'entreprise ?\"\n",
    "resp2 = ask(q2, mode=\"fast\", filters={\"doc_type\": \"FACTURE\"})\n",
    "print(json.dumps(resp2, ensure_ascii=False, indent=2))\n",
    "\n",
    "# PO (si tu as des docs classés BON_DE_COMMANDE dans CORPUS)\n",
    "q3 = \"Donne les lignes d'articles et vérifie les totaux\"\n",
    "resp3 = ask(q3, mode=\"fast\", filters={\"doc_type\": \"BON_DE_COMMANDE\"})\n",
    "print(json.dumps(resp3, ensure_ascii=False, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
