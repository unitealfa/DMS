{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b6c389",
   "metadata": {},
   "source": [
    "## Pipeline OCR (Tesseract + OpenCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d3df9",
   "metadata": {},
   "source": [
    "Langue (par défaut FR, mais bascule en EN si détecté)\n",
    "\n",
    "Par défaut, l’OCR est en français :\n",
    "\n",
    "DEFAULT_LANG = \"fra\" (côté Tesseract)\n",
    "\n",
    "spacy.load(\"fr_core_news_sm\", ...) (côté spaCy)\n",
    "\n",
    "Si tu détectes que le texte est en anglais, tu fais basculer :\n",
    "\n",
    "DEFAULT_LANG = \"eng\" (ou fra+eng si tu veux tolérer les deux)\n",
    "\n",
    "spacy.load(\"en_core_web_sm\", ...)\n",
    "\n",
    "Trucs à modifier quand tu changes de langue :\n",
    "\n",
    "la constante DEFAULT_LANG\n",
    "\n",
    "le modèle spaCy chargé (fr_core_news_sm ↔ en_core_web_sm)\n",
    "\n",
    "---\n",
    "\n",
    "Fonctionnement global du script\n",
    "\n",
    "Prendre une image (INPUT_FILE)\n",
    "\n",
    "L’améliorer via le prétraitement (gris, upscale, contraste/sharpness, seuil, etc.)\n",
    "\n",
    "Lancer Tesseract sur l’image prétraitée pour extraire le texte (OCR_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1b1ed",
   "metadata": {},
   "source": [
    "### importation img et prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b767eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "INPUT_FILE: Optional[str] = \"image2tab.webp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c916ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[info] Using INPUT_FILE=C:\\Users\\moura\\OneDrive\\Bureau\\DMS\\test\\image2tab.webp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies:\n",
    "  * Python 3.8+\n",
    "  * pytesseract\n",
    "  * pillow\n",
    "  * Tesseract binary with tessdata\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    np = None\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In notebooks __file__ is undefined; fall back to current working directory.\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DEFAULT_LANG = \"fra\"\n",
    "DEFAULT_CONTRAST = 1.5\n",
    "DEFAULT_SHARPNESS = 1.2\n",
    "DEFAULT_BRIGHTNESS = 1.0\n",
    "DEFAULT_UPSCALE = 1.5\n",
    "DEFAULT_DPI = 300\n",
    "\n",
    " #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "SHOW_PREPROCESSED = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhanceOptions:\n",
    "    contrast: float = DEFAULT_CONTRAST\n",
    "    sharpness: float = DEFAULT_SHARPNESS\n",
    "    brightness: float = DEFAULT_BRIGHTNESS\n",
    "    upscale: float = DEFAULT_UPSCALE\n",
    "    gamma: Optional[float] = None  # gamma correction; <1 brightens darks, >1 darkens\n",
    "    pad: int = 0  # pixels to pad around the image\n",
    "    median: Optional[int] = None  # kernel size for median filter (odd int, e.g., 3)\n",
    "    unsharp_radius: Optional[float] = None  # e.g., 1.0\n",
    "    unsharp_percent: int = 150\n",
    "    invert: bool = False\n",
    "    autocontrast_cutoff: Optional[int] = None  # 0-100; percentage to clip for autocontrast\n",
    "    equalize: bool = False  # histogram equalization\n",
    "    auto_rotate: bool = False  # attempt orientation detection + rotate\n",
    "    otsu: bool = False  # auto-threshold with Otsu (requires numpy)\n",
    "    threshold: Optional[int] = None  # 0-255; if set, applies a binary threshold\n",
    "\n",
    "\n",
    "def build_config(\n",
    "    oem: Optional[int],\n",
    "    psm: Optional[int],\n",
    "    base_flags: Iterable[str],\n",
    "    dpi: Optional[int],\n",
    "    tessdata_dir: Optional[Path],\n",
    "    user_words: Optional[Path],\n",
    "    user_patterns: Optional[Path],\n",
    ") -> str:\n",
    "    parts: List[str] = []\n",
    "    if oem is not None:\n",
    "        parts.append(f\"--oem {oem}\")\n",
    "    if psm is not None:\n",
    "        parts.append(f\"--psm {psm}\")\n",
    "    if dpi is not None:\n",
    "        parts.append(f\"--dpi {dpi}\")\n",
    "    if tessdata_dir is not None:\n",
    "        parts.append(f'--tessdata-dir \"{tessdata_dir}\"')\n",
    "    if user_words is not None:\n",
    "        parts.append(f'--user-words \"{user_words}\"')\n",
    "    if user_patterns is not None:\n",
    "        parts.append(f'--user-patterns \"{user_patterns}\"')\n",
    "    parts.extend(base_flags)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def ensure_environment(lang: str) -> None:\n",
    "    try:\n",
    "        _ = pytesseract.get_tesseract_version()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        sys.exit(\"Tesseract binary not found on PATH. Install it and its language data.\")\n",
    "    if lang:\n",
    "        try:\n",
    "            available = set(pytesseract.get_languages(config=\"\"))\n",
    "            requested = set(lang.split(\"+\"))\n",
    "            missing = requested - available\n",
    "            if missing:\n",
    "                print(\n",
    "                    f\"Warning: missing languages: {', '.join(sorted(missing))}. \"\n",
    "                    f\"Available: {', '.join(sorted(available))}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "        except pytesseract.TesseractError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def auto_rotate_if_needed(img: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    if not enhance.auto_rotate:\n",
    "        return img\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        angle = None\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                try:\n",
    "                    angle = int(line.split(\":\")[1].strip())\n",
    "                except ValueError:\n",
    "                    angle = None\n",
    "                break\n",
    "        if angle is not None and angle % 360 != 0:\n",
    "            return img.rotate(-angle, expand=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image: Image.Image, enhance: EnhanceOptions) -> Image.Image:\n",
    "    img = image.convert(\"L\")\n",
    "    img = auto_rotate_if_needed(img, enhance)\n",
    "\n",
    "    if enhance.invert:\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    if enhance.pad and enhance.pad > 0:\n",
    "        img = ImageOps.expand(img, border=enhance.pad, fill=255)\n",
    "\n",
    "    if enhance.autocontrast_cutoff is not None:\n",
    "        cutoff = max(0, min(100, enhance.autocontrast_cutoff))\n",
    "        img = ImageOps.autocontrast(img, cutoff=cutoff)\n",
    "\n",
    "    if enhance.equalize:\n",
    "        img = ImageOps.equalize(img)\n",
    "\n",
    "    if enhance.upscale and enhance.upscale != 1.0:\n",
    "        w, h = img.size\n",
    "        img = img.resize((int(w * enhance.upscale), int(h * enhance.upscale)), Image.LANCZOS)\n",
    "\n",
    "    if enhance.gamma and enhance.gamma > 0:\n",
    "        inv_gamma = 1.0 / enhance.gamma\n",
    "        lut = [pow(x / 255.0, inv_gamma) * 255 for x in range(256)]\n",
    "        img = img.point(lut)\n",
    "\n",
    "    if enhance.brightness and enhance.brightness != 1.0:\n",
    "        img = ImageEnhance.Brightness(img).enhance(enhance.brightness)\n",
    "\n",
    "    if enhance.contrast and enhance.contrast != 1.0:\n",
    "        img = ImageEnhance.Contrast(img).enhance(enhance.contrast)\n",
    "\n",
    "    if enhance.sharpness and enhance.sharpness != 1.0:\n",
    "        img = ImageEnhance.Sharpness(img).enhance(enhance.sharpness)\n",
    "\n",
    "    if enhance.unsharp_radius:\n",
    "        img = img.filter(\n",
    "            ImageFilter.UnsharpMask(\n",
    "                radius=enhance.unsharp_radius,\n",
    "                percent=enhance.unsharp_percent,\n",
    "                threshold=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if enhance.median and enhance.median > 1 and enhance.median % 2 == 1:\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=enhance.median))\n",
    "\n",
    "    if enhance.threshold is not None:\n",
    "        thr = max(0, min(255, enhance.threshold))\n",
    "        img = img.point(lambda p, t=thr: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "    elif enhance.otsu and np is not None:\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        hist, _ = np.histogram(arr, bins=256, range=(0, 256))\n",
    "        total = arr.size\n",
    "        sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "        sum_b = 0.0\n",
    "        w_b = 0.0\n",
    "        max_var = 0.0\n",
    "        threshold = 0\n",
    "\n",
    "        for i in range(256):\n",
    "            w_b += hist[i]\n",
    "            if w_b == 0:\n",
    "                continue\n",
    "            w_f = total - w_b\n",
    "            if w_f == 0:\n",
    "                break\n",
    "            sum_b += i * hist[i]\n",
    "            m_b = sum_b / w_b\n",
    "            m_f = (sum_total - sum_b) / w_f\n",
    "            var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "            if var_between > max_var:\n",
    "                max_var = var_between\n",
    "                threshold = i\n",
    "\n",
    "        img = img.point(lambda p, t=threshold: 255 if p > t else 0, mode=\"1\").convert(\"L\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-l\", \"--lang\", default=DEFAULT_LANG)\n",
    "    parser.add_argument(\"--oem\", type=int, choices=range(0, 4), default=None)\n",
    "    parser.add_argument(\"--psm\", type=int, choices=range(0, 14), default=None)\n",
    "    parser.add_argument(\"--dpi\", type=int, default=DEFAULT_DPI)\n",
    "    parser.add_argument(\"--tessdata-dir\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-words\", type=Path, default=None)\n",
    "    parser.add_argument(\"--user-patterns\", type=Path, default=None)\n",
    "    parser.add_argument(\"--whitelist\", type=str, default=None)\n",
    "    parser.add_argument(\"--blacklist\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--contrast\", type=float, default=DEFAULT_CONTRAST)\n",
    "    parser.add_argument(\"--sharpness\", type=float, default=DEFAULT_SHARPNESS)\n",
    "    parser.add_argument(\"--brightness\", type=float, default=DEFAULT_BRIGHTNESS)\n",
    "    parser.add_argument(\"--upscale\", type=float, default=DEFAULT_UPSCALE)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--pad\", type=int, default=0)\n",
    "    parser.add_argument(\"--threshold\", type=int, default=None)\n",
    "    parser.add_argument(\"--median\", type=int, default=None)\n",
    "    parser.add_argument(\"--unsharp-radius\", type=float, default=None)\n",
    "    parser.add_argument(\"--unsharp-percent\", type=int, default=150)\n",
    "    parser.add_argument(\"--invert\", action=\"store_true\")\n",
    "    parser.add_argument(\"--autocontrast-cutoff\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--auto-rotate\", action=\"store_true\")\n",
    "    parser.add_argument(\"--otsu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"*\",\n",
    "        default=[],\n",
    "        metavar=\"CFG\",\n",
    "        help=\"Additional configuration flags passed verbatim to tesseract (e.g., -c foo=bar).\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args(list(argv) if argv is not None else [])\n",
    "\n",
    "\n",
    "# --------- Exécution Cellule 1 (jusqu’à l’affichage) ---------\n",
    "\n",
    "args = parse_args()\n",
    "ensure_environment(args.lang)\n",
    "\n",
    "enhance = EnhanceOptions(\n",
    "    contrast=args.contrast,\n",
    "    sharpness=args.sharpness,\n",
    "    brightness=args.brightness,\n",
    "    upscale=args.upscale,\n",
    "    gamma=args.gamma,\n",
    "    pad=args.pad,\n",
    "    median=args.median,\n",
    "    unsharp_radius=args.unsharp_radius,\n",
    "    unsharp_percent=args.unsharp_percent,\n",
    "    invert=args.invert,\n",
    "    autocontrast_cutoff=args.autocontrast_cutoff,\n",
    "    equalize=args.equalize,\n",
    "    auto_rotate=args.auto_rotate,\n",
    "    otsu=args.otsu,\n",
    "    threshold=args.threshold,\n",
    ")\n",
    "\n",
    "config_flags: List[str] = list(args.config)\n",
    "if args.whitelist:\n",
    "    config_flags.append(f\"-c tessedit_char_whitelist={args.whitelist}\")\n",
    "if args.blacklist:\n",
    "    config_flags.append(f\"-c tessedit_char_blacklist={args.blacklist}\")\n",
    "\n",
    "if not INPUT_FILE:\n",
    "    sys.exit(\"INPUT_FILE is not set. Put your image filename in INPUT_FILE.\")\n",
    "\n",
    "path = Path(INPUT_FILE)\n",
    "if not path.is_absolute():\n",
    "    path = (SCRIPT_DIR / path).resolve()\n",
    "\n",
    "if not path.exists():\n",
    "    sys.exit(f\"INPUT_FILE not found: {path}\")\n",
    "\n",
    "print(f\"[info] Using INPUT_FILE={path}\", file=sys.stderr)\n",
    "\n",
    "original = Image.open(path)\n",
    "prepped = preprocess_image(original, enhance)\n",
    "\n",
    "# Afficher les 2 images (original + prétraitée)\n",
    "original.show(title=\"original\")\n",
    "if \"SHOW_PREPROCESSED\" not in globals() or SHOW_PREPROCESSED:\n",
    "    prepped.show(title=\"preprocessed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6eeae",
   "metadata": {},
   "source": [
    "### tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9199410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTURE\n",
      "\n",
      "CODE CLENT NUMERO\n",
      "FCo0o1 4/20/2016 0002\n",
      "Ma petite entreprise CLIENT\n",
      "19,rue de place 1° mai SARL EL HANA\n",
      "16000 Alger Centre IROUTE DE BEJAIA SETIF\n",
      "Tel : 00-00-52-12- 119000\n",
      "Ident Fiscal : 160\n",
      "N°art : 160100000000\n",
      "Mode de paiement : Espèce\n",
      "Date Échéance : 5/20/2016\n",
      "Référence Description Produit Quantité P.Unitaire Valeur\n",
      "cl001 _Produit1 1000 1.00 1,000.00\n",
      "c1002 _ |Produit 2 1001 2.00 2,002.00\n",
      "c1003 _ jProduit 3 1002 3.00 3,006.00\n",
      "c1004 _ |Produit4 1003 4.00 4,012.00\n",
      "c1005 __|Produit5 1004 5.00 5,020.00\n",
      "c1006 _ |Produit 6 1005 6.00 6,030.00\n",
      "c1007 _ |Produit 7 1006 11.00 11,066.00\n",
      "c1008 Produit8 1007 118.00 118,826.00\n",
      "c1009 Produit 9 1008 19.00 19,152.00\n",
      "c1010 _ |Produit 10 1009 10.00 10,090.00\n",
      "Non assujetti à latva [Montant à payer 180,204.00\n",
      "[rimbre 1,802.00\n",
      "Montant à payer ttc 182,006.00\n",
      "\n",
      "Monatnt Facture enLettre … Cinq mille huit cent quatre vingt huit Dinars Algériens\n",
      "\n",
      "Cachet & Signature\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = build_config(\n",
    "    args.oem,\n",
    "    args.psm,\n",
    "    config_flags,\n",
    "    args.dpi,\n",
    "    args.tessdata_dir,\n",
    "    args.user_words,\n",
    "    args.user_patterns,\n",
    ")\n",
    "\n",
    "OCR_TEXT = pytesseract.image_to_string(prepped, lang=args.lang, config=config)\n",
    "print(OCR_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d74de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c33836",
   "metadata": {},
   "source": [
    "### Pipeline SpaCy de base & Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1b445",
   "metadata": {},
   "source": [
    "modifer pour la langue :fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cbf1730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phrase : FACTURE\n",
      "\n",
      "CODE CLENT NUMERO\n",
      "FCo0o1 4/20/2016 0002\n",
      "Ma petite entreprise CLIENT\n",
      "19,rue de place 1° mai SARL EL HANA\n",
      "16000 Alger Centre IROUTE DE BEJAIA SETIF\n",
      "Tel : 00-00-52-12- 119000\n",
      "Ident Fiscal : 160\n",
      "N°art : 160100000000\n",
      "Mode de paiement : Espèce\n",
      "Date Échéance : 5/20/2016\n",
      "Référence Description Produit Quantité P.Unitaire Valeur\n",
      "cl001 _Produit1 1000 1.00 1,000.00\n",
      "c1002 _ |Produit 2 1001 2.00 2,002.00\n",
      "c1003 _ jProduit 3 1002 3.00 3,006.00\n",
      "c1004 _ |Produit4 1003 4.00 4,012.00\n",
      "c1005 __|Produit5 1004 5.00 5,020.00\n",
      "c1006 _ |Produit 6 1005 6.00 6,030.00\n",
      "c1007 _ |Produit 7 1006 11.00 11,066.00\n",
      "c1008 Produit8 1007 118.00 118,826.00\n",
      "c1009 Produit 9 1008 19.00 19,152.00\n",
      "c1010 _ |Produit 10 1009 10.00 10,090.00\n",
      "Non assujetti à latva [Montant à payer 180,204.00\n",
      "[rimbre 1,802.00\n",
      "Montant à payer ttc 182,006.00\n",
      "\n",
      "Monatnt Facture enLettre … Cinq mille huit cent quatre vingt huit Dinars Algériens\n",
      "\n",
      "Cachet & Signature\n",
      "Langue : fr\n",
      "Tokens : ['FACTURE', '\\n\\n', 'CODE', 'CLENT', 'NUMERO', '\\n', 'FCo0o1', '4/20/2016', '0002', '\\n', 'Ma', 'petite', 'entreprise', 'CLIENT', '\\n', '19,rue', 'de', 'place', '1', '°', 'mai', 'SARL', 'EL', 'HANA', '\\n', '16000', 'Alger', 'Centre', 'IROUTE', 'DE', 'BEJAIA', 'SETIF', '\\n', 'Tel', ':', '00', '-', '00', '-', '52', '-', '12-', '119000', '\\n', 'Ident', 'Fiscal', ':', '160', '\\n', 'N', '°', 'art', ':', '160100000000', '\\n', 'Mode', 'de', 'paiement', ':', 'Espèce', '\\n', 'Date', 'Échéance', ':', '5/20/2016', '\\n', 'Référence', 'Description', 'Produit', 'Quantité', 'P.Unitaire', 'Valeur', '\\n', 'cl001', '_', 'Produit1', '1000', '1.00', '1,000.00', '\\n', 'c1002', '_', '|Produit', '2', '1001', '2.00', '2,002.00', '\\n', 'c1003', '_', 'jProduit', '3', '1002', '3.00', '3,006.00', '\\n', 'c1004', '_', '|Produit4', '1003', '4.00', '4,012.00', '\\n', 'c1005', '_', '_', '|Produit5', '1004', '5.00', '5,020.00', '\\n', 'c1006', '_', '|Produit', '6', '1005', '6.00', '6,030.00', '\\n', 'c1007', '_', '|Produit', '7', '1006', '11.00', '11,066.00', '\\n', 'c1008', 'Produit8', '1007', '118.00', '118,826.00', '\\n', 'c1009', 'Produit', '9', '1008', '19.00', '19,152.00', '\\n', 'c1010', '_', '|Produit', '10', '1009', '10.00', '10,090.00', '\\n', 'Non', 'assujetti', 'à', 'latva', '[', 'Montant', 'à', 'payer', '180,204.00', '\\n', '[', 'rimbre', '1,802.00', '\\n', 'Montant', 'à', 'payer', 'ttc', '182,006.00', '\\n\\n', 'Monatnt', 'Facture', 'enLettre', '…', 'Cinq', 'mille', 'huit', 'cent', 'quatre', 'vingt', 'huit', 'Dinars', 'Algériens', '\\n\\n', 'Cachet', '&', 'Signature']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "texte = OCR_TEXT\n",
    "\n",
    "# 1) détecter la langue sur un gros extrait (plus stable et plus rapide)\n",
    "sample = texte[:2000]  \n",
    "try:\n",
    "    doc_lang = detect(sample)\n",
    "except:\n",
    "    doc_lang = \"fr\"  # si probleme avec detection de langue (on forcer fr)\n",
    " \n",
    "# 2) charger UN seul modèle\n",
    "nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "# 3) split phrases rapide\n",
    "sent_split = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "for phrase in sent_split.split(texte):\n",
    "    phrase = phrase.strip()\n",
    "    if len(phrase) < 20:\n",
    "        continue\n",
    "\n",
    "    # tokenisation spaCy (mais pipeline ultra léger)\n",
    "    doc = nlp.make_doc(phrase)\n",
    "    print(\"\\nPhrase :\", phrase)\n",
    "    print(\"Langue :\", doc_lang)\n",
    "    print(\"Tokens :\", [t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae6df1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38b5574",
   "metadata": {},
   "source": [
    "## Schéma de BDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3da0aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"mmd-103c33d75ead42a08fe0f5513f6bc341\" class=\"mermaid\">\n",
       "%%{init: {\"theme\": \"redux-dark-color\", \"layout\": \"elk\"} }%%\n",
       "erDiagram\n",
       "    ROLES {\n",
       "        INT id PK\n",
       "        VARCHAR name\n",
       "    }\n",
       "\n",
       "    USERS {\n",
       "        INT id PK\n",
       "        VARCHAR username\n",
       "        VARCHAR email\n",
       "        VARCHAR password_hash\n",
       "        INT role_id FK\n",
       "        DATETIME created_at\n",
       "    }\n",
       "\n",
       "    DOMAINS {\n",
       "        INT id PK\n",
       "        VARCHAR name\n",
       "    }\n",
       "\n",
       "    RULE_CONFIGS {\n",
       "        INT id PK\n",
       "        INT domain_id FK\n",
       "        VARCHAR version_label_Regex\n",
       "        JSONB Regex_json\n",
       "        INT created_by FK\n",
       "        DATETIME created_at\n",
       "        BOOLEAN is_active\n",
       "    }\n",
       "\n",
       "    %% Nouvelle table: \"API\" = profil/config par domaine (langue, règles, paramètres)\n",
       "    APIS {\n",
       "        INT id PK\n",
       "        INT domain_id FK\n",
       "        INT rule_config_id FK\n",
       "        VARCHAR name\n",
       "        VARCHAR language_code          \n",
       "        JSONB settings_json           \n",
       "        BOOLEAN is_active\n",
       "        INT created_by FK\n",
       "        DATETIME created_at\n",
       "    }\n",
       "\n",
       "    %% Clés par API (une API/profil peut avoir plusieurs clés d'accès)\n",
       "    API_KEYS {\n",
       "        INT id PK\n",
       "        INT api_id FK\n",
       "        VARCHAR key_hash              \n",
       "        JSONB scopes                  \n",
       "        DATETIME created_at\n",
       "        DATETIME last_used_at\n",
       "        DATETIME expires_at\n",
       "        DATETIME revoked_at\n",
       "    }\n",
       "\n",
       "    DOCUMENTS {\n",
       "        UUID id PK\n",
       "        VARCHAR filename\n",
       "        INT domain_id FK\n",
       "        VARCHAR status\n",
       "        VARCHAR empreinte_numerique\n",
       "        DATETIME uploaded_at\n",
       "        INT uploaded_by FK\n",
       "        INT api_id FK                 \n",
       "    }\n",
       "\n",
       "    FILE_STORAGE {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        VARCHAR object_path\n",
       "        INT size\n",
       "        VARCHAR empreinte_numerique\n",
       "        DATETIME stored_at\n",
       "    }\n",
       "\n",
       "    EXTRACTIONS {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        INT rule_config_id FK\n",
       "        VARCHAR field_name\n",
       "        TEXT extracted_value\n",
       "        JSONB coordinates\n",
       "        BOOLEAN is_valid\n",
       "        BOOLEAN is_overridden\n",
       "        INT overridden_by FK\n",
       "        DATETIME overridden_at\n",
       "    }\n",
       "\n",
       "    QUALITY_GATE_LOGS {\n",
       "        INT id PK\n",
       "        UUID document_id FK\n",
       "        BOOLEAN is_passed\n",
       "        TEXT failure_reason\n",
       "        DATETIME checked_at\n",
       "        VARCHAR check_origin          \n",
       "        INT checked_by FK            \n",
       "        BOOLEAN is_final_decision    \n",
       "        TEXT decision_comment\n",
       "    }\n",
       "\n",
       "    AUDIT_LOGS {\n",
       "        INT id PK\n",
       "        INT user_id FK\n",
       "        UUID document_id FK\n",
       "        VARCHAR action\n",
       "        VARCHAR entity_type\n",
       "        INT entity_id\n",
       "        JSONB changes\n",
       "        DATETIME timestamp\n",
       "        VARCHAR ip_address\n",
       "    }\n",
       "\n",
       "    %% Relations façon Workbench\n",
       "    ROLES ||--o{ USERS : \"role_id\"\n",
       "    USERS ||--o{ RULE_CONFIGS : \"created_by\"\n",
       "    USERS ||--o{ AUDIT_LOGS : \"user_id\"\n",
       "\n",
       "    DOMAINS ||--o{ RULE_CONFIGS : \"domain_id\"\n",
       "\n",
       "    %% Domaine -> APIs (plusieurs APIs dans le même domaine)\n",
       "    DOMAINS ||--o{ APIS : \"domain_id\"\n",
       "    RULE_CONFIGS ||--o{ APIS : \"rule_config_id\"\n",
       "    USERS ||--o{ APIS : \"created_by\"\n",
       "\n",
       "    %% API -> API_KEYS (plusieurs clés par API)\n",
       "    APIS ||--o{ API_KEYS : \"api_id\"\n",
       "\n",
       "    %% Domaine -> Documents\n",
       "    DOMAINS ||--o{ DOCUMENTS : \"domain_id\"\n",
       "    USERS ||--o{ DOCUMENTS : \"uploaded_by\"\n",
       "    APIS ||--o{ DOCUMENTS : \"api_id\"\n",
       "\n",
       "    %% Documents -> stockage + extractions + quality\n",
       "    DOCUMENTS ||--|| FILE_STORAGE : \"document_id\"\n",
       "    DOCUMENTS ||--o{ EXTRACTIONS : \"document_id\"\n",
       "    RULE_CONFIGS ||--o{ EXTRACTIONS : \"rule_config_id\"\n",
       "    USERS ||--o{ EXTRACTIONS : \"overridden_by\"\n",
       "\n",
       "    DOCUMENTS ||--o{ QUALITY_GATE_LOGS : \"document_id\"\n",
       "    USERS ||--o{ QUALITY_GATE_LOGS : \"checked_by\"\n",
       "\n",
       "    DOCUMENTS ||--o{ AUDIT_LOGS : \"document_id\"\n",
       "</div>\n",
       "\n",
       "<script type=\"module\">\n",
       "  const render = async () => {\n",
       "    if (!window.__mermaid_loaded__) {\n",
       "      const mermaid = (await import(\"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\")).default;\n",
       "      window.mermaid = mermaid;\n",
       "      window.__mermaid_loaded__ = true;\n",
       "      mermaid.initialize({\n",
       "        startOnLoad: false,\n",
       "        securityLevel: \"loose\"\n",
       "      });\n",
       "    }\n",
       "    await window.mermaid.run({\n",
       "      nodes: [document.getElementById(\"mmd-103c33d75ead42a08fe0f5513f6bc341\")]\n",
       "    });\n",
       "  };\n",
       "  render();\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re, json, uuid\n",
    "\n",
    "raw = r\"\"\"\n",
    "---\n",
    "config:\n",
    "  layout: elk\n",
    "  theme: redux-dark-color\n",
    "---\n",
    "\n",
    "erDiagram\n",
    "    ROLES {\n",
    "        INT id PK\n",
    "        VARCHAR name\n",
    "    }\n",
    "\n",
    "    USERS {\n",
    "        INT id PK\n",
    "        VARCHAR username\n",
    "        VARCHAR email\n",
    "        VARCHAR password_hash\n",
    "        INT role_id FK\n",
    "        DATETIME created_at\n",
    "    }\n",
    "\n",
    "    DOMAINS {\n",
    "        INT id PK\n",
    "        VARCHAR name\n",
    "    }\n",
    "\n",
    "    RULE_CONFIGS {\n",
    "        INT id PK\n",
    "        INT domain_id FK\n",
    "        VARCHAR version_label_Regex\n",
    "        JSONB Regex_json\n",
    "        INT created_by FK\n",
    "        DATETIME created_at\n",
    "        BOOLEAN is_active\n",
    "    }\n",
    "\n",
    "    %% Nouvelle table: \"API\" = profil/config par domaine (langue, règles, paramètres)\n",
    "    APIS {\n",
    "        INT id PK\n",
    "        INT domain_id FK\n",
    "        INT rule_config_id FK\n",
    "        VARCHAR name\n",
    "        VARCHAR language_code          \n",
    "        JSONB settings_json           \n",
    "        BOOLEAN is_active\n",
    "        INT created_by FK\n",
    "        DATETIME created_at\n",
    "    }\n",
    "\n",
    "    %% Clés par API (une API/profil peut avoir plusieurs clés d'accès)\n",
    "    API_KEYS {\n",
    "        INT id PK\n",
    "        INT api_id FK\n",
    "        VARCHAR key_hash              \n",
    "        JSONB scopes                  \n",
    "        DATETIME created_at\n",
    "        DATETIME last_used_at\n",
    "        DATETIME expires_at\n",
    "        DATETIME revoked_at\n",
    "    }\n",
    "\n",
    "    DOCUMENTS {\n",
    "        UUID id PK\n",
    "        VARCHAR filename\n",
    "        INT domain_id FK\n",
    "        VARCHAR status\n",
    "        VARCHAR empreinte_numerique\n",
    "        DATETIME uploaded_at\n",
    "        INT uploaded_by FK\n",
    "        INT api_id FK                 \n",
    "    }\n",
    "\n",
    "    FILE_STORAGE {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        VARCHAR object_path\n",
    "        INT size\n",
    "        VARCHAR empreinte_numerique\n",
    "        DATETIME stored_at\n",
    "    }\n",
    "\n",
    "    EXTRACTIONS {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        INT rule_config_id FK\n",
    "        VARCHAR field_name\n",
    "        TEXT extracted_value\n",
    "        JSONB coordinates\n",
    "        BOOLEAN is_valid\n",
    "        BOOLEAN is_overridden\n",
    "        INT overridden_by FK\n",
    "        DATETIME overridden_at\n",
    "    }\n",
    "\n",
    "    QUALITY_GATE_LOGS {\n",
    "        INT id PK\n",
    "        UUID document_id FK\n",
    "        BOOLEAN is_passed\n",
    "        TEXT failure_reason\n",
    "        DATETIME checked_at\n",
    "        VARCHAR check_origin          \n",
    "        INT checked_by FK            \n",
    "        BOOLEAN is_final_decision    \n",
    "        TEXT decision_comment\n",
    "    }\n",
    "\n",
    "    AUDIT_LOGS {\n",
    "        INT id PK\n",
    "        INT user_id FK\n",
    "        UUID document_id FK\n",
    "        VARCHAR action\n",
    "        VARCHAR entity_type\n",
    "        INT entity_id\n",
    "        JSONB changes\n",
    "        DATETIME timestamp\n",
    "        VARCHAR ip_address\n",
    "    }\n",
    "\n",
    "    %% Relations façon Workbench\n",
    "    ROLES ||--o{ USERS : \"role_id\"\n",
    "    USERS ||--o{ RULE_CONFIGS : \"created_by\"\n",
    "    USERS ||--o{ AUDIT_LOGS : \"user_id\"\n",
    "\n",
    "    DOMAINS ||--o{ RULE_CONFIGS : \"domain_id\"\n",
    "\n",
    "    %% Domaine -> APIs (plusieurs APIs dans le même domaine)\n",
    "    DOMAINS ||--o{ APIS : \"domain_id\"\n",
    "    RULE_CONFIGS ||--o{ APIS : \"rule_config_id\"\n",
    "    USERS ||--o{ APIS : \"created_by\"\n",
    "\n",
    "    %% API -> API_KEYS (plusieurs clés par API)\n",
    "    APIS ||--o{ API_KEYS : \"api_id\"\n",
    "\n",
    "    %% Domaine -> Documents\n",
    "    DOMAINS ||--o{ DOCUMENTS : \"domain_id\"\n",
    "    USERS ||--o{ DOCUMENTS : \"uploaded_by\"\n",
    "    APIS ||--o{ DOCUMENTS : \"api_id\"\n",
    "\n",
    "    %% Documents -> stockage + extractions + quality\n",
    "    DOCUMENTS ||--|| FILE_STORAGE : \"document_id\"\n",
    "    DOCUMENTS ||--o{ EXTRACTIONS : \"document_id\"\n",
    "    RULE_CONFIGS ||--o{ EXTRACTIONS : \"rule_config_id\"\n",
    "    USERS ||--o{ EXTRACTIONS : \"overridden_by\"\n",
    "\n",
    "    DOCUMENTS ||--o{ QUALITY_GATE_LOGS : \"document_id\"\n",
    "    USERS ||--o{ QUALITY_GATE_LOGS : \"checked_by\"\n",
    "\n",
    "    DOCUMENTS ||--o{ AUDIT_LOGS : \"document_id\"\n",
    "\"\"\"\n",
    "\n",
    "def extract_front_matter(mermaid_text: str):\n",
    "    s = mermaid_text.strip(\"\\n\")\n",
    "    if not s.lstrip().startswith(\"---\"):\n",
    "        return {}, s\n",
    "\n",
    "    m = re.match(r\"^\\s*---\\s*(.*?)\\s*---\\s*(.*)$\", s, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}, s\n",
    "\n",
    "    front = m.group(1)\n",
    "    body = m.group(2)\n",
    "\n",
    "    theme = None\n",
    "    layout = None\n",
    "    for line in front.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"theme:\"):\n",
    "            theme = line.split(\":\", 1)[1].strip()\n",
    "        if line.startswith(\"layout:\"):\n",
    "            layout = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    init = {}\n",
    "    if theme:\n",
    "        init[\"theme\"] = theme\n",
    "    if layout:\n",
    "        init[\"layout\"] = layout\n",
    "\n",
    "    return init, body.strip(\"\\n\")\n",
    "\n",
    "init_cfg, diagram = extract_front_matter(raw)\n",
    "init_directive = \"\"\n",
    "if init_cfg:\n",
    "    init_directive = f\"%%{{init: {json.dumps(init_cfg)} }}%%\\n\"\n",
    "\n",
    "diagram_final = init_directive + diagram\n",
    "div_id = f\"mmd-{uuid.uuid4().hex}\"\n",
    "\n",
    "html = f\"\"\"\n",
    "<div id=\"{div_id}\" class=\"mermaid\">\n",
    "{diagram_final}\n",
    "</div>\n",
    "\n",
    "<script type=\"module\">\n",
    "  const render = async () => {{\n",
    "    if (!window.__mermaid_loaded__) {{\n",
    "      const mermaid = (await import(\"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\")).default;\n",
    "      window.mermaid = mermaid;\n",
    "      window.__mermaid_loaded__ = true;\n",
    "      mermaid.initialize({{\n",
    "        startOnLoad: false,\n",
    "        securityLevel: \"loose\"\n",
    "      }});\n",
    "    }}\n",
    "    await window.mermaid.run({{\n",
    "      nodes: [document.getElementById(\"{div_id}\")]\n",
    "    }});\n",
    "  }};\n",
    "  render();\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f87e0",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22847b6f",
   "metadata": {},
   "source": [
    "lire les mot cle (token) du document et dire de quelle classe apartien selon les mot cle quil a :\n",
    "\n",
    "    \"BON_DE_COMMANDE\": [\n",
    "        \"BON DE COMMANDE\",\n",
    "        \"COMMANDE\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"TVA\"\n",
    "    ],\n",
    "    \"PURCHASE_ORDER\": [\n",
    "        \"PURCHASE ORDER\",\n",
    "        \"PO NUMBER\",\n",
    "        \"UNIT PRICE\",\n",
    "        \"QUANTITY\",\n",
    "        \"TOTAL AMOUNT\"\n",
    "    ],\n",
    "    \"CONTRAT\": [\n",
    "        \"CONTRAT\",\n",
    "        \"IL A ÉTÉ CONVENU\",\n",
    "        \"ENTRE LES SOUSSIGNÉS\",\n",
    "        \"RÉSILIATION\",\n",
    "        \"SIGNATURE\"\n",
    "    ],\n",
    "    \"ARTICLE\": [\n",
    "        \"ARTICLE\",\n",
    "        \"VU LA LOI\",\n",
    "        \"CONSIDÉRANT\",\n",
    "        \"DÉCRET\",\n",
    "        \"DISPOSITION\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue détectée : fr\n",
      "Classe détectée : FACTURE\n",
      "\n",
      "Scores détaillés :\n",
      "  FACTURE -> 7\n",
      "  BON_DE_COMMANDE -> 4\n",
      "  CONTRAT -> 2\n",
      "  ARTICLE -> 2\n",
      "  FORMULAIRE -> 5\n",
      "\n",
      "Mots-clés détectés :\n",
      "  FACTURE -> ['FACTURE', 'TVA', 'MODE DE PAIEMENT', 'PAIEMENT', 'CLIENT', 'DESCRIPTION', 'TTC']\n",
      "  BON_DE_COMMANDE -> ['MONTANT', 'TVA', 'SIGNATURE', 'DESCRIPTION']\n",
      "  CONTRAT -> ['SIGNATURE', 'SIGNATURE']\n",
      "  ARTICLE -> ['CODE', 'ACT']\n",
      "  FORMULAIRE -> ['DATE', 'SIGNATURE', 'CACHET', 'DATE', 'SIGNATURE']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from collections import defaultdict\n",
    "\n",
    "# TEXTE OCR \n",
    "texte = OCR_TEXT\n",
    "\n",
    "#  MOTS-CLÉS PAR CLASSE \n",
    "KEYWORDS = {\n",
    "\n",
    "    \"FACTURE\": [\n",
    "        # FR forts\n",
    "        \"FACTURE\",\n",
    "        \"NUMERO DE FACTURE\",\n",
    "        \"N° FACTURE\",\n",
    "        \"REFERENCE FACTURE\",\n",
    "        \"DATE DE FACTURE\",\n",
    "        \"ECHEANCE\",\n",
    "        \"DATE D'ECHEANCE\",\n",
    "        \"MONTANT A PAYER\",\n",
    "        \"MONTANT A PAYER TTC\",\n",
    "        \"MONTANT TTC\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"TOTAL HT\",\n",
    "        \"MONTANT HT\",\n",
    "        \"TVA\",\n",
    "        \"TAUX DE TVA\",\n",
    "        \"MONTANT TVA\",\n",
    "        \"SOUS-TOTAL\",\n",
    "        \"NET A PAYER\",\n",
    "        \"NET A PAYER TTC\",\n",
    "        \"SOLDE DU\",\n",
    "        \"A REGLER\",\n",
    "        \"MODE DE PAIEMENT\",\n",
    "        \"REGLEMENT\",\n",
    "        \"PAIEMENT\",\n",
    "        \"IBAN\",\n",
    "        \"BIC\",\n",
    "        \"RIB\",\n",
    "        \"VIREMENT\",\n",
    "        \"CHEQUE\",\n",
    "        \"ESPECES\",\n",
    "        \"BANQUE\",\n",
    "        \"REFERENCE CLIENT\",\n",
    "        \"CODE CLIENT\",\n",
    "        \"CLIENT\",\n",
    "        \"ADRESSE DE FACTURATION\",\n",
    "        \"ADRESSE DE LIVRAISON\",\n",
    "        \"SIRET\",\n",
    "        \"SIREN\",\n",
    "        \"RCS\",\n",
    "        \"N° TVA\",\n",
    "        \"TVA INTRACOMMUNAUTAIRE\",\n",
    "        \"N° TVA INTRACOMMUNAUTAIRE\",\n",
    "        \"BON DE LIVRAISON\",\n",
    "        \"BL\",\n",
    "        \"COMMANDE\",\n",
    "        \"N° COMMANDE\",\n",
    "        \"REFERENCE COMMANDE\",\n",
    "        \"DESIGNATION\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"QUANTITE\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"P.U.\",\n",
    "        \"MONTANT LIGNE\",\n",
    "        \"TOTAL LIGNE\",\n",
    "        \"REMISE\",\n",
    "        \"DISCOUNT\",\n",
    "        \"FRAIS DE PORT\",\n",
    "        \"LIVRAISON\",\n",
    "        \"PENALITES DE RETARD\",\n",
    "        \"CONDITIONS DE PAIEMENT\",\n",
    "        \"TTC\",\n",
    "        \"HT\",\n",
    "        \"TIMBRE\",\n",
    "        # EN\n",
    "        \"INVOICE\",\n",
    "        \"INVOICE NUMBER\",\n",
    "        \"INVOICE NO\",\n",
    "        \"BILL TO\",\n",
    "        \"SHIP TO\",\n",
    "        \"DUE DATE\",\n",
    "        \"PAYMENT TERMS\",\n",
    "        \"SUBTOTAL\",\n",
    "        \"TAX\",\n",
    "        \"VAT\",\n",
    "        \"TOTAL\",\n",
    "        \"TOTAL AMOUNT\",\n",
    "        \"AMOUNT DUE\",\n",
    "        \"BALANCE DUE\",\n",
    "        \"BANK TRANSFER\",\n",
    "        \"IBAN\",\n",
    "        \"BIC\",\n",
    "        \"SWIFT\"\n",
    "    ],\n",
    "\n",
    "    \"BON_DE_COMMANDE\": [\n",
    "        # FR forts\n",
    "        \"BON DE COMMANDE\",\n",
    "        \"BC\",\n",
    "        \"N° BC\",\n",
    "        \"NUMERO DE COMMANDE\",\n",
    "        \"N° COMMANDE\",\n",
    "        \"REFERENCE COMMANDE\",\n",
    "        \"DATE DE COMMANDE\",\n",
    "        \"COMMANDE\",\n",
    "        \"ACHETEUR\",\n",
    "        \"FOURNISSEUR\",\n",
    "        \"ADRESSE DE LIVRAISON\",\n",
    "        \"ADRESSE DE FACTURATION\",\n",
    "        \"LIVRAISON\",\n",
    "        \"DATE DE LIVRAISON\",\n",
    "        \"CONDITIONS DE LIVRAISON\",\n",
    "        \"INCOTERM\",\n",
    "        \"INCOTERMS\",\n",
    "        \"DESIGNATION\",\n",
    "        \"ARTICLE\",\n",
    "        \"REFERENCE\",\n",
    "        \"REF\",\n",
    "        \"CODE ARTICLE\",\n",
    "        \"CODE PRODUIT\",\n",
    "        \"SKU\",\n",
    "        \"QUANTITE\",\n",
    "        \"QTE\",\n",
    "        \"UNITE\",\n",
    "        \"PU\",\n",
    "        \"P.U.\",\n",
    "        \"PRIX UNITAIRE\",\n",
    "        \"PRIX UNITARE\",  # typo OCR fréquent\n",
    "        \"MONTANT\",\n",
    "        \"TOTAL\",\n",
    "        \"TOTAL HT\",\n",
    "        \"TOTAL TTC\",\n",
    "        \"TVA\",\n",
    "        \"SOUS-TOTAL\",\n",
    "        \"REMISE\",\n",
    "        \"CONDITIONS DE PAIEMENT\",\n",
    "        \"DELAI DE PAIEMENT\",\n",
    "        \"SIGNATURE\",\n",
    "        \"VALIDATION\",\n",
    "        \"APPROBATION\",\n",
    "        \"BON POUR ACCORD\",\n",
    "        # EN\n",
    "        \"PURCHASE ORDER\",\n",
    "        \"PO\",\n",
    "        \"PO NUMBER\",\n",
    "        \"ORDER NUMBER\",\n",
    "        \"ORDER DATE\",\n",
    "        \"BUYER\",\n",
    "        \"VENDOR\",\n",
    "        \"SUPPLIER\",\n",
    "        \"SHIP TO\",\n",
    "        \"BILL TO\",\n",
    "        \"DELIVERY DATE\",\n",
    "        \"DELIVERY TERMS\",\n",
    "        \"INCOTERMS\",\n",
    "        \"ITEM\",\n",
    "        \"ITEM CODE\",\n",
    "        \"SKU\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"QUANTITY\",\n",
    "        \"QTY\",\n",
    "        \"UNIT PRICE\",\n",
    "        \"PRICE\",\n",
    "        \"SUBTOTAL\",\n",
    "        \"TAX\",\n",
    "        \"VAT\",\n",
    "        \"TOTAL AMOUNT\",\n",
    "        \"AUTHORIZED SIGNATURE\"\n",
    "    ],\n",
    "\n",
    "    \"CONTRAT\": [\n",
    "        # FR forts\n",
    "        \"CONTRAT\",\n",
    "        \"CONVENTION\",\n",
    "        \"ACCORD\",\n",
    "        \"IL A ETE CONVENU\",\n",
    "        \"ENTRE LES SOUSSIGNES\",\n",
    "        \"LES PARTIES\",\n",
    "        \"PARTIE\",\n",
    "        \"PREAMBULE\",\n",
    "        \"OBJET DU CONTRAT\",\n",
    "        \"OBJET\",\n",
    "        \"DUREE\",\n",
    "        \"DATE D'EFFET\",\n",
    "        \"ENTREE EN VIGUEUR\",\n",
    "        \"RENOUVELLEMENT\",\n",
    "        \"RESILIATION\",\n",
    "        \"RESILIATION ANTICIPEE\",\n",
    "        \"CLAUSE\",\n",
    "        \"ARTICLE 1\",\n",
    "        \"ARTICLE 2\",\n",
    "        \"OBLIGATIONS\",\n",
    "        \"ENGAGEMENTS\",\n",
    "        \"RESPONSABILITE\",\n",
    "        \"CONFIDENTIALITE\",\n",
    "        \"NON-DIVULGATION\",\n",
    "        \"PROPRIETE INTELLECTUELLE\",\n",
    "        \"FORCE MAJEURE\",\n",
    "        \"LITIGE\",\n",
    "        \"JURIDICTION\",\n",
    "        \"TRIBUNAL COMPETENT\",\n",
    "        \"DROIT APPLICABLE\",\n",
    "        \"LOI APPLICABLE\",\n",
    "        \"INDEMNISATION\",\n",
    "        \"PENALITES\",\n",
    "        \"GARANTIE\",\n",
    "        \"ANNEXE\",\n",
    "        \"AVENANT\",\n",
    "        \"SIGNATURE\",\n",
    "        \"FAIT A\",\n",
    "        \"LE PRESENT CONTRAT\",\n",
    "        # EN\n",
    "        \"CONTRACT\",\n",
    "        \"AGREEMENT\",\n",
    "        \"THIS AGREEMENT\",\n",
    "        \"WHEREAS\",\n",
    "        \"BETWEEN THE UNDERSIGNED\",\n",
    "        \"PARTIES\",\n",
    "        \"TERM\",\n",
    "        \"EFFECTIVE DATE\",\n",
    "        \"COMMENCEMENT\",\n",
    "        \"RENEWAL\",\n",
    "        \"TERMINATION\",\n",
    "        \"CONFIDENTIALITY\",\n",
    "        \"NONDISCLOSURE\",\n",
    "        \"INTELLECTUAL PROPERTY\",\n",
    "        \"GOVERNING LAW\",\n",
    "        \"JURISDICTION\",\n",
    "        \"LIABILITY\",\n",
    "        \"INDEMNIFICATION\",\n",
    "        \"FORCE MAJEURE\",\n",
    "        \"AMENDMENT\",\n",
    "        \"APPENDIX\",\n",
    "        \"SIGNATURE\"\n",
    "    ],\n",
    "\n",
    "    \"ARTICLE\": [\n",
    "        # FR\n",
    "        \"ARTICLE\",\n",
    "        \"ART.\",\n",
    "        \"VU LA LOI\",\n",
    "        \"VU LE CODE\",\n",
    "        \"CODE\",\n",
    "        \"CONSIDERANT\",\n",
    "        \"CONSIDÉRANT\",\n",
    "        \"ATTENDU QUE\",\n",
    "        \"DECRET\",\n",
    "        \"DÉCRET\",\n",
    "        \"ARRETE\",\n",
    "        \"ARRÊTÉ\",\n",
    "        \"LOI\",\n",
    "        \"ORDONNANCE\",\n",
    "        \"CIRCULAIRE\",\n",
    "        \"DISPOSITION\",\n",
    "        \"ALINEA\",\n",
    "        \"PARAGRAPHE\",\n",
    "        \"CHAPITRE\",\n",
    "        \"SECTION\",\n",
    "        \"TITRE\",\n",
    "        \"JOURNAL OFFICIEL\",\n",
    "        \"REPUBLIC\",\n",
    "        \"REPUBLIQUE\",\n",
    "        \"MINISTERE\",\n",
    "        \"MINISTÈRE\",\n",
    "        \"TRIBUNAL\",\n",
    "        \"COUR D'APPEL\",\n",
    "        \"CONSEIL D'ETAT\",\n",
    "        \"CONSEIL D’ÉTAT\",\n",
    "        \"DECISION\",\n",
    "        \"DÉCISION\",\n",
    "        \"JURISPRUDENCE\",\n",
    "        \"PROCEDURE\",\n",
    "        \"PROCÉDURE\",\n",
    "        \"SANCTION\",\n",
    "        \"AMENDE\",\n",
    "        \"AMENDEMENT\",\n",
    "        \"CONFORMEMENT A\",\n",
    "        \"EN APPLICATION DE\",\n",
    "        \"A COMPTER DU\",\n",
    "        \"ENTREE EN VIGUEUR\",\n",
    "        # EN\n",
    "        \"ARTICLE\",\n",
    "        \"SECTION\",\n",
    "        \"CHAPTER\",\n",
    "        \"WHEREAS\",\n",
    "        \"ACT\",\n",
    "        \"DECREE\",\n",
    "        \"REGULATION\",\n",
    "        \"LAW\",\n",
    "        \"PROVISION\",\n",
    "        \"Pursuant to\",\n",
    "        \"In accordance with\",\n",
    "        \"ENTRY INTO FORCE\",\n",
    "        \"EFFECTIVE\"\n",
    "    ],\n",
    "\n",
    "    \"FORMULAIRE\": [\n",
    "        # FR\n",
    "        \"FORMULAIRE\",\n",
    "        \"DEMANDE\",\n",
    "        \"DEMANDEUR\",\n",
    "        \"BENEFICIAIRE\",\n",
    "        \"BÉNÉFICIAIRE\",\n",
    "        \"NOM\",\n",
    "        \"PRENOM\",\n",
    "        \"PRÉNOM\",\n",
    "        \"DATE\",\n",
    "        \"DATE DE L'EXAMEN\",\n",
    "        \"SIGNATURE\",\n",
    "        \"CACHET\",\n",
    "        \"SIGNATURE ET CACHET\",\n",
    "        \"JE CERTIFIE\",\n",
    "        \"CERTIFIE QUE\",\n",
    "        \"CERTIFICATION\",\n",
    "        \"REPRESENTANT\",\n",
    "        \"REPRÉSENTANT\",\n",
    "        \"REPRESENTANT DU CLUB\",\n",
    "        \"CLUB\",\n",
    "        \"LICENCIE\",\n",
    "        \"LICENCIÉ\",\n",
    "        \"PIECES FOURNIES\",\n",
    "        \"DOCUMENT\",\n",
    "        \"INFORMATIONS FIGURANT\",\n",
    "        \"CACHEt DOIT ETRE LISIBLE\",\n",
    "        \"CADRE RESERVE\",\n",
    "        \"A REMPLIR\",\n",
    "        # EN\n",
    "        \"FORM\",\n",
    "        \"APPLICATION\",\n",
    "        \"APPLICANT\",\n",
    "        \"BENEFICIARY\",\n",
    "        \"NAME\",\n",
    "        \"FIRST NAME\",\n",
    "        \"DATE\",\n",
    "        \"SIGNATURE\",\n",
    "        \"STAMP\",\n",
    "        \"I CERTIFY\",\n",
    "        \"CERTIFY THAT\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#  DETECTION LANGUE \n",
    "sample = texte[:2000]\n",
    "try:\n",
    "    doc_lang = detect(sample)\n",
    "except:\n",
    "    doc_lang = \"fr\"\n",
    "\n",
    "#  CHARGER SPACY LEGER \n",
    "nlp = spacy.load(\n",
    "    \"fr_core_news_sm\",\n",
    "    disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"]\n",
    ")\n",
    "\n",
    "#  TOKENISATION GLOBALE \n",
    "doc = nlp.make_doc(texte)\n",
    "\n",
    "tokens = [t.text.upper() for t in doc if not t.is_space]\n",
    "\n",
    "# texte normalisé pour détection de phrases clés\n",
    "text_upper = \" \".join(tokens)\n",
    "\n",
    "#  SCORING DETERMINISTE \n",
    "scores = defaultdict(int)\n",
    "matched_keywords = defaultdict(list)\n",
    "\n",
    "for doc_type, keywords in KEYWORDS.items():\n",
    "    for kw in keywords:\n",
    "        if kw in text_upper:\n",
    "            scores[doc_type] += 1\n",
    "            matched_keywords[doc_type].append(kw)\n",
    "\n",
    "#  DECISION \n",
    "if scores:\n",
    "    detected_class = max(scores, key=scores.get)\n",
    "else:\n",
    "    detected_class = \"UNKNOWN\"\n",
    "\n",
    "#  RESULTAT \n",
    "print(\"Langue détectée :\", doc_lang)\n",
    "print(\"Classe détectée :\", detected_class)\n",
    "print(\"\\nScores détaillés :\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"  {k} -> {v}\")\n",
    "\n",
    "print(\"\\nMots-clés détectés :\")\n",
    "for k, v in matched_keywords.items():\n",
    "    print(f\"  {k} -> {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5fd1a",
   "metadata": {},
   "source": [
    "## Produire une sortie JSON (verification du la clasification et aplication de regex d'extaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0617391",
   "metadata": {},
   "source": [
    "construit un JSON de sortie en décidant doc_type et status (OK/REVIEW) à partir des scores et des paramètres THRESHOLD/MARGIN, puis elle prépare le routage des règles en chargeant (et fusionnant) automatiquement des fichiers rules/common, rules/{doc_type} et éventuellement rules/templates/{template_id} afin de savoir quelles règles d’extraction/validation appliquer ensuite, avec un fallback minimal si aucun fichier n’existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5f42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doc_id\": \"39bb07c4-b0a7-490e-afcb-a276555097bb\",\n",
      "  \"doc_type\": \"FACTURE\",\n",
      "  \"status\": \"OK\",\n",
      "  \"scores\": {\n",
      "    \"FACTURE\": 7,\n",
      "    \"BON_DE_COMMANDE\": 4,\n",
      "    \"CONTRAT\": 2,\n",
      "    \"ARTICLE\": 2,\n",
      "    \"FORMULAIRE\": 5\n",
      "  },\n",
      "  \"matched_keywords\": {\n",
      "    \"FACTURE\": [\n",
      "      \"FACTURE\",\n",
      "      \"TVA\",\n",
      "      \"MODE DE PAIEMENT\",\n",
      "      \"PAIEMENT\",\n",
      "      \"CLIENT\",\n",
      "      \"DESCRIPTION\",\n",
      "      \"TTC\"\n",
      "    ],\n",
      "    \"BON_DE_COMMANDE\": [\n",
      "      \"MONTANT\",\n",
      "      \"TVA\",\n",
      "      \"SIGNATURE\",\n",
      "      \"DESCRIPTION\"\n",
      "    ],\n",
      "    \"CONTRAT\": [\n",
      "      \"SIGNATURE\",\n",
      "      \"SIGNATURE\"\n",
      "    ],\n",
      "    \"ARTICLE\": [\n",
      "      \"CODE\",\n",
      "      \"ACT\"\n",
      "    ],\n",
      "    \"FORMULAIRE\": [\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\",\n",
      "      \"CACHET\",\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\"\n",
      "    ]\n",
      "  },\n",
      "  \"threshold\": 3,\n",
      "  \"margin\": 2,\n",
      "  \"language_hint\": \"fr\",\n",
      "  \"decision_debug\": {\n",
      "    \"top_score\": 7,\n",
      "    \"second_score\": 5,\n",
      "    \"diff\": 2\n",
      "  },\n",
      "  \"routing\": {\n",
      "    \"rules_dir\": \"rules\",\n",
      "    \"doc_type\": \"FACTURE\",\n",
      "    \"template_id\": null,\n",
      "    \"ruleset\": {\n",
      "      \"ruleset_id\": \"INLINE_FACTURE_V1\",\n",
      "      \"extractors\": {},\n",
      "      \"validators\": {}\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "# PARAMS de décision (stables)\n",
    "\n",
    "THRESHOLD = 3 # score minimum pour dire “je suis assez sûr\"\n",
    "MARGIN = 2 # l’écart minimum entre le meilleur score et le 2e meilleur score pour éviter les confusions\n",
    "\n",
    "\n",
    "# 1) Normaliser la langue -> language_hint\n",
    "# (doc_lang vient de la cellule précédente)\n",
    "\n",
    "if isinstance(doc_lang, str):\n",
    "    if doc_lang.startswith(\"fr\"):\n",
    "        language_hint = \"fr\"\n",
    "    elif doc_lang.startswith(\"en\"):\n",
    "        language_hint = \"en\"\n",
    "    else:\n",
    "        language_hint = \"mix\"\n",
    "else:\n",
    "    language_hint = \"mix\"\n",
    "\n",
    "\n",
    "# 2) Stabiliser scores/matched_keywords\n",
    "# - On veut que toutes les classes apparaissent, même à 0 / []\n",
    "# - KEYWORDS vient de la cellule précédente\n",
    "\n",
    "scores_stable = {cls: int(scores.get(cls, 0)) for cls in KEYWORDS.keys()}\n",
    "matched_stable = {cls: list(matched_keywords.get(cls, [])) for cls in KEYWORDS.keys()}\n",
    "\n",
    "\n",
    "# 3) Calculer top_score et second_score\n",
    "\n",
    "sorted_items = sorted(scores_stable.items(), key=lambda kv: kv[1], reverse=True)\n",
    "top_type, top_score = sorted_items[0] if sorted_items else (\"UNKNOWN\", 0)\n",
    "second_score = sorted_items[1][1] if len(sorted_items) > 1 else 0\n",
    "\n",
    "\n",
    "# 4) Décision OK/REVIEW + doc_type final\n",
    "# - si aucun signal -> UNKNOWN + REVIEW\n",
    "# - sinon OK si seuil+margin, sinon REVIEW (classe = meilleure hypothèse)\n",
    "\n",
    "if top_score == 0: \n",
    "    doc_type_final = \"UNKNOWN\" \n",
    "    status = \"REVIEW\" \n",
    "else:\n",
    "    doc_type_final = top_type\n",
    "    if top_score >= THRESHOLD and (top_score - second_score) >= MARGIN:\n",
    "        status = \"OK\" \n",
    "    else:\n",
    "        status = \"REVIEW\" \n",
    "\n",
    "\n",
    "# 5) Construire le JSON\n",
    "\n",
    "result: Dict[str, Any] = {\n",
    "    \"doc_id\": str(uuid.uuid4()),\n",
    "    \"doc_type\": doc_type_final,\n",
    "    \"status\": status,\n",
    "    \"scores\": scores_stable,\n",
    "    \"matched_keywords\": matched_stable,\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"margin\": MARGIN,\n",
    "    \"language_hint\": language_hint \n",
    "}\n",
    "\n",
    "# Debug\n",
    "result[\"decision_debug\"] = { \n",
    "    \"top_score\": top_score, \n",
    "    \"second_score\": second_score, \n",
    "    \"diff\": top_score - second_score \n",
    "} \n",
    "\n",
    "\n",
    "# (6.5) Routage JSON/YAML (déterministe) \n",
    "# Objectif: préparer le \"cerveau\" qui charge les règles \n",
    "# par doc_type + template_id (sans duplication de code) \n",
    "# \n",
    "# - Si PyYAML n'est pas installé, fallback JSON (ou dict en dur) \n",
    "# - Pour notebook MVP, on peut garder des \"règles inline\" si fichiers absents \n",
    " \n",
    " \n",
    "def safe_load_yaml_or_json(path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Charge un fichier YAML ou JSON et retourne un dict.\n",
    "    Retourne None si fichier introuvable/erreur.\n",
    "    \"\"\"\n",
    "    try:  \n",
    "        import os \n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "\n",
    "        if path.lower().endswith(\".json\"):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        # YAML\n",
    "        try:\n",
    "            import yaml  # type: ignore\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return yaml.safe_load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def merge_rules(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Merge simple et déterministe:\n",
    "    - dict: override keys\n",
    "    - list: concat (base + override) sans dédup (tu peux dédup plus tard si besoin)\n",
    "    - autres: override\n",
    "    \"\"\"\n",
    "    out = dict(base)\n",
    "    for k, v in override.items():\n",
    "        if k not in out:\n",
    "            out[k] = v\n",
    "            continue\n",
    "        if isinstance(out[k], dict) and isinstance(v, dict):\n",
    "            out[k] = merge_rules(out[k], v)\n",
    "        elif isinstance(out[k], list) and isinstance(v, list):\n",
    "            out[k] = out[k] + v\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def route_rules(\n",
    "    doc_type: str,\n",
    "    template_id: Optional[str] = None,\n",
    "    rules_dir: str = \"rules\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Résolution déterministe:\n",
    "    1) rules/common.(yaml|json) (facultatif)\n",
    "    2) rules/{doc_type}.(yaml|json) (facultatif)\n",
    "    3) rules/templates/{template_id}.(yaml|json) (facultatif)\n",
    "    Fallback: règles inline minimales.\n",
    "    \"\"\"\n",
    "    # common\n",
    "    common = (\n",
    "        safe_load_yaml_or_json(f\"{rules_dir}/common.yaml\")\n",
    "        or safe_load_yaml_or_json(f\"{rules_dir}/common.json\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    # doc_type specific\n",
    "    dt_rules = (\n",
    "        safe_load_yaml_or_json(f\"{rules_dir}/{doc_type}.yaml\")\n",
    "        or safe_load_yaml_or_json(f\"{rules_dir}/{doc_type}.json\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    merged = merge_rules(common, dt_rules)\n",
    "\n",
    "    #   template specific\n",
    "    if template_id:\n",
    "        tpl_rules = (\n",
    "            safe_load_yaml_or_json(f\"{rules_dir}/templates/{template_id}.yaml\")\n",
    "            or safe_load_yaml_or_json(f\"{rules_dir}/templates/{template_id}.json\")\n",
    "            or {}\n",
    "        )\n",
    "        merged = merge_rules(merged, tpl_rules)\n",
    "\n",
    "    # Fallback minimal si rien trouvé: règles vides, mais structure stable\n",
    "    if not merged:\n",
    "        merged = {\n",
    "            \"ruleset_id\": f\"INLINE_{doc_type}_V1\",\n",
    "            \"extractors\": {},\n",
    "            \"validators\": {}\n",
    "        }\n",
    "\n",
    "    # Ajout meta de routage (traçabilité)\n",
    "    merged.setdefault(\"ruleset_id\", f\"RULESET_{doc_type}_V1\")\n",
    "    merged.setdefault(\"extractors\", {})\n",
    "    merged.setdefault(\"validators\", {})\n",
    "    return merged\n",
    "\n",
    "# On attache une \"config active\" au result (pour l'étape API ensuite)\n",
    "# template_id peut ne pas exister encore (calculé plus tard), donc None ici\n",
    "result[\"routing\"] = {\n",
    "    \"rules_dir\": \"rules\",\n",
    "    \"doc_type\": result[\"doc_type\"],\n",
    "    \"template_id\": result.get(\"template_id\", None),\n",
    "    \"ruleset\": route_rules(result[\"doc_type\"], template_id=result.get(\"template_id\", None))\n",
    "}\n",
    "\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb612b1",
   "metadata": {},
   "source": [
    "## METADATA “empreinte de mise en page”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcedc4a",
   "metadata": {},
   "source": [
    "calcule une “empreinte” de la mise en page (orientation, inclinaison, taille, présence de tableau, densité de texte, qualité OCR), génère un template_id stable (hash) pour reconnaître le même modèle plus tard, puis applique un quality gate : si la qualité est mauvaise (ex: OCR faible, trop peu de mots, skew élevé), il force le status du document à REVIEW même si la classification était OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doc_id\": \"39bb07c4-b0a7-490e-afcb-a276555097bb\",\n",
      "  \"doc_type\": \"FACTURE\",\n",
      "  \"status\": \"OK\",\n",
      "  \"scores\": {\n",
      "    \"FACTURE\": 7,\n",
      "    \"BON_DE_COMMANDE\": 4,\n",
      "    \"CONTRAT\": 2,\n",
      "    \"ARTICLE\": 2,\n",
      "    \"FORMULAIRE\": 5\n",
      "  },\n",
      "  \"matched_keywords\": {\n",
      "    \"FACTURE\": [\n",
      "      \"FACTURE\",\n",
      "      \"TVA\",\n",
      "      \"MODE DE PAIEMENT\",\n",
      "      \"PAIEMENT\",\n",
      "      \"CLIENT\",\n",
      "      \"DESCRIPTION\",\n",
      "      \"TTC\"\n",
      "    ],\n",
      "    \"BON_DE_COMMANDE\": [\n",
      "      \"MONTANT\",\n",
      "      \"TVA\",\n",
      "      \"SIGNATURE\",\n",
      "      \"DESCRIPTION\"\n",
      "    ],\n",
      "    \"CONTRAT\": [\n",
      "      \"SIGNATURE\",\n",
      "      \"SIGNATURE\"\n",
      "    ],\n",
      "    \"ARTICLE\": [\n",
      "      \"CODE\",\n",
      "      \"ACT\"\n",
      "    ],\n",
      "    \"FORMULAIRE\": [\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\",\n",
      "      \"CACHET\",\n",
      "      \"DATE\",\n",
      "      \"SIGNATURE\"\n",
      "    ]\n",
      "  },\n",
      "  \"threshold\": 3,\n",
      "  \"margin\": 2,\n",
      "  \"language_hint\": \"fr\",\n",
      "  \"decision_debug\": {\n",
      "    \"top_score\": 7,\n",
      "    \"second_score\": 5,\n",
      "    \"diff\": 2\n",
      "  },\n",
      "  \"routing\": {\n",
      "    \"rules_dir\": \"rules\",\n",
      "    \"doc_type\": \"FACTURE\",\n",
      "    \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "    \"ruleset\": {\n",
      "      \"ruleset_id\": \"INLINE_FACTURE_V1\",\n",
      "      \"extractors\": {},\n",
      "      \"validators\": {}\n",
      "    }\n",
      "  },\n",
      "  \"layout_fingerprint\": {\n",
      "    \"page_count\": 1,\n",
      "    \"page_sizes\": [\n",
      "      {\n",
      "        \"w\": 1152,\n",
      "        \"h\": 1536\n",
      "      }\n",
      "    ],\n",
      "    \"orientation_deg\": 0,\n",
      "    \"skew_angle_deg\": [\n",
      "      0.0\n",
      "    ],\n",
      "    \"has_table\": [\n",
      "      true\n",
      "    ],\n",
      "    \"table_line_counts\": [\n",
      "      {\n",
      "        \"h_lines\": 115,\n",
      "        \"v_lines\": 13\n",
      "      }\n",
      "    ],\n",
      "    \"text_density\": [\n",
      "      {\n",
      "        \"text_density_alnum_chars_per_mpx\": 390.5119719328704,\n",
      "        \"image_area_mpx\": 1.769472\n",
      "      }\n",
      "    ],\n",
      "    \"ocr_confidence\": [\n",
      "      {\n",
      "        \"ocr_avg_conf\": 85.55555555555556,\n",
      "        \"ocr_words\": 180\n",
      "      }\n",
      "    ],\n",
      "    \"aspect_ratio\": [\n",
      "      0.75\n",
      "    ],\n",
      "    \"grid_type\": \"ITEM_TABLE\"\n",
      "  },\n",
      "  \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "  \"template_signature\": {\n",
      "    \"page_count\": 1,\n",
      "    \"w_rounded\": 1150,\n",
      "    \"h_rounded\": 1550,\n",
      "    \"aspect_ratio_rounded\": 0.75,\n",
      "    \"orientation_deg\": 0,\n",
      "    \"has_table\": [\n",
      "      true\n",
      "    ],\n",
      "    \"h_lines_bucket\": \">15\",\n",
      "    \"v_lines_bucket\": \"6-15\",\n",
      "    \"grid_type\": \"ITEM_TABLE\"\n",
      "  },\n",
      "  \"quality_gate\": {\n",
      "    \"status\": \"PASS\",\n",
      "    \"reasons\": [],\n",
      "    \"thresholds\": {\n",
      "      \"min_ocr_avg_conf\": 75,\n",
      "      \"min_ocr_words\": 40,\n",
      "      \"max_abs_skew_deg\": 5.0,\n",
      "      \"min_text_density_alnum_chars_per_mpx\": 120\n",
      "    }\n",
      "  },\n",
      "  \"metadata_summary\": {\n",
      "    \"page_count\": 1,\n",
      "    \"template_id\": \"tpl_223dc390c94e7d2c\",\n",
      "    \"orientation_deg\": 0,\n",
      "    \"avg_skew_deg\": 0.0,\n",
      "    \"has_table_any\": true,\n",
      "    \"ocr_avg_conf_mean\": 85.55555555555556,\n",
      "    \"grid_type\": \"ITEM_TABLE\",\n",
      "    \"quality_gate_status\": \"PASS\",\n",
      "    \"status_after_quality_gate\": \"OK\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# suppose: prepped (PIL image), OCR_TEXT (str), result (dict) existent déjà\n",
    "\n",
    "def pil_to_gray_np(pil_img):\n",
    "    return np.array(pil_img.convert(\"L\"))\n",
    "\n",
    "def get_orientation_deg(pil_img) -> int:\n",
    "    \"\"\"\n",
    "    Orientation 0/90/180/270 via Tesseract OSD.\n",
    "    Si OSD échoue, retourne 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(pil_img)\n",
    "        for line in osd.splitlines():\n",
    "            if line.lower().startswith(\"rotate:\"):\n",
    "                return int(line.split(\":\")[1].strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "def rotate_pil_by_deg(pil_img, deg: int):\n",
    "    if deg % 360 == 0:\n",
    "        return pil_img\n",
    "    return pil_img.rotate(-deg, expand=True)\n",
    "\n",
    "def estimate_skew_hough_deg(gray: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estime un skew petit angle via lignes Hough.\n",
    "    Retour typiquement dans [-10, +10] si doc normal.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    _, bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    edges = cv2.Canny(bw, 50, 150)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges,\n",
    "        1,\n",
    "        np.pi / 180,\n",
    "        threshold=120,\n",
    "        minLineLength=max(60, int(min(gray.shape) * 0.12)),\n",
    "        maxLineGap=10\n",
    "    )\n",
    "\n",
    "    if lines is None:\n",
    "        return 0.0\n",
    "\n",
    "    angles = []\n",
    "    for x1, y1, x2, y2 in lines[:, 0]:\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        if dx == 0:\n",
    "            continue\n",
    "        angle = np.degrees(np.arctan2(dy, dx))\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        if angle > 45:\n",
    "            angle -= 90\n",
    "        if -15 <= angle <= 15:\n",
    "            angles.append(angle)\n",
    "\n",
    "    if not angles:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.median(angles))\n",
    "\n",
    "def has_table_hough(gray: np.ndarray) -> dict:\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    _, bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    edges = cv2.Canny(bw, 50, 150)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges, 1, np.pi / 180,\n",
    "        threshold=80,\n",
    "        minLineLength=max(40, int(min(gray.shape) * 0.08)),\n",
    "        maxLineGap=10\n",
    "    )\n",
    "\n",
    "    h_count = 0\n",
    "    v_count = 0\n",
    "    if lines is not None:\n",
    "        for x1, y1, x2, y2 in lines[:, 0]:\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            if abs(dy) <= max(2, 0.1 * abs(dx)):\n",
    "                h_count += 1\n",
    "            if abs(dx) <= max(2, 0.1 * abs(dy)):\n",
    "                v_count += 1\n",
    "\n",
    "    has_table = (h_count >= 6 and v_count >= 4)\n",
    "    return {\"has_table\": bool(has_table), \"h_lines\": int(h_count), \"v_lines\": int(v_count)}\n",
    "\n",
    "def ocr_confidence_stats(pil_img) -> dict:\n",
    "    try:\n",
    "        data = pytesseract.image_to_data(\n",
    "            pil_img,\n",
    "            lang=getattr(args, \"lang\", None) or \"fra\",\n",
    "            config=globals().get(\"config\", \"\"),\n",
    "            output_type=pytesseract.Output.DICT\n",
    "        )\n",
    "        confs = []\n",
    "        for c in data.get(\"conf\", []):\n",
    "            try:\n",
    "                c = float(c)\n",
    "                if c >= 0:\n",
    "                    confs.append(c)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not confs:\n",
    "            return {\"ocr_avg_conf\": None, \"ocr_words\": 0}\n",
    "\n",
    "        return {\"ocr_avg_conf\": float(sum(confs) / len(confs)), \"ocr_words\": int(len(confs))}\n",
    "    except Exception:\n",
    "        return {\"ocr_avg_conf\": None, \"ocr_words\": 0}\n",
    "\n",
    "def text_density_stats(ocr_text: str, pil_img) -> dict:\n",
    "    w, h = pil_img.size\n",
    "    area_mpx = (w * h) / 1_000_000.0\n",
    "    area_mpx = area_mpx if area_mpx > 0 else 1.0\n",
    "    alnum_chars = sum(1 for ch in ocr_text if ch.isalnum())\n",
    "    return {\n",
    "        \"text_density_alnum_chars_per_mpx\": float(alnum_chars / area_mpx),\n",
    "        \"image_area_mpx\": float(area_mpx)\n",
    "    }\n",
    "\n",
    "def stable_template_id(fingerprint: dict) -> str:\n",
    "    canon = json.dumps(fingerprint, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "    digest = hashlib.sha256(canon.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"tpl_{digest[:16]}\"\n",
    "\n",
    "def bucketize_lines(n: int) -> str:\n",
    "    if n <= 0:\n",
    "        return \"0\"\n",
    "    if 1 <= n <= 5:\n",
    "        return \"1-5\"\n",
    "    if 6 <= n <= 15:\n",
    "        return \"6-15\"\n",
    "    return \">15\"\n",
    "\n",
    "def round_size(x: int, base: int = 50) -> int:\n",
    "    return int(base * round(x / base))\n",
    "\n",
    "def round_ratio(x: float, decimals: int = 2) -> float:\n",
    "    return float(round(x, decimals))\n",
    "\n",
    "def compute_grid_type_from_counts(h_lines: int, v_lines: int) -> str:\n",
    "    \"\"\"\n",
    "    Heuristique déterministe:\n",
    "      - ITEM_TABLE: tableau d'articles (beaucoup de lignes H et V)\n",
    "      - BOXED_FORM: formulaire encadré (beaucoup de H, peu de V)\n",
    "      - NONE: pas de structure de grille notable\n",
    "    \"\"\"\n",
    "    if h_lines >= 20 and v_lines >= 8:\n",
    "        return \"ITEM_TABLE\"\n",
    "    if h_lines >= 20 and v_lines < 8:\n",
    "        return \"BOXED_FORM\"\n",
    "    return \"NONE\"\n",
    "\n",
    "def compute_quality_gate(layout_fp: dict) -> dict:\n",
    "    reasons = []\n",
    "\n",
    "    # OCR confidence\n",
    "    ocr_list = layout_fp.get(\"ocr_confidence\", [])\n",
    "    ocr_avg = None\n",
    "    ocr_words = None\n",
    "    if ocr_list:\n",
    "        ocr_avg = ocr_list[0].get(\"ocr_avg_conf\", None)\n",
    "        ocr_words = ocr_list[0].get(\"ocr_words\", None)\n",
    "\n",
    "    if ocr_avg is not None and ocr_avg < 75:\n",
    "        reasons.append(\"LOW_OCR_CONF\")\n",
    "    if ocr_words is not None and ocr_words < 40:\n",
    "        reasons.append(\"TOO_FEW_WORDS\")\n",
    "\n",
    "    # skew\n",
    "    skew_list = layout_fp.get(\"skew_angle_deg\", [])\n",
    "    if skew_list:\n",
    "        skew = float(skew_list[0])\n",
    "        if abs(skew) > 5.0:\n",
    "            reasons.append(\"HIGH_SKEW\")\n",
    "\n",
    "    # densité\n",
    "    dens_list = layout_fp.get(\"text_density\", [])\n",
    "    if dens_list:\n",
    "        dens = dens_list[0].get(\"text_density_alnum_chars_per_mpx\", None)\n",
    "        if dens is not None and dens < 120:\n",
    "            reasons.append(\"LOW_TEXT_DENSITY\")\n",
    "\n",
    "    status = \"PASS\" if len(reasons) == 0 else \"REVIEW\"\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"reasons\": reasons,\n",
    "        \"thresholds\": {\n",
    "            \"min_ocr_avg_conf\": 75,\n",
    "            \"min_ocr_words\": 40,\n",
    "            \"max_abs_skew_deg\": 5.0,\n",
    "            \"min_text_density_alnum_chars_per_mpx\": 120\n",
    "        }\n",
    "    }\n",
    "\n",
    "----\n",
    "# 1) Orientation + correction\n",
    "----\n",
    "orientation_deg = get_orientation_deg(prepped)\n",
    "prepped_oriented = rotate_pil_by_deg(prepped, orientation_deg)\n",
    "\n",
    "----\n",
    "# 2) Calcul features par page (1 page ici)\n",
    "----\n",
    "pages = [prepped_oriented]\n",
    "\n",
    "page_sizes = []\n",
    "skew_angles = []\n",
    "table_flags = []\n",
    "ocr_confs = []\n",
    "densities = []\n",
    "\n",
    "for p in pages:\n",
    "    w, h = p.size\n",
    "    page_sizes.append({\"w\": int(w), \"h\": int(h)})\n",
    "\n",
    "    gray = pil_to_gray_np(p)\n",
    "\n",
    "    skew_angles.append(estimate_skew_hough_deg(gray))\n",
    "    tbl = has_table_hough(gray)\n",
    "    table_flags.append(tbl)\n",
    "\n",
    "    ocr_confs.append(ocr_confidence_stats(p))\n",
    "    densities.append(text_density_stats(OCR_TEXT, p))\n",
    "\n",
    "----\n",
    "# 3) layout_fingerprint complet\n",
    "----\n",
    "layout_fingerprint = {\n",
    "    \"page_count\": int(len(pages)),\n",
    "    \"page_sizes\": page_sizes,\n",
    "    \"orientation_deg\": int(orientation_deg),\n",
    "    \"skew_angle_deg\": [float(a) for a in skew_angles],\n",
    "    \"has_table\": [bool(t[\"has_table\"]) for t in table_flags],\n",
    "    \"table_line_counts\": [{\"h_lines\": t[\"h_lines\"], \"v_lines\": t[\"v_lines\"]} for t in table_flags],\n",
    "    \"text_density\": densities,\n",
    "    \"ocr_confidence\": ocr_confs,\n",
    "    \"aspect_ratio\": [float(ps[\"w\"] / ps[\"h\"]) for ps in page_sizes]\n",
    "}\n",
    "\n",
    "----\n",
    "# 3bis) grid_type\n",
    "----\n",
    "h_lines0 = int(layout_fingerprint[\"table_line_counts\"][0][\"h_lines\"]) if layout_fingerprint[\"table_line_counts\"] else 0\n",
    "v_lines0 = int(layout_fingerprint[\"table_line_counts\"][0][\"v_lines\"]) if layout_fingerprint[\"table_line_counts\"] else 0\n",
    "grid_type = compute_grid_type_from_counts(h_lines0, v_lines0)\n",
    "layout_fingerprint[\"grid_type\"] = grid_type\n",
    "\n",
    "----\n",
    "# 3ter) quality_gate\n",
    "----\n",
    "quality_gate = compute_quality_gate(layout_fingerprint)\n",
    "\n",
    "\n",
    "# ENFORCE: appliquer la quality gate sur status\n",
    "\n",
    "# - si gate FAIL/REVIEW => on force status REVIEW (même si classification OK)\n",
    "# - on laisse doc_type inchangé (hypothèse), mais status devient REVIEW\n",
    "if isinstance(result, dict) and quality_gate.get(\"status\") == \"REVIEW\":\n",
    "    result[\"status\"] = \"REVIEW\"\n",
    "    result.setdefault(\"quality_gate_enforced\", {})\n",
    "    result[\"quality_gate_enforced\"] = {\n",
    "        \"forced_status\": \"REVIEW\",\n",
    "        \"reason_count\": len(quality_gate.get(\"reasons\", [])),\n",
    "        \"reasons\": list(quality_gate.get(\"reasons\", [])),\n",
    "        \"rule_id\": \"RULE_QUALITY_GATE_V1\"\n",
    "    }\n",
    "\n",
    "----\n",
    "# 4) template_id stable bucketed\n",
    "----\n",
    "w0 = int(page_sizes[0][\"w\"]) if page_sizes else 0\n",
    "h0_size = int(page_sizes[0][\"h\"]) if page_sizes else 0\n",
    "ratio0 = (w0 / h0_size) if (w0 and h0_size) else 0.0\n",
    "\n",
    "layout_signature_bucketed = {\n",
    "    \"page_count\": layout_fingerprint[\"page_count\"],\n",
    "    \"w_rounded\": round_size(w0, base=50),\n",
    "    \"h_rounded\": round_size(h0_size, base=50),\n",
    "    \"aspect_ratio_rounded\": round_ratio(ratio0, 2),\n",
    "    \"orientation_deg\": layout_fingerprint[\"orientation_deg\"],\n",
    "    \"has_table\": layout_fingerprint[\"has_table\"],\n",
    "    \"h_lines_bucket\": bucketize_lines(int(layout_fingerprint[\"table_line_counts\"][0][\"h_lines\"])) if layout_fingerprint[\"table_line_counts\"] else \"0\",\n",
    "    \"v_lines_bucket\": bucketize_lines(int(layout_fingerprint[\"table_line_counts\"][0][\"v_lines\"])) if layout_fingerprint[\"table_line_counts\"] else \"0\",\n",
    "    \"grid_type\": grid_type\n",
    "}\n",
    "template_id = stable_template_id(layout_signature_bucketed)\n",
    "\n",
    "----\n",
    "# 5) Attacher au JSON principal\n",
    "----\n",
    "result[\"layout_fingerprint\"] = layout_fingerprint\n",
    "result[\"template_id\"] = template_id\n",
    "result[\"template_signature\"] = layout_signature_bucketed\n",
    "result[\"quality_gate\"] = quality_gate\n",
    "\n",
    "# Update routing si présent (doc_type déjà, maintenant template_id est connu)\n",
    "if \"routing\" in result and isinstance(result[\"routing\"], dict):\n",
    "    result[\"routing\"][\"template_id\"] = template_id\n",
    "    # On peut re-router avec template_id (si tu as des règles template)\n",
    "    try:\n",
    "        ruleset = result[\"routing\"].get(\"ruleset\", {})\n",
    "        # Si route_rules existe (cellule précédente), on recharge:\n",
    "        if \"route_rules\" in globals():\n",
    "            result[\"routing\"][\"ruleset\"] = route_rules(result.get(\"doc_type\", \"UNKNOWN\"), template_id=template_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# metadata_summary\n",
    "summary = result.get(\"metadata_summary\", {})\n",
    "summary.update({\n",
    "    \"page_count\": layout_fingerprint[\"page_count\"],\n",
    "    \"template_id\": template_id,\n",
    "    \"orientation_deg\": orientation_deg,\n",
    "    \"avg_skew_deg\": float(sum(skew_angles) / len(skew_angles)) if skew_angles else 0.0,\n",
    "    \"has_table_any\": bool(any(layout_fingerprint[\"has_table\"])),\n",
    "    \"ocr_avg_conf_mean\": (\n",
    "        float(\n",
    "            sum(c[\"ocr_avg_conf\"] for c in ocr_confs if c[\"ocr_avg_conf\"] is not None) /\n",
    "            max(1, len([c for c in ocr_confs if c[\"ocr_avg_conf\"] is not None]))\n",
    "        )\n",
    "        if ocr_confs else None\n",
    "    ),\n",
    "    \"grid_type\": grid_type,\n",
    "    \"quality_gate_status\": quality_gate[\"status\"],\n",
    "    \"status_after_quality_gate\": result.get(\"status\")\n",
    "})\n",
    "result[\"metadata_summary\"] = summary\n",
    "\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f5c08",
   "metadata": {},
   "source": [
    "## Chunking par page avec identifiants (audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcc40a",
   "metadata": {},
   "source": [
    "découpe le texte OCR en petits morceaux (chunks) par page avec un overlap, et pour chaque chunk crée un chunk_id + positions start/end + excerpt, afin de pouvoir faire la recherche puis citer précisément la source (page + chunk + extrait). si le document est en status=REVIEW, la cellule ne crée aucun chunk (elle “skip”) pour éviter d’indexer/répondre avec un document jugé de mauvaise qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks générés: 2\n",
      "Aperçu 2 premiers chunks:\n",
      "- page=1 chunk_index=0 start=0 end=800 len=800 id=chk_cea0474ff3f54ad7\n",
      "  excerpt: FACTURE CODE CLENT NUMERO FCo0o1 4/20/2016 0002 Ma petite entreprise CLIENT 19,rue de place 1° mai SARL EL HANA 16000 Alger Centre IROUTE DE BEJAIA SETIF Tel : 00-00-52-12- 119000 Ident Fiscal : 160 N°art : 160100000000 ...\n",
      "- page=1 chunk_index=1 start=650 end=907 len=257 id=chk_ef0f2e72a304464c\n",
      "  excerpt:  19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui...\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# - result (dict) : contient au moins result[\"doc_id\"] + result[\"status\"]\n",
    "# - OCR_TEXT (str) : texte OCR complet (pour 1 page dans ton pipeline actuel)\n",
    "# - skip si status=REVIEW (quality_gate enforced)\n",
    "# - excerpt (pour citations)\n",
    "# - char_len (debug/qualité)\n",
    "# - nettoyage minimal stable\n",
    "# - garde page + start/end pour citer précisément\n",
    "\n",
    "CHUNK_SIZE = 800      \n",
    "CHUNK_OVERLAP = 150   \n",
    "EXCERPT_LEN = 220     \n",
    "\n",
    "\n",
    "def clean_text_stable(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text_fixed(text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Découpe en chunks de taille fixe avec overlap.\n",
    "    Retourne: {chunk_index, start_char, end_char, text}\n",
    "    \"\"\"\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size doit être > 0\")\n",
    "    if overlap < 0 or overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap doit être >= 0 et < chunk_size\")\n",
    "\n",
    "    text = clean_text_stable(text)\n",
    "    n = len(text)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    step = chunk_size - overlap\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    idx = 0\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk_str = text[start:end]\n",
    "\n",
    "        if len(chunk_str.strip()) >= 20:\n",
    "            chunks.append({\n",
    "                \"chunk_index\": idx,\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": end,\n",
    "                \"text\": chunk_str\n",
    "            })\n",
    "\n",
    "        idx += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        start += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks_for_pages(\n",
    "    doc_id: str,\n",
    "    pages_text: List[str],\n",
    "    chunk_size: int,\n",
    "    overlap: int,\n",
    "    excerpt_len: int\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Construit des chunks par page.\n",
    "    Format chunk:\n",
    "      chunk_id, doc_id, page, chunk_index, start_char, end_char, char_len, text, excerpt\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    for page_num, page_text in enumerate(pages_text, start=1):\n",
    "        cleaned = clean_text_stable(page_text)\n",
    "        page_chunks = chunk_text_fixed(cleaned, chunk_size, overlap)\n",
    "\n",
    "        for c in page_chunks:\n",
    "            chunk_id = f\"chk_{uuid.uuid4().hex[:16]}\"\n",
    "            chunk_text = c[\"text\"]\n",
    "            excerpt = chunk_text[:excerpt_len]\n",
    "\n",
    "            all_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"page\": page_num,\n",
    "                \"chunk_index\": c[\"chunk_index\"],\n",
    "                \"start_char\": c[\"start_char\"],\n",
    "                \"end_char\": c[\"end_char\"],\n",
    "                \"char_len\": int(c[\"end_char\"] - c[\"start_char\"]),\n",
    "                \"text\": chunk_text,\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "# Enforced: si status=REVIEW -> pas de chunking\n",
    "\n",
    "if result.get(\"status\") == \"REVIEW\":\n",
    "    result[\"chunking\"] = {\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"overlap\": CHUNK_OVERLAP,\n",
    "        \"excerpt_len\": EXCERPT_LEN,\n",
    "        \"chunks_count\": 0,\n",
    "        \"skipped_reason\": \"status=REVIEW (quality_gate enforced)\"\n",
    "    }\n",
    "    result[\"chunks\"] = []\n",
    "    print(\"[skip] status=REVIEW (quality_gate enforced) -> pas de chunking\")\n",
    "else:\n",
    "    doc_id = result[\"doc_id\"]\n",
    "    pages_text = [OCR_TEXT]  # 1 page (plus tard: liste de textes par page)\n",
    "\n",
    "    chunks = build_chunks_for_pages(\n",
    "        doc_id=doc_id,\n",
    "        pages_text=pages_text,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=CHUNK_OVERLAP,\n",
    "        excerpt_len=EXCERPT_LEN\n",
    "    )\n",
    "\n",
    "    result[\"chunking\"] = {\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"overlap\": CHUNK_OVERLAP,\n",
    "        \"excerpt_len\": EXCERPT_LEN,\n",
    "        \"chunks_count\": len(chunks)\n",
    "    }\n",
    "    result[\"chunks\"] = chunks\n",
    "\n",
    "    print(f\"Chunks générés: {len(chunks)}\")\n",
    "    print(\"Aperçu 2 premiers chunks:\")\n",
    "    for c in chunks[:2]:\n",
    "        preview = c[\"excerpt\"].replace(\"\\n\", \" \")\n",
    "        print(f\"- page={c['page']} chunk_index={c['chunk_index']} start={c['start_char']} end={c['end_char']} len={c['char_len']} id={c['chunk_id']}\")\n",
    "        print(f\"  excerpt: {preview}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c5a13",
   "metadata": {},
   "source": [
    "## Indexation + recherche (Elasticsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1941c4",
   "metadata": {},
   "source": [
    "garde une liste CORPUS de documents déjà traités (en évitant les doublons), calcule un source_hash pour chaque doc, ignore les docs en status=REVIEW, envoie tous les chunks dans Elasticsearch (création/reset de l’index + bulk index), puis fournit une fonction retrieve() qui fait une recherche texte dans ES avec filtres (doc_type, template_id, language_hint, status) et retourne les meilleurs chunks (avec dédoublonnage), et enfin lance un test de recherche pour afficher les hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99b79065",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"montant à payer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1393e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[index] backend=es docs=2 indexed_chunks=3 index_name=doc_chunks_v1\n",
      "[retrieve] query: montant à payer\n",
      "- score=2.2253 doc_id=39bb07c4-b0a7-490e-afcb-a276555097bb page=1 chunk_id=chk_ef0f2e72a304464c\n",
      "  excerpt:  19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre...\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# NumPy 2.x compat shim pour elasticsearch-py 7.x (OBLIGATOIRE avant import elasticsearch)\n",
    "if not hasattr(np, \"float_\"):\n",
    "    np.float_ = np.float64\n",
    "if not hasattr(np, \"int_\"):\n",
    "    np.int_ = np.int64\n",
    "\n",
    "import elasticsearch  # garde ton import\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "\n",
    "# Pré-checks (PAS de result factice)\n",
    "\n",
    "if \"result\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Variable 'result' manquante. Lance d'abord les cellules OCR -> JSON stable -> chunking \"\n",
    "        \"(elles créent result + result['chunks']).\"\n",
    "    )\n",
    "\n",
    "if not isinstance(result, dict):\n",
    "    raise RuntimeError(\"Variable 'result' invalide: attendu un dict.\")\n",
    "\n",
    "required_keys = [\"doc_id\", \"status\", \"chunks\"]\n",
    "missing = [k for k in required_keys if k not in result]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        f\"'result' existe mais il manque: {missing}. \"\n",
    "        \"Relance OCR -> JSON stable -> chunking.\"\n",
    "    )\n",
    "\n",
    "if not isinstance(result.get(\"chunks\"), list):\n",
    "    raise RuntimeError(\"'result[\\\"chunks\\\"]' doit être une liste. Relance la cellule de chunking.\")\n",
    "\n",
    "\n",
    "# Corpus en mémoire + anti-doublons robuste\n",
    "\n",
    "if \"CORPUS\" not in globals():\n",
    "    CORPUS = []\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s or \"\")\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def normalize_text_words(text: str) -> str:\n",
    "    t = strip_accents(text or \"\").lower()\n",
    "    t = \"\".join(ch if ch.isalnum() else \" \" for ch in t)\n",
    "    t = \" \".join(t.split())\n",
    "    return t\n",
    "\n",
    "def compute_source_hash_bow(text: str, top_k: int = 250) -> str:\n",
    "    norm = normalize_text_words(text)\n",
    "    toks = [w for w in norm.split() if len(w) >= 3]\n",
    "    if not toks:\n",
    "        return \"src_\" + \"0\" * 16\n",
    "\n",
    "    freq: Dict[str, int] = {}\n",
    "    for w in toks:\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "\n",
    "    items = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))[:top_k]\n",
    "    canon = \"|\".join([f\"{k}:{v}\" for k, v in items])\n",
    "    digest = hashlib.sha256(canon.encode(\"utf-8\")).hexdigest()\n",
    "    return \"src_\" + digest[:16]\n",
    "\n",
    "def compute_doc_text_from_chunks(doc: Dict[str, Any]) -> str:\n",
    "    return \"\\n\".join([ch.get(\"text\", \"\") for ch in doc.get(\"chunks\", [])])\n",
    "\n",
    "# dé-doublonnage du CORPUS existant\n",
    "seen = set()\n",
    "deduped = []\n",
    "for d in CORPUS:\n",
    "    if not isinstance(d, dict) or \"chunks\" not in d:\n",
    "        continue\n",
    "    base_text = compute_doc_text_from_chunks(d)\n",
    "    d[\"source_hash\"] = compute_source_hash_bow(base_text)\n",
    "    if d[\"source_hash\"] in seen:\n",
    "        continue\n",
    "    seen.add(d[\"source_hash\"])\n",
    "    deduped.append(d)\n",
    "CORPUS = deduped\n",
    "\n",
    "# ajout du doc courant si nouveau\n",
    "current_text = compute_doc_text_from_chunks(result)\n",
    "result[\"source_hash\"] = compute_source_hash_bow(current_text)\n",
    "\n",
    "existing_hashes = {d.get(\"source_hash\") for d in CORPUS if isinstance(d, dict)}\n",
    "if result.get(\"source_hash\") not in existing_hashes:\n",
    "    CORPUS.append(result)\n",
    "\n",
    "\n",
    "#  Backend Elasticsearch\n",
    "\n",
    "ES_URL = \"http://127.0.0.1:9200\"\n",
    "ES_INDEX = \"doc_chunks_v1\"\n",
    "ES_RESET_INDEX_EACH_BUILD = True  # notebook: True; plus tard: False\n",
    "ES_TIMEOUT = 30\n",
    "\n",
    "_es = Elasticsearch([ES_URL], timeout=ES_TIMEOUT)\n",
    "\n",
    "if not _es.ping():\n",
    "    raise ConnectionError(\n",
    "        f\"Impossible de joindre Elasticsearch sur {ES_URL}. \"\n",
    "        \"Vérifie que Elasticsearch est lancé et accessible sur 127.0.0.1:9200.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Collecte chunks (commune)\n",
    "\n",
    "_chunks_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "def _collect_all_chunks(corpus: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    all_chunks = []\n",
    "    for doc in corpus:\n",
    "        if not isinstance(doc, dict):\n",
    "            continue\n",
    "\n",
    "        # Enforced: ne pas indexer les docs REVIEW\n",
    "        if doc.get(\"status\") == \"REVIEW\":\n",
    "            continue\n",
    "\n",
    "        doc_id = doc.get(\"doc_id\")\n",
    "        doc_type = doc.get(\"doc_type\")\n",
    "        template_id = doc.get(\"template_id\")\n",
    "        lang = doc.get(\"language_hint\")\n",
    "        status = doc.get(\"status\")\n",
    "        source_hash = doc.get(\"source_hash\")\n",
    "\n",
    "        for ch in doc.get(\"chunks\", []):\n",
    "            txt = ch.get(\"text\", \"\") or \"\"\n",
    "            ex = ch.get(\"excerpt\", \"\") or (txt[:220] if txt else \"\")\n",
    "            all_chunks.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_type\": doc_type,\n",
    "                \"template_id\": template_id,\n",
    "                \"language_hint\": lang,\n",
    "                \"status\": status,\n",
    "                \"source_hash\": source_hash,\n",
    "                \"page\": int(ch.get(\"page\") or 0),\n",
    "                \"chunk_id\": ch.get(\"chunk_id\"),\n",
    "                \"text\": txt,\n",
    "                \"excerpt\": ex\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Index ES: mapping + build_index()\n",
    "\n",
    "def _create_or_reset_index_es(es, index_name: str):\n",
    "    mapping = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"doc_id\": {\"type\": \"keyword\"},\n",
    "                \"doc_type\": {\"type\": \"keyword\"},\n",
    "                \"template_id\": {\"type\": \"keyword\"},\n",
    "                \"language_hint\": {\"type\": \"keyword\"},\n",
    "                \"status\": {\"type\": \"keyword\"},\n",
    "                \"source_hash\": {\"type\": \"keyword\"},\n",
    "                \"page\": {\"type\": \"integer\"},\n",
    "                \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"excerpt\": {\"type\": \"text\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    exists = es.indices.exists(index=index_name)\n",
    "    if exists and ES_RESET_INDEX_EACH_BUILD:\n",
    "        es.indices.delete(index=index_name)\n",
    "        exists = False\n",
    "    if not exists:\n",
    "        es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def build_index(corpus: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Build ES index (reset + bulk index) depuis CORPUS.\n",
    "    Garde le même nom build_index() pour ne rien casser downstream.\n",
    "    \"\"\"\n",
    "    global _chunks_meta\n",
    "\n",
    "    _chunks_meta = _collect_all_chunks(corpus)\n",
    "    _create_or_reset_index_es(_es, ES_INDEX)\n",
    "\n",
    "    actions = []\n",
    "    for c in _chunks_meta:\n",
    "        doc_id = c.get(\"doc_id\") or \"\"\n",
    "        chunk_id = c.get(\"chunk_id\") or \"\"\n",
    "        es_id = f\"{doc_id}::{chunk_id}\"\n",
    "\n",
    "        actions.append({\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": ES_INDEX,\n",
    "            \"_id\": es_id,\n",
    "            \"_source\": c\n",
    "        })\n",
    "\n",
    "    if actions:\n",
    "        bulk(_es, actions, refresh=True)\n",
    "\n",
    "\n",
    "# Retrieve (ES)\n",
    "\n",
    "def _as_set(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, list):\n",
    "        return set(v)\n",
    "    return {v}\n",
    "\n",
    "def _norm_excerpt_for_dedup(s: str) -> str:\n",
    "    return normalize_text_words(s or \"\")[:600]\n",
    "\n",
    "def retrieve(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    "    dedup_by_doc: bool = True,\n",
    "    dedup_by_excerpt: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retourne top-k chunks avec {doc_id, page, chunk_id, excerpt, score}\n",
    "    filters possibles:\n",
    "      - doc_type: str ou [str]\n",
    "      - template_id: str ou [str]\n",
    "      - language_hint: str ou [str]\n",
    "      - status: str ou [str]\n",
    "    \"\"\"\n",
    "    filters = filters or {}\n",
    "\n",
    "    # Enforced par défaut: pas de REVIEW\n",
    "    if \"status\" not in filters:\n",
    "        filters = dict(filters)\n",
    "        filters[\"status\"] = \"OK\"\n",
    "\n",
    "    must_filters = []\n",
    "    dt = _as_set(filters.get(\"doc_type\"))\n",
    "    tpl = _as_set(filters.get(\"template_id\"))\n",
    "    lang = _as_set(filters.get(\"language_hint\"))\n",
    "    st = _as_set(filters.get(\"status\"))\n",
    "\n",
    "    def _terms(field, values):\n",
    "        if values is None:\n",
    "            return\n",
    "        vals = list(values)\n",
    "        if len(vals) == 1:\n",
    "            must_filters.append({\"term\": {field: vals[0]}})\n",
    "        else:\n",
    "            must_filters.append({\"terms\": {field: vals}})\n",
    "\n",
    "    _terms(\"doc_type\", dt)\n",
    "    _terms(\"template_id\", tpl)\n",
    "    _terms(\"language_hint\", lang)\n",
    "    _terms(\"status\", st)\n",
    "\n",
    "    body = {\n",
    "        \"size\": int(top_k) * 5,  # sur-récup pour dédoublonner ensuite\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"simple_query_string\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"text\"],\n",
    "                            \"default_operator\": \"and\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"filter\": must_filters\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = _es.search(index=ES_INDEX, body=body)\n",
    "    hits = res.get(\"hits\", {}).get(\"hits\", [])\n",
    "\n",
    "    out = []\n",
    "    seen_sources = set()\n",
    "    seen_excerpts = set()\n",
    "\n",
    "    for h in hits:\n",
    "        src = h.get(\"_source\", {}) or {}\n",
    "        score = float(h.get(\"_score\", 0.0))\n",
    "\n",
    "        if dedup_by_doc:\n",
    "            sh = src.get(\"source_hash\")\n",
    "            if sh and sh in seen_sources:\n",
    "                continue\n",
    "            if sh:\n",
    "                seen_sources.add(sh)\n",
    "\n",
    "        ex_norm = _norm_excerpt_for_dedup(src.get(\"excerpt\", \"\"))\n",
    "        if dedup_by_excerpt and ex_norm:\n",
    "            if ex_norm in seen_excerpts:\n",
    "                continue\n",
    "            seen_excerpts.add(ex_norm)\n",
    "\n",
    "        out.append({\n",
    "            \"doc_id\": src.get(\"doc_id\"),\n",
    "            \"page\": src.get(\"page\"),\n",
    "            \"chunk_id\": src.get(\"chunk_id\"),\n",
    "            \"excerpt\": src.get(\"excerpt\", \"\") or (src.get(\"text\", \"\")[:220]),\n",
    "            \"score\": score\n",
    "        })\n",
    "        if len(out) >= top_k:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build + test (sécurisé)\n",
    "\n",
    "build_index(CORPUS)\n",
    "print(f\"[index] backend=es docs={len(CORPUS)} indexed_chunks={len(_chunks_meta)} index_name={ES_INDEX}\")\n",
    "\n",
    "if \"test_query\" not in globals():\n",
    "    raise RuntimeError(\"Variable 'test_query' manquante. Exemple: test_query = \\\"tel\\\"\")\n",
    "\n",
    "hits = retrieve(test_query, top_k=3, filters={\"doc_type\": \"FACTURE\"})\n",
    "print(\"[retrieve] query:\", test_query)\n",
    "for h in hits:\n",
    "    print(f\"- score={h['score']:.4f} doc_id={h['doc_id']} page={h['page']} chunk_id={h['chunk_id']}\")\n",
    "    print(f\"  excerpt: {h['excerpt'][:180]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd69369",
   "metadata": {},
   "source": [
    "## API “chat” minimal : answer + sources (auditabilité / déterministe / quality gate) Applique des REGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1c809",
   "metadata": {},
   "source": [
    "## REGEX + extraction (rules only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09851a4",
   "metadata": {},
   "source": [
    "Prend le texte OCR (context), le “nettoie” de façon stable (minuscules, sans accents, espaces normalisés), puis applique un dictionnaire de règles INVOICE_RULES (chaque règle = un rule_id + une REGEX) pour extraire des champs précis d’une facture (montants, timbre, total, date, numéro facture, téléphone) ; pour chaque champ trouvé il renvoie à la fois la valeur extraite (fields) et une preuve d’où ça vient (audit: rule_id + match_raw), en normalisant aussi certains formats (nombres, date en ISO, téléphone sans séparateurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 1 — REGEX + extraction (rules only)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, Any\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = s.replace(\"’\", \"'\")\n",
    "    s = strip_accents(s)\n",
    "    s = s.lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def parse_number_pair(raw: str) -> Dict[str, Any]:\n",
    "    r = raw.strip()\n",
    "    compact = r.replace(\" \", \"\")\n",
    "\n",
    "    if \",\" in compact and \".\" in compact:\n",
    "        norm_val = compact.replace(\",\", \"\")\n",
    "        return {\"raw\": r, \"norm\": norm_val}\n",
    "\n",
    "    if \",\" in compact and \".\" not in compact:\n",
    "        norm_val = compact.replace(\",\", \".\")\n",
    "        return {\"raw\": r, \"norm\": norm_val}\n",
    "\n",
    "    return {\"raw\": r, \"norm\": compact}\n",
    "\n",
    "def normalize_date_iso(date_str: str) -> Dict[str, Any]:\n",
    "    raw = date_str.strip()\n",
    "    m = re.match(r\"^(\\d{1,2})[\\/\\-](\\d{1,2})[\\/\\-](\\d{2,4})$\", raw)\n",
    "    if not m:\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    a = int(m.group(1))\n",
    "    b = int(m.group(2))\n",
    "    y = int(m.group(3))\n",
    "    if y < 100:\n",
    "        y += 2000\n",
    "\n",
    "    if a > 12 and 1 <= b <= 12:\n",
    "        dd, mm = a, b\n",
    "    elif b > 12 and 1 <= a <= 12:\n",
    "        mm, dd = a, b\n",
    "    else:\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    if not (1 <= mm <= 12 and 1 <= dd <= 31):\n",
    "        return {\"raw\": raw, \"iso\": None}\n",
    "\n",
    "    return {\"raw\": raw, \"iso\": f\"{y:04d}-{mm:02d}-{dd:02d}\"}\n",
    "\n",
    "\n",
    "# Règles (audit) : Rule IDs + REGEX patterns\n",
    "\n",
    "INVOICE_RULES = {\n",
    "    \"montant_a_payer_ttc\": {\n",
    "        \"rule_id\": \"INV_R001_MONTANT_A_PAYER_TTC\",\n",
    "        \"pattern\": r\"montant\\s+a\\s+payer\\s+ttc\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\"\n",
    "    },\n",
    "    \"montant_a_payer\": {\n",
    "        \"rule_id\": \"INV_R002_MONTANT_A_PAYER\",\n",
    "        \"pattern\": r\"montant\\s+a\\s+payer\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\"\n",
    "    },\n",
    "    \"timbre\": {\n",
    "        \"rule_id\": \"INV_R003_TIMBRE\",\n",
    "        \"pattern\": r\"(timbre|rimbre)\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\"\n",
    "    },\n",
    "    \"total_ttc\": {\n",
    "        \"rule_id\": \"INV_R004_TOTAL_TTC\",\n",
    "        \"pattern\": r\"total\\s+ttc\\s*[:\\]\\[]?\\s*([0-9][0-9\\s,\\.]{2,})\"\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"rule_id\": \"INV_R005_DATE\",\n",
    "        \"pattern\": r\"\\b(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4})\\b\"\n",
    "    },\n",
    "    \"numero_facture\": {\n",
    "        \"rule_id\": \"INV_R006_NUMERO_FACTURE\",\n",
    "        \"pattern\": r\"(numero\\s+facture|n[°o]\\s*facture|invoice\\s+number)\\s*[:\\]\\[]?\\s*([a-z0-9\\-\\/]+)\"\n",
    "    },\n",
    "    # Nouveau champ: téléphone (tolérant OCR: espaces, tirets, +213, 0xxxxxxxxx, etc.)\n",
    "    \"telephone\": {\n",
    "        \"rule_id\": \"INV_R007_TELEPHONE\",\n",
    "        \"pattern\": r\"(?:tel|tél|telephone|téléphone)\\s*[:\\]\\-]?\\s*([+\\(]?\\d[\\d\\-\\s\\(\\)]{6,}\\d)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_invoice_fields(context: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retour:\n",
    "      fields: {champ: valeur}\n",
    "      audit:  {champ: {rule_id, match_raw}}\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\"fields\": {}, \"audit\": {}}\n",
    "    c = norm(context)\n",
    "\n",
    "    # Montant à payer TTC\n",
    "    r = INVOICE_RULES[\"montant_a_payer_ttc\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"fields\"][\"montant_a_payer_ttc\"] = val\n",
    "        out[\"audit\"][\"montant_a_payer_ttc\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Montant à payer (si TTC absent)\n",
    "    r = INVOICE_RULES[\"montant_a_payer\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m and \"montant_a_payer_ttc\" not in out[\"fields\"]:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"fields\"][\"montant_a_payer\"] = val\n",
    "        out[\"audit\"][\"montant_a_payer\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Timbre\n",
    "    r = INVOICE_RULES[\"timbre\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(2))\n",
    "        out[\"fields\"][\"timbre\"] = val\n",
    "        out[\"audit\"][\"timbre\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Total TTC\n",
    "    r = INVOICE_RULES[\"total_ttc\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        val = parse_number_pair(m.group(1))\n",
    "        out[\"fields\"][\"total_ttc\"] = val\n",
    "        out[\"audit\"][\"total_ttc\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Date\n",
    "    r = INVOICE_RULES[\"date\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        val = normalize_date_iso(m.group(1))\n",
    "        out[\"fields\"][\"date\"] = val\n",
    "        out[\"audit\"][\"date\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Numéro facture\n",
    "    r = INVOICE_RULES[\"numero_facture\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        val = {\"raw\": m.group(2), \"norm\": m.group(2)}\n",
    "        out[\"fields\"][\"numero_facture\"] = val\n",
    "        out[\"audit\"][\"numero_facture\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    # Téléphone\n",
    "    r = INVOICE_RULES[\"telephone\"]\n",
    "    m = re.search(r[\"pattern\"], c)\n",
    "    if m:\n",
    "        raw_tel = m.group(1).strip()\n",
    "        tel_norm = re.sub(r\"[^\\d+]\", \"\", raw_tel)  # enlève espaces/tirets/parenthèses\n",
    "        out[\"fields\"][\"telephone\"] = {\"raw\": raw_tel, \"norm\": tel_norm}\n",
    "        out[\"audit\"][\"telephone\"] = {\"rule_id\": r[\"rule_id\"], \"match_raw\": m.group(0)}\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf05a77",
   "metadata": {},
   "source": [
    "## Test / question (retrieve + réponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72165df5",
   "metadata": {},
   "source": [
    "Exécute une “question” sur ton corpus : il appelle retrieve() (Elasticsearch) pour récupérer les meilleurs chunks liés à query (avec tes filtres comme doc_type=FACTURE et status=OK), concatène leurs excerpt pour former un petit contexte, applique extract_invoice_fields() dessus, puis construit une réponse lisible + un JSON de sortie (answer/fields/audit/sources) qui contient à la fois les champs extraits (ex: téléphone, date…) et les sources (chunks + score) pour pouvoir justifier d’où vient chaque valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"montant à payer\",\n",
      "  \"mode\": \"fast\",\n",
      "  \"filters_applied\": {\n",
      "    \"doc_type\": \"FACTURE\",\n",
      "    \"status\": \"OK\"\n",
      "  },\n",
      "  \"answer\": \"Montant à payer TTC : 182,006.00 (norm=182006.00)\\nTimbre : 1,802.00 (norm=1802.00)\",\n",
      "  \"fields\": {\n",
      "    \"montant_a_payer_ttc\": {\n",
      "      \"raw\": \"182,006.00\",\n",
      "      \"norm\": \"182006.00\"\n",
      "    },\n",
      "    \"timbre\": {\n",
      "      \"raw\": \"1,802.00\",\n",
      "      \"norm\": \"1802.00\"\n",
      "    }\n",
      "  },\n",
      "  \"audit\": {\n",
      "    \"montant_a_payer_ttc\": {\n",
      "      \"rule_id\": \"INV_R001_MONTANT_A_PAYER_TTC\",\n",
      "      \"match_raw\": \"montant a payer ttc 182,006.00 \"\n",
      "    },\n",
      "    \"timbre\": {\n",
      "      \"rule_id\": \"INV_R003_TIMBRE\",\n",
      "      \"match_raw\": \"rimbre 1,802.00 \"\n",
      "    }\n",
      "  },\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"doc_id\": \"39bb07c4-b0a7-490e-afcb-a276555097bb\",\n",
      "      \"page\": 1,\n",
      "      \"chunk_id\": \"chk_ef0f2e72a304464c\",\n",
      "      \"excerpt\": \" 19.00 19,152.00 c1010 _ |Produit 10 1009 10.00 10,090.00 Non assujetti à latva [Montant à payer 180,204.00 [rimbre 1,802.00 Montant à payer ttc 182,006.00 Monatnt Facture enLettre … Cinq mille huit cent quatre vingt hui\",\n",
      "      \"score\": 2.2253423\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 2 — Test / question (retrieve + réponse)\n",
    "\n",
    "import json\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "def build_answer(query: str, hits: List[Dict[str, Any]], mode: str) -> Dict[str, Any]:\n",
    "    if not hits:\n",
    "        return {\"answer\": \"Je n’ai trouvé aucun passage pertinent dans le corpus.\", \"fields\": {}, \"audit\": {}}\n",
    "\n",
    "    context = \"\\n\".join([h[\"excerpt\"] for h in hits])\n",
    "    extracted = extract_invoice_fields(context)\n",
    "    fields = extracted[\"fields\"]\n",
    "    audit = extracted[\"audit\"]\n",
    "\n",
    "    if fields:\n",
    "        lines = []\n",
    "        if \"telephone\" in fields:\n",
    "            v = fields[\"telephone\"]\n",
    "            lines.append(f\"Téléphone : {v['raw']} (norm={v['norm']})\")\n",
    "        if \"montant_a_payer_ttc\" in fields:\n",
    "            v = fields[\"montant_a_payer_ttc\"]\n",
    "            lines.append(f\"Montant à payer TTC : {v['raw']} (norm={v['norm']})\")\n",
    "        if \"timbre\" in fields:\n",
    "            v = fields[\"timbre\"]\n",
    "            lines.append(f\"Timbre : {v['raw']} (norm={v['norm']})\")\n",
    "        if \"date\" in fields:\n",
    "            d = fields[\"date\"]\n",
    "            if d[\"iso\"]:\n",
    "                lines.append(f\"Date : {d['raw']} (iso={d['iso']})\")\n",
    "            else:\n",
    "                lines.append(f\"Date : {d['raw']} (iso=UNKNOWN)\")\n",
    "        if \"numero_facture\" in fields:\n",
    "            lines.append(f\"Numéro de facture : {fields['numero_facture']['raw']}\")\n",
    "\n",
    "        return {\"answer\": \"\\n\".join(lines), \"fields\": fields, \"audit\": audit}\n",
    "\n",
    "    n = 3 if mode == \"fast\" else 5\n",
    "    answer = \"Passages pertinents trouvés :\\n\" + \"\\n\".join(\n",
    "        [f\"- {h['excerpt'][:220]}...\" for h in hits[:n]]\n",
    "    )\n",
    "    return {\"answer\": answer, \"fields\": {}, \"audit\": {}}\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    mode: str = \"fast\",\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    "    top_k_fast: int = 5,\n",
    "    top_k_normal: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    mode = (mode or \"fast\").lower().strip()\n",
    "    top_k = top_k_fast if mode == \"fast\" else top_k_normal\n",
    "\n",
    "    filters = dict(filters or {})\n",
    "    if \"status\" not in filters:\n",
    "        filters[\"status\"] = \"OK\"\n",
    "\n",
    "    hits = retrieve(query, top_k=top_k, filters=filters)\n",
    "    built = build_answer(query, hits, mode)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"mode\": mode,\n",
    "        \"filters_applied\": filters,\n",
    "        \"answer\": built[\"answer\"],\n",
    "        \"fields\": built[\"fields\"],\n",
    "        \"audit\": built[\"audit\"],\n",
    "        \"sources\": hits\n",
    "    }\n",
    "\n",
    "# Test : extraction téléphone\n",
    "q = test_query\n",
    "resp = ask(q, mode=\"fast\", filters={\"doc_type\": \"FACTURE\"})\n",
    "print(json.dumps(resp, ensure_ascii=False, indent=2)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
